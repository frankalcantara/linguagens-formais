# Gramáticas e Linguagens Livres de Contexto {#sec-lingagens-livres-de-contexto}

::: {.content-hidden when-format="pdf"}
:::{.callout-tip}
Está sem tempo? Leia o [Expresso](.\ex\04-expresso.html).
:::
:::

No campo da ciência da computação teórica, a Hierarquia de Chomsky serve como o principal sistema de classificação para linguagens formais. Proposta por [Noam Chomsky](https://en.wikipedia.org/wiki/Noam_Chomsky), esta hierarquia organiza as linguagens em uma série de quatro níveis aninhados, cada um sendo um subconjunto do nível anterior $\text{Tipo 3} \subset \text{Tipo 2} \subset \text{Tipo 1} \subset \text{Tipo 0}$. A classificação baseia-se na complexidade das regras gramaticais necessárias para gerar as _strings_  de uma linguagem, estabelecendo uma correspondência direta entre a classe da linguagem, o tipo de gramática que a gera e o modelo de autômato capaz de reconhecê-la. Os níveis são:

* **Tipo 3: Linguagens Regulares**, geradas por gramáticas regulares e reconhecidas por autômatos finitos. 
* **Tipo 2: Linguagens Livres de Contexto**, geradas por Gramáticas Livres de Contexto e reconhecidas por autômatos com pilha. 
* **Tipo 1: Linguagens Sensíveis ao Contexto**, geradas por gramáticas sensíveis ao contexto e reconhecidas por autômatos linearmente limitados. 
* **Tipo 0: Linguagens Recursivamente Enumeráveis**, geradas por gramáticas irrestritas e reconhecidas por Máquinas de Turing.

Há uma história rica e complexa por trás do desenvolvimento da Hierarquia de Chomsky, que reflete as mudanças nas necessidades e nas compreensões da linguística e da computação ao longo do tempo.

## Hierarquia de Chomsky: História e Contexto

A Hierarquia de Chomsky representa um dos marcos mais elegantes da ciência do século XX, estabelecendo pontes entre linguística, matemática e ciência da computação. **Desenvolvida por Noam Chomsky entre 1956 e 1959**, esta classificação não apenas transformou os estudos linguísticos de uma disciplina descritiva em uma ciência formal rigorosa, mas também **forneceu as bases teóricas para a construção de compiladores e o processamento computacional de linguagens**. A hierarquia emergiu de uma convergência única de fatores históricos: a revolução cognitiva dos anos 1950, o desenvolvimento da teoria da computação, e a genialidade de um jovem linguista que soube sintetizar influências matemáticas diversas em uma estrutura unificada que perdura até hoje.

A história da Hierarquia de Chomsky começa com um encontro transformador que ocorreu em 1947, quando Chomsky, então com 19 anos e considerando abandonar a universidade, conheceu [Zellig S. Harris](https://en.wikipedia.org/wiki/Zellig_Harris) na Universidade da Pensilvânia. Harris era um dos fundadores da linguística estrutural americana e tinha estabelecido o primeiro departamento moderno de linguística dos Estados Unidos em 1946. Sob a orientação de Harris, Chomsky também estudou matemática com [Nathan Fine](https://en.wikipedia.org/wiki/Nathan_Fine) em Harvard. **Essas influências foram cruciais para moldar sua abordagem aos sistemas formais e metodologia científica**. Sua tese de mestrado de 1951, _A Morfofonêmica do Hebraico Moderno_, e especialmente seu trabalho em _A Estrutura Lógica da Teoria Linguística_ (LSLT), escrito enquanto era fellow júnior em Harvard (1951-55), começaram a transformar a abordagem estrutural de Harris em algo inteiramente novo.

### A revolução cognitiva e o ambiente do MIT

Os anos 1950 representavam um período de fermentação intelectual que se tornaria conhecido como a **Revolução Cognitiva**. [George Miller](https://en.wikipedia.org/wiki/George_Armitage_Miller), uma das figuras-chave, datou o início dessa revolução em 11 de setembro de 1956, quando pesquisadores de psicologia experimental, ciência da computação e linguística teórica apresentaram trabalhos sobre temas relacionados à ciência cognitiva em uma reunião do _Special Interest Group in Information Theory_ no MIT.

Durante esse período, vários paradigmas dominantes estavam sendo desafiados. O behaviorismo era a doutrina dominante em psicologia e linguística, enfatizando organização de dados e classificação taxonômica enquanto rejeitava o estudo de estados mentais internos. A linguística estrutural, liderada por figuras como Leonard Bloomfield, tratava a linguagem como um fenômeno social estudado por meio de análise de corpus.

A Hierarquia de Chomsky emergiu de uma convergência sofisticada de lógica matemática, teoria dos autômatos e teoria das funções recursivas desenvolvidas entre as décadas de 1930 e 1950. Chomsky construiu sistematicamente sobre trabalhos matemáticos anteriores, criando uma síntese notável.

**O trabalho de [Emil Post](https://en.wikipedia.org/wiki/Emil_Leon_Post) (1936, 1944)** foi fundamental. Post desenvolveu _sistemas canônicos de Post_ usando técnicas de reescrita de _strings_  que se tornaram fundamentais para a abordagem de Chomsky. Seu trabalho de 1936 _Processos combinatórios finitos – Formulação 1_ criou modelos computacionais essencialmente equivalentes às máquinas de Turing. Post queria _derivar mecanicamente inferências de uma sentença axiomática inicial_, e Chomsky aplicou diretamente essa estrutura lógica para descrever conjuntos de _strings_  na linguagem humana.

**O modelo de [McCulloch-Pitt](https://www.historyofinformation.com/detail.php?entryid=782)s de 1943** forneceu a ponte computacional. Seu trabalho _Um Cálculo Lógico das Ideias Imanentes na Atividade Nervosa_ ofereceu o primeiro modelo matemático conectando redes neurais à computação, levando diretamente à noção de autômatos finitos que se tornaram o Tipo-3 na Hierarquia de Chomsky. Eles demonstraram que neurônios poderiam funcionar como portas lógicas, estabelecendo a conexão entre sistemas biológicos e modelos computacionais formais.

**O trabalho de [Stephen Cole Kleene](https://en.wikipedia.org/wiki/Stephen_Cole_Kleene)** estabeleceu a equivalência fundamental entre expressões regulares e máquinas de estado finito (Teorema de Kleene), fornecendo a base matemática para as gramáticas do Tipo-3. Suas propriedades de fechamento para linguagens regulares sob união, concatenação e operação estrela de Kleene criaram o framework teórico necessário.

A estrutura teórica se baseou explicitamente na **teoria das funções recursivas**. Chomsky declarou que a gramática gerativa se desenvolveu dentro de _uma teoria matemática particular, a saber, a teoria das funções recursivas_. Isso foi importante porque a teoria das funções recursivas nas décadas de 1930-1940 formalizou a noção de _computação_, construindo **_sobre a tese de Church-Turing_** e as provas de equivalência entre diferentes modelos de computação.

### A elegância matemática dos quatro tipos

O desenvolvimento dos **quatro tipos específicos de gramáticas emergiu da aplicação de restrições matemáticas crescentes**. A classificação não foi arbitrária, mas resultou de uma lógica matemática rigorosa que Chomsky articulou em seus trabalhos seminais de 1956 (_Três Modelos para a Descrição da Linguagem_) e 1959 (_Sobre Certas Propriedades Formais das Gramáticas_).

**A estrutura hierárquica aninhada** representa uma beleza matemática notável. Cada classe é um subconjunto próprio da próxima classe menos restritiva: $\text{Tipo 3} ⊂ \text{Tipo 2} ⊂ \text{Tipo 1} ⊂ \text{Tipo 0}$. Cada tipo de gramática corresponde exatamente a uma classe de autômatos com poder computacional correspondente, criando uma correspondência perfeita entre restrições gramaticais e modelos computacionais.

- O **Tipo 0 (Gramáticas Irrestritas)** não possui restrições nas regras de produção ($\alpha \rightarrow \beta$), sendo equivalente às máquinas de Turing e capaz de gerar linguagens recursivamente enumeráveis.

- O **Tipo 1 (Gramáticas Sensíveis ao Contexto)** adiciona a propriedade não-contrativa ($\mid \alpha \mid ≤ \mid \beta \mid$), correspondendo aos autômatos linearmente limitados. 

- O **Tipo 2 (Gramáticas Livres de Contexto)** restringe o lado esquerdo a um único não-terminal ($A \rightarrow γ$), equivalendo aos autômatos de pilha.

- O **Tipo 3 (Gramáticas Regulares)** impõe as restrições mais específicas em padrões terminal/não-terminal, correspondendo aos autômatos finitos.

**A elegância conceitual reside na relação inversa** entre liberdade gramatical e complexidade computacional: gramáticas mais restritas (Tipo 3) requerem menor poder computacional (autômatos finitos), enquanto gramáticas menos restritas (Tipo 0) necessitam do maior poder computacional (máquinas de Turing).

Na ciência da computação, a hierarquia estabeleceu **as bases matemáticas para a teoria das linguagens formais**, criando uma classificação sistemática que permanece fundamental para a ciência da computação teórica. A realização de Chomsky de que _a descrição de um tipo de autômato e de um tipo de linguagem estão relacionadas_ conectou campos anteriormente desconectados da linguística e computação.

### Aplicações práticas em compiladores e processamento de linguagem

Na análise léxica, gramáticas do Tipo-3 (regulares) e autômatos finitos tornaram-se fundamentais para _tokenizar_ código fonte, identificando palavras-chave, _strings_  e identificadores. Na análise sintática, gramáticas do Tipo-2 (livres de contexto) formam a base teórica para analisar a maioria das linguagens de programação, lidando com estruturas aninhadas como parênteses, chamadas de função e estruturas de controle.

**Geradores de parser modernos usam especificações de Gramáticas Livres de Contextos** para gerar automaticamente parsers, com ferramentas implementando diretamente conceitos da Hierarquia de Chomsky. Classes de Gramáticas Livres de Contexto (**LL**, **LR**) tornaram-se padrão para parsing eficiente, tornando a compilação tratável mantendo poder expressivo suficiente.

A **Forma de Backus-Naur ($BNF$)** desenvolvida por [John Backus](https://en.wikipedia.org/wiki/John_Backus) e [Peter Naur](https://en.wikipedia.org/wiki/Peter_Naur) para **ALGOL 60** foi criada com conhecimento explícito do trabalho anterior de Chomsky sobre Gramáticas Livres de Contexto, tornando-se o padrão da indústria para definir sintaxe de linguagens de programação e implementando diretamente conceitos de gramática formal de Chomsky.

Não bastasse o que a atenta leitora viu até o momento, cada nível da hierarquia possui propriedades de complexidade computacional bem definidas. Linguagens regulares (Tipo 3) podem ser reconhecidas em tempo linear $O(n)$ com espaço constante. Linguagens livres de contexto (Tipo 2) são decidíveis em tempo polinomial $O(n^3)$ usando o algoritmo CYK, formando a base para a maioria dos compiladores de linguagens de programação. Linguagens sensíveis ao contexto (Tipo 1) são PSPACE-completas, com complexidade de tempo exponencial no pior caso, mas complexidade de espaço linear. Linguagens recursivamente enumeráveis (Tipo 0) são indecidíveis em geral.

:::{callout-tip}
**Algoritmo CYK (Cocke-Younger-Kasami)**

O algoritmo CYK é um método de programação dinâmica para determinar se uma _string_ pertence a uma linguagem livre de contexto. Desenvolvido independentemente por Cocke, Younger e Kasami, o algoritmo requer que a gramática esteja na Forma Normal de Chomsky, na qual todas as produções têm a forma $A \rightarrow BC$ ou $A \rightarrow a$.

O algoritmo constrói uma tabela triangular de tamanho $n \times n$, na qual $n$ é o comprimento da _string_ de entrada. Cada entrada $(i,j)$ da tabela indica quais não-terminais podem derivar a sub_string_ que inicia na posição $i$ com comprimento $j$. O preenchimento ocorre bottom-up: primeiro as _substrings_  de comprimento 1, depois 2, e assim sucessivamente até $n$.

A complexidade $O(n^3)$ resulta de três loops aninhados: dois para percorrer a tabela ($O(n^2)$ entradas) e um terceiro para testar todas as possíveis divisões de cada sub_string_ ($O(n)$ divisões por entrada). Esta complexidade polinomial torna o CYK prático para parsing de linguagens de programação, sendo fundamental na construção de parsers bottom-up.
:::

:::{callout-tip}
**PSPACE-Completas**

PSPACE é a classe de complexidade que contém todos os problemas de decisão solucionáveis por uma máquina de Turing determinística usando espaço polinomial. Um problema é PSPACE-completo se pertence ao PSPACE e qualquer problema em PSPACE pode ser reduzido a ele em tempo polinomial.

As linguagens sensíveis ao contexto (Tipo 1) são PSPACE-completas porque seu reconhecimento requer espaço proporcional ao comprimento da entrada, mas pode demandar tempo exponencial. Isso ocorre porque um autômato linearmente limitado (que reconhece essas linguagens) possui número finito de configurações possíveis, mas esse número cresce exponencialmente com o tamanho da entrada.

A aparente contradição entre "espaço linear" e "PSPACE-completo" se resolve considerando que PSPACE inclui problemas que usam espaço polinomial, e espaço linear é um caso particular de espaço polinomial ($O(n) \subset O(n^k)$). O tempo exponencial surge porque, mesmo com espaço limitado, o número de estados alcançáveis pode crescer exponencialmente, exigindo exploração exaustiva no pior caso.
:::

Além disso, os lemas de bombeamento para cada classe fornecem ferramentas matemáticas para provar que linguagens NÃO pertencem às classes inferiores. As propriedades de fechamento variam sistematicamente: **linguagens regulares são fechadas sob todas as operações padrão, linguagens livres de contexto são fechadas sob união, concatenação e estrela de Kleene mas NÃO sob interseção e complemento**.

### O Poder e as Limitações das Linguagens Regulares (Tipo 3)

As linguagens regulares representam a classe mais fundamental e restrita da hierarquia. Elas são formalmente definidas como o conjunto de linguagens que podem ser descritas por expressões regulares ou, equivalentemente, geradas por gramáticas regulares.

A estrutura de uma gramática regular é estritamente limitada. Suas regras de produção, que ditam como os símbolos podem ser reescritos, devem aderir a um formato rígido. Em uma gramática linear à direita, por exemplo, todas as regras devem ser da forma $A \rightarrow tN$ ou $A \rightarrow t$, na qual $A$ e $N$ são símbolos não terminais e $t$ é uma _string_ de símbolos terminais que pode ser vazia. Uma restrição similar se aplica às gramáticas lineares à esquerda. Esta limitação estrutural não é meramente uma convenção; ela é a fonte da principal limitação computacional das linguagens regulares: a incapacidade de modelar dependências aninhadas ou recursivas. 

O autômato finito, o reconhecedor para a classe das linguagens regulares, opera sem uma memória externa; seu único _histórico_ está contido no estado atual em que se encontra. Consequentemente, ele não pode _lembrar_ ou _contar_ ocorrências de símbolos para garantir correspondências em uma _string_. O exemplo canônico das limitações de um autômato finito são os parênteses aninhados. Um autômato finito não pode verificar se uma _string_ contém um número igual de parênteses abertos e fechados se estes parênteses puderem ser aninhados.

### A Ascensão às Linguagens Livres de Contexto (Tipo 2)

A necessidade de modelar estruturas sintáticas mais complexas, como expressões aritméticas com parênteses aninhados, blocos de código delimitados (´begin´...´end´) ou estruturas de dados recursivas, revela a insuficiência das linguagens regulares. Para superar essas limitações, ascendemos na hierarquia para as linguagens livres de contexto.

As Linguagens Livres de Contexto formam um superconjunto estrito das linguagens regulares; toda linguagem regular é, por definição, livre de contexto, mas o inverso não é verdadeiro. O poder expressivo adicional das Linguagens Livres de Contexto emana diretamente de uma flexibilização nas regras de produção de suas gramáticas. Em uma Gramática Livre de Contexto, uma regra de produção tem a forma $A \rightarrow \beta$, na qual $A$ é um único símbolo não-terminal e $\beta$ é uma _string_ qualquer de símbolos terminais e não terminais. A ausência de restrições sobre a posição dos não terminais em $\beta$ permite a definição de recursão central, como na regra $S \rightarrow aSb$, que é a chave para modelar estruturas aninhadas.

### Exemplos Práticos que Distinguem as Duas Classes

A distinção teórica entre essas duas classes é melhor ilustrada pelo exemplo canônico da linguagem $L={a^n b^n \mid n \geq 0}$, que consiste em __strings__ com um número de `a`s seguido pelo mesmo número de `b`s.

Esta linguagem não é regular. A tentativa de construir um autômato finito para reconhecê-la falha porque a máquina precisaria de uma quantidade infinita de estados para _lembrar_ o número exato de `a`s lidos e garantir que o número de `b`s corresponda. Uma prova formal de que $L$ não é uma linguagem regular pode ser rigorosamente construída usando o Lema do Bombeamento para linguagens regulares. Este lema afirma que, para qualquer _string_ suficientemente longa em uma linguagem regular, existe uma sub_string_ que pode ser _bombeada_ (repetida um número arbitrário de vezes) e a nova _string_ resultante ainda pertencerá à linguagem. Ao aplicar o lema à _string_ com $p$ 'a's seguidos de $p$ 'b's é a $a^pb^p$, no qual $p$ é o comprimento de bombeamento, a sub_string_ bombeada consistirá inteiramente de `a`s, quebrando o equilíbrio entre `a`s e `b`s e provando que a linguagem não pode ser regular. No entanto, $L$ é uma linguagem livre de contexto por excelência. Ela pode ser gerada pela gramática extremamente simples e elegante:

$$S \rightarrow aSb \mid \epsilon$$

Nesta gramática o símbolo $\epsilon$ representa a _string_ vazia. A regra recursiva $S \rightarrow aSb$ encapsula perfeitamente a capacidade de aninhamento que define as Linguagens Livres de Contexto. Outros exemplos práticos que exigem o poder das Linguagens Livres de Contexto incluem a linguagem dos palíndromos (ex.: _radar_) e a validação estrutural de documentos XML, no qual as _tags_ de abertura e fechamento devem ser corretamente aninhadas, uma tarefa impossível para expressões regulares.

A atenta leitora deve observar que essa progressão não é apenas um exercício teórico. A necessidade prática de analisar a sintaxe de linguagens de programação, que são repletas de construções aninhadas como laços, condicionais e chamadas de função, é a força motriz que torna as linguagens regulares insuficientes e as linguagens livres de contexto absolutamente essenciais para a ciência da computação. A Tabela #tbl-resumo1 condensa as diferenças entre linguagens regulares e linguagens livres de contexto.

| Característica | Linguagens Regulares (Tipo 3) | Linguagens Livres de Contexto (Tipo 2) |
| :---- | :---- | :---- |
| **Tipo de Gramática** | Gramática Regular | Gramática Livre de Contexto |
| **Formato das Regras** | Restrito (ex.: $A \rightarrow aB$ ou $A\rightarrow a$) | Irrestrito (ex.: $A \rightarrow \beta$, na qual $\beta$ é qualquer _string_) |
| **Autômato Reconhecedor** | Autômato Finito | Autômato com Pilha |
| **Capacidade de _Memória_** | Nenhuma (limitada a estados finitos) | Ilimitada (via pilha LIFO) |
| **Exemplo Característico** | $a(ba)^∗$ | $a^nb^n$ para $n \geq 0$ |
| **Exemplo Não-Pertencente** | $a^nb^n$ para $n \geq 0$ | $a^nb^nc^n$ para $n \geq 0$ |

: Resumo das características e diferenças das linguagens regulares e livres de contexto.{#tbl-resumo1}

## A Anatomia das Gramáticas Livres de Contexto {#sec-anatomia-gramaticas-livres-de-contexto}

A atenta leitora verá que para analisar e processar linguagens com estruturas aninhadas, é imprescindível o uso de um formalismo matemático preciso. Uma Gramática Livre de Contexto fornece a base formal necessária.

O termo _livre de contexto_ é fundamental para a compreensão da natureza das gramáticas que a atenta leitora estudará neste capítulo. Esse termo significa que a aplicação de uma regra de produção $A\rightarrow \beta$ a um símbolo não-terminal $A$ é incondicional; ela pode ocorrer independentemente dos símbolos que cercam $A$, seu _contexto_, em uma forma sentencial intermediária. Esta propriedade simplifica a análise sintática em comparação com gramáticas mais complexas, como as sensíveis ao contexto (Tipo 1).

Formalmente, uma Gramática Livre de Contexto é definida como uma quádrupla $G=(N,\Sigma,P,S)$, na qual cada componente é um conjunto e tem um papel específico na geração das _strings_  da linguagem. Os quatro componentes da tupla que define uma Gramática Livre de Contexto são:

1. **N: O Conjunto de Símbolos Não Terminais.** Este é um conjunto finito de variáveis que representam as diferentes construções sintáticas ou categorias gramaticais da linguagem. Por exemplo, em uma gramática para uma linguagem de programação, os não terminais podem incluir EXPRESSÃO, COMANDO e DECLARAÇÃO. Eles são os elementos que podem ser substituídos ou expandidos durante o processo de derivação e que aqui, neste livro, serão representados por letras maiúsculas do alfabeto latino.

2. **$\Sigma$: O Conjunto de Símbolos Terminais.** Este é um conjunto finito, disjunto de *N*, que constitui o alfabeto da linguagem. Os terminais são os símbolos literais, os _átomos_ que compõem as _strings_  finais da linguagem e não podem ser mais decompostos. Exemplos incluem palavras-chave (`if`, `while`, etc.), operadores (`+`, `\`, `*`, etc.) e identificadores (`contador`, `salario`, etc.).

3. **P: O Conjunto de Regras de Produção.** Este é um conjunto finito de regras que definem como os não terminais podem ser substituídos. Cada regra tem a forma $A \rightarrow β$, na qual $A\in N$ é um único não-terminal (a _cabeça_ da produção) e $\beta \in (N∪Σ)^∗$ é uma _string_, possivelmente vazia, de símbolos terminais e/ou não terminais que chamaremos de _corpo_ da produção. Essas regras são o motor do sistema gerativo.

4. **S: O Símbolo Inicial.** Um não-terminal especial, $S \in N$, que serve como ponto de partida para todas as derivações. Ele geralmente representa a construção sintática mais abrangente da linguagem, como um programa ou uma sentença.

A linguagem gerada por uma gramática $G$, denotada por $L(G)$, é o conjunto de todas as _strings_  de símbolos terminais que podem ser derivadas a partir do símbolo inicial $S$ por meio da aplicação sucessiva das regras de produção em $P$.

Embora a definição formal de uma Gramática Livre de Contexto seja inerentemente gerativa, descrevendo como construir sentenças válidas a partir de $S$, sua aplicação primária em compiladores é reconhecedora. O objetivo de um analisador sintático não é gerar programas aleatórios, mas sim verificar se uma dada sequência de _tokens_, produzida pelo programador, pode ser gerada pela gramática. A gramática, portanto, atua como a especificação formal contra a qual o processo de reconhecimento é executado. O parser, na prática, tenta reverter o processo de derivação para validar a estrutura do programa fonte.

### Exemplo Canônico: A Linguagem dos Palíndromos

Para solidificar esses conceitos abstratos, vamos construir uma Gramática Livre de Contexto para a linguagem dos palíndromos sobre o alfabeto $\Sigma = \{0,1\}$. Só para refrescar a memória: um palíndromo é uma _string_ que se lê da mesma forma da esquerda para a direita e da direita para a esquerda.

A estrutura recursiva dos palíndromos pode ser definida da seguinte forma:

1. **Casos Base**: A _string_ vazia ($\epsilon$), `0` e `1` são palíndromos. 
2. **Passo Indutivo**: Se $w$ é um palíndromo, então $0w0$ e $1w1$ também são palíndromos.

Esta definição recursiva pode ser traduzida diretamente em um conjunto de regras de produção para uma Gramática Livre de Contexto. Seja $K$ o nosso símbolo não-terminal para _palíndromo_:

1. $K \rightarrow \epsilon$  
2. $K \rightarrow 0$  
3. $K \rightarrow 1$  
4. $K \rightarrow 0K0$  
5. $K \rightarrow 1K1$

Neste exemplo, a gramática completa $G_{pal}$ é definida pela quádrupla:

* $N = \{K\}$
* $\Sigma = \{0,1\}$
* $P$ é o conjunto das cinco regras listadas acima.
* $S = K$

Esta gramática pode gerar qualquer palíndromo sobre $\{0,1\}$. Por exemplo, a _string_ _0110_ pode ser derivada da seguinte forma: 

A derivação da _string_ `0110` a partir do símbolo inicial $K$ é feita aplicando-se as regras da gramática sequencialmente.

1. **Passo 1: Iniciar com o símbolo inicial.**
    * Começamos com o símbolo inicial da gramática, que é $K$.
    * $K$

2. **Passo 2: Aplicar a Regra 4 ($K \rightarrow 0K0$).**
    * Para gerar uma _string_ que começa e termina com `0`, aplicamos a regra 4.
    * $K \Rightarrow 0K0$

3. **Passo 3: Aplicar a Regra 5 ($K \rightarrow 1K1$).**
    * Agora, precisamos gerar a parte interna do palíndromo. Substituímos o $K$ restante pela regra 5 para obter os `1`s internos.
    * $0K0 \Rightarrow 0(1K1)0 = 01K10$

4. **Passo 4: Aplicar a Regra 1 ($K \rightarrow \epsilon$).**
    * O centro do palíndromo `0110` é vazio. Para finalizar a derivação, substituímos o último $K$ pela cadeia vazia, $\epsilon$ (epsilon), usando a regra 1.
    * $01K10 \Rightarrow 01(\epsilon)10 = 0110$

A sequência completa da derivação será:

$$K \Rightarrow 0K0 \Rightarrow 01K10 \Rightarrow 01\epsilon10 \Rightarrow 0110$$

### Exercícios de Derivação {#sec-execicios-derivacao}

#### Exercício 1: Palíndromo Ímpar

Dada a gramática de palíndromos $G_{pal}$:

- $N = \{K\}$
- $\Sigma = \{0,1\}$
- $P = \{ K \rightarrow \epsilon, K \rightarrow 0, K \rightarrow 1, K \rightarrow 0K0, K \rightarrow 1K1 \}$
- $S = K$

Faça a derivação da *_string_* `101`.

#### Exercício 2: Expressão Aritmética Simples

Considere uma gramática simplificada para expressões aritméticas, $G_{exp}$ dada por:

* $N = \{E\}$
* $\Sigma = \{id, +, *, (, )\}$
* $P = \{ E \rightarrow E + E, E \rightarrow E * E, E \rightarrow (E), E \rightarrow id \}$
* $S = E$

Faça a derivação da *_string_* `id * id + id`.

#### Exercício 3: Palíndromo de Comprimento Par e Aninhado

Usando a gramática de palíndromos $G_{pal}$ dada a seguir, faça a derivação da *_string_* `011110`.

- $N = \{K\}$
- $\Sigma = \{0,1\}$
- $P = \{ K \rightarrow \epsilon, K \rightarrow 0, K \rightarrow 1, K \rightarrow 0K0, K \rightarrow 1K1 \}$
- $S = K$

#### Exercício 4: Linguagem $a^nb^n$

Considere a gramática $G_{ab}$ que gera *_strings_* com um número de `a``s seguido pelo mesmo número de `b``s:

* $N = \{S\}$
* $\Sigma = \{a, b\}$
* $P = \{ S \rightarrow aSb, S \rightarrow \epsilon \}$
* $S = S$

Faça a derivação da *_string_* `aaabbb`.

#### Exercício 5: Comando Condicional `if-else`

Seja uma gramática para um comando `if-else` simplificado, $G_{if}$:

* $N = \{C, A\}$
* $\Sigma = \{ \text{if}, \text{then}, \text{else}, id, :=, 0 \}$
* $P = \{ C \rightarrow \text{if } id \text{ then } A \text{ else } A, A \rightarrow id := 0 \}$
* $S = C$

Faça a derivação da *_string_* `if id then id := 0 else id := 0`.

#### Exercício 6: Parênteses Balanceados

Considere a gramática $G_{par}$ para gerar sequências de parênteses balanceados:

* $N = \{B\}$
* $\Sigma = \{ (, ) \}$
* $P = \{ B \rightarrow (B), B \rightarrow BB, B \rightarrow \epsilon \}$
* $S = B$

Faça a derivação da *_string_* `()(())`.

#### Exercício 7: Palíndromo Vazio

Usando a gramática de palíndromos $G_{pal}$ do primeiro exercício, faça a derivação da *_string_* vazia, $\epsilon$.

## A Notação $BNF$ {#sec-notacao-BNF}

A **F**orma de **B**ackus-**N**aur, universalmente conhecida pela sigla **$BNF$**, representa um dos sistemas de notação mais elegantes e influentes da ciência da computação teórica. Desenvolvida no final da década de 1950 para descrever formalmente a sintaxe da linguagem de programação **ALGOL 60**, a $BNF$ tornou-se o padrão _de facto_ para especificar a sintaxe de linguagens de programação e protocolos de comunicação.

### Contexto Histórico e Desenvolvimento

A criação da $BNF$ emergiu de uma necessidade prática urgente durante o desenvolvimento de **ALGOL 60**. **[John Backus](https://en.wikipedia.org/wiki/John_Backus)**, líder do projeto, enfrentava o desafio de descrever a sintaxe da linguagem de forma precisa e não ambígua para uma audiência internacional de implementadores. As descrições informais em linguagem natural mostravam-se inadequadas, gerando interpretações divergentes e implementações incompatíveis.

A solução veio através da colaboração entre Backus e **[Peter Naur](https://en.wikipedia.org/wiki/Peter_Naur)**, que refinou e popularizou a notação. Naur, como editor do relatório **ALGOL 60**, foi responsável por sistematizar e formalizar o que inicialmente eram esboços conceituais de Backus. A notação resultante combinava a precisão matemática das gramáticas formais com uma legibilidade que a tornava acessível tanto para teóricos quanto para implementadores práticos.

A $BNF$ foi desenvolvida com conhecimento explícito dos trabalhos de Chomsky sobre Gramáticas Livres de Contexto. Backus e Naur compreenderam que estavam essencialmente criando uma notação prática para expressar gramáticas do Tipo 2 da Hierarquia de Chomsky, estabelecendo assim uma ponte fundamental entre a teoria formal e a aplicação industrial.

### Estrutura e Componentes da Notação $BNF$

A $BNF$ define uma gramática através de um conjunto de **regras de produção** (ou **regras de reescrita**), na qual cada regra especifica como um símbolo não-terminal pode ser expandido em uma sequência de símbolos terminais e não-terminais. A estrutura básica de uma regra $BNF$ segue o padrão:

```bnf
<símbolo-não-terminal> ::= expressão-alternativa
```

Os elementos fundamentais da notação $BNF$ são:

1. **Símbolos Não-terminais**: representados entre colchetes angulares (`< >`), estes são os elementos que podem ser definidos por outras regras. Exemplos: `<expressão>`, `<comando>`, `<identificador>`.

2. **Símbolos Terminais**: representam os elementos atômicos da linguagem - palavras-chave, operadores, delimitadores. São escritos literalmente, frequentemente entre aspas. Exemplos: `"if"`, `"+"`, `";"`.

3. **Operador de Definição** (`::=`): lê-se "é definido como" ou "pode ser substituído por". Estabelece a relação entre o lado esquerdo (não-terminal) e o lado direito (expressão de definição).

4. **Operador de Alternativa** (`|`): indica escolhas mutuamente exclusivas. Lê-se "ou". Permite múltiplas definições para um mesmo não-terminal.

5. **Agrupamento**: parênteses são utilizados para agrupar elementos e controlar a precedência das operações.

### Exemplo Fundamental: Expressões Aritméticas Simples

Para ilustrar a aplicação prática da $BNF$, consideremos uma gramática para expressões aritméticas básicas que inclui adição, multiplicação, parênteses e identificadores:

```bnf
<expressão>     ::= <termo> "+" <expressão> | <termo>
<termo>         ::= <fator> "*" <termo> | <fator>
<fator>         ::= "(" <expressão> ")" | <identificador>
<identificador> ::= <letra> | <letra> <identificador>
<letra>         ::= "a" | "b" | "c" | "d" | "e" | "f" | "g" | "h" | 
                    "i" | "j" | "k" | "l" | "m" | "n" | "o" | "p" | 
                    "q" | "r" | "s" | "t" | "u" | "v" | "w" | "x" | 
                    "y" | "z"
```

Esta gramática demonstra características essenciais da $BNF$:

- **Hierarquia de Precedência**: a estrutura `<expressão>` → `<termo>` → `<fator>` estabelece que multiplicação tem precedência sobre adição.
- **Recursão**: `<expressão>` referencia a si mesma, permitindo expressões de comprimento arbitrário.
- **Associatividade**: a recursão à direita em `<termo> "+" <expressão>` define associatividade à esquerda para adição.

Utilizando a gramática acima, podemos derivar a expressão `a + b * c`:

```bnf
<expressão>
→ <termo> "+" <expressão>
→ <fator> "+" <expressão>
→ <identificador> "+" <expressão>
→ <letra> "+" <expressão>
→ "a" "+" <expressão>
→ "a" "+" <termo>
→ "a" "+" <fator> "*" <termo>
→ "a" "+" <identificador> "*" <termo>
→ "a" "+" <letra> "*" <termo>
→ "a" "+" "b" "*" <termo>
→ "a" "+" "b" "*" <fator>
→ "a" "+" "b" "*" <identificador>
→ "a" "+" "b" "*" <letra>
→ "a" "+" "b" "*" "c"
```

Esta derivação demonstra como a gramática força a interpretação `a + (b * c)`, respeitando a precedência de operadores.

:::{.callout-tip}
**Extensões da $BNF$**

A $BNF$ original foi posteriormente estendida para aumentar sua expressividade e conveniência:

- **$EBNF$ (Extended $BNF$)**: introduz operadores adicionais como `{}` para repetição (zero ou mais), `[]` para elementos opcionais, e `()` para agrupamento explícito.

- **$ABNF$ (Augmented $BNF$)**: usado em RFCs da Internet, adiciona suporte para especificação de repetições numéricas e valores em diferentes bases.

Exemplo em EBNF:

```ebnf
identificador = letra, { letra | dígito };
letra = "a" | "b" | ... | "z";
dígito = "0" | "1" | ... | "9";
```

Esta notação é equivalente à $BNF$ original, mas significativamente mais concisa.
:::

### Comparação com Gramáticas Livres de Contexto

A $BNF$ é essencialmente uma notação alternativa para Gramáticas Livres de Contexto. A correspondência é direta, como pode ser visto na @tbl-compara2:

| Elemento GLC | Notação $BNF$ | Exemplo |
|:-------------|:------------|:--------|
| Não-terminal | `<símbolo>` | `<expressão>` |
| Terminal | Literal ou entre aspas | `"if"`, `+` |
| Regra de produção | `::=` | `<E> ::= <E> + <T>` |
| Alternativas | `\|` | `<E> ::= <T> \| <E> + <T>` |

: Correspondência entre elementos de Gramáticas Livres de Contexto e notação $BNF$. {#tbl-compara2}

### Aplicações e Influência

A $BNF$ estabeleceu um precedente que influenciou profundamente o desenvolvimento de linguagens de programação e ferramentas de compilação:

**Especificação de Linguagens**: praticamente todas as linguagens modernas utilizam variações da $BNF$ para definir sintaxe formal. Exemplos incluem C, Pascal, Ada, e mais recentemente, linguagens como Python e Rust.

**Geradores de Parsers**: ferramentas como YACC, Bison, ANTLR e muitas outras utilizam sintaxe baseada em $BNF$ como entrada, traduzindo especificações declarativas em código de análise sintática.

**Protocolos de Rede**: muitos [RFCs](https://en.wikipedia.org/wiki/Request_for_Comments) da Internet utilizam $ABNF$ para especificar protocolos como HTTP, SMTP, e URI.

**Documentação Técnica**: A $BNF$ tornou-se linguagem franca para comunicar especificações sintáticas de forma precisa e não ambígua.

A elegância da $BNF$ reside em sua capacidade de tornar formal acessível. Ela democratizou a especificação rigorosa de linguagens, permitindo que desenvolvedores sem formação profunda em teoria formal pudessem criar e comunicar especificações precisas. Esta contribuição para a prática da engenharia de software não pode ser subestimada: a $BNF$ facilitou o desenvolvimento sistemático e a interoperabilidade de linguagens de programação, estabelecendo as fundações sobre as quais repousa muito do ecossistema computacional contemporâneo.

## Geração de Sentenças: Derivações, Árvores e Ambiguidade

Uma **derivação** é a sequência de passos que transforma o símbolo inicial em uma _string_ final de terminais por meio da aplicação das regras de produção. Exatamente o processo que a esforçada leitora aprendeu na seção @sec-anatomia-gramaticas-livres-de-contexto. Em cada passo, um não-terminal é escolhido e substituído pelo corpo de uma da regras de produção deste não-terminal. Como uma forma sentencial intermediária pode conter múltiplos não terminais, precisaremos de uma  convenção para determinar qual deles expandir. Isso leva a duas estratégias de derivação:

1. **Derivação Mais à Esquerda (*Leftmost Derivation*)**: em cada passo, o símbolo não-terminal que aparece mais à esquerda na forma sentencial é sempre o escolhido para ser substituído.  
2. **Derivação Mais à Direita (*Rightmost Derivation*)**: em cada passo, o símbolo não-terminal que aparece mais à direita é o escolhido para ser substituído.

Para uma gramática não ambígua, embora as sequências de passos sejam diferentes, tanto a derivação mais à esquerda quanto a mais à direita para uma dada sentença resultarão na mesma estrutura sintática. Como exemplo, a atenta leitora pode estudar o exemplo a seguir:

**Exemplo 1**: Gramática Não Ambígua para Expressões Aritméticas

Considere a seguinte gramática:

* **Símbolos não-terminais (N)**: $\{E, T, F\}$
* **Símbolos terminais (\Sigma)**: $\{id, +, *, (, )\}$
* **Símbolo inicial (S)**: $E$
* **Regras de produção (P)**:
  1. $E \rightarrow E + T$
  2. $E \rightarrow T$
  3. $T \rightarrow T * F$
  4. $T \rightarrow F$
  5. $F \rightarrow (E)$
  6. $F \rightarrow id$

Vamos fazer a Derivação da _string_ "id + id * id", primeiro usando a derivação Mais à Esquerda (_Leftmost_):

$$
\begin{align}
E &\Rightarrow E + T                &&\text{[regra 1: expandir E]}\
  &\Rightarrow T + T                &&\text{[regra 2: expandir E mais à esquerda]}\
  &\Rightarrow F + T                &&\text{[regra 4: expandir T mais à esquerda]}\
  &\Rightarrow id + T               &&\text{[regra 6: expandir F mais à esquerda]}\
  &\Rightarrow id + T * F           &&\text{[regra 3: expandir T]}\
  &\Rightarrow id + F * F           &&\text{[regra 4: expandir T mais à esquerda]}\
  &\Rightarrow id + id * F          &&\text{[regra 6: expandir F mais à esquerda]}\
  &\Rightarrow id + id * id         &&\text{[regra 6: expandir F]}
\end{align}
$$

Agora vamos derivar a mesma _string_ usando a derivação Mais à Direita (_Rightmost_):

$$
\begin{align}
E &\Rightarrow E + T                &&\text{[regra 1: expandir E]}\
  &\Rightarrow E + T * F            &&\text{[regra 3: expandir T mais à direita]}\
  &\Rightarrow E + T * id           &&\text{[regra 6: expandir F mais à direita]}\
  &\Rightarrow E + F * id           &&\text{[regra 4: expandir T mais à direita]}\
  &\Rightarrow E + id * id          &&\text{[regra 6: expandir F mais à direita]}\
  &\Rightarrow T + id * id          &&\text{[regra 2: expandir E]}\
  &\Rightarrow F + id * id          &&\text{[regra 4: expandir T]}\
  &\Rightarrow id + id * id         &&\text{[regra 6: expandir F]}
\end{align}
$$

Observando as duas derivações é possível perceber que a gramática usada neste exemplo, não é ambígua. Observe que:

1. **Precedência clara**: A multiplicação $(*)$ tem precedência maior que a adição $(+)$
   * $T$ (termo) gera multiplicações
   * $E$ (expressão) gera adições de termos

2. **Associatividade definida**: Operadores associam à esquerda
   * $E \rightarrow E + T$ (não $E \rightarrow T + E$)
   * $T \rightarrow T * F$ (não $T \rightarrow F * T$)

3. **Estrutura única**: Para qualquer _string_ válida, existe exatamente uma árvore de derivação,
   independentemente da ordem de derivação (leftmost ou rightmost).

A atenta leitora deve notar que embora a sequência de passos seja diferente nas duas derivações, ambas produzem a mesma estrutura sintática: $id + (id * id)$, na qual a multiplicação tem precedência sobre a adição.

### A Construção de Árvores de Derivação

O processo mais intuitivo de visualizar a estrutura hierárquica imposta por uma gramática a uma sentença é por meio de uma **árvore de derivação**, ou **árvore sintática**. Uma árvore de derivação é uma representação gráfica de uma derivação que abstrai a ordem em que as produções foram aplicadas. Suas propriedades são definidas por:

1. A raiz da árvore é rotulada com o símbolo inicial $S$.
2. Cada nó interno é rotulado com um símbolo não-terminal.
3. Cada folha é rotulada com um símbolo terminal ou com $\epsilon$.
4. Se um nó interno é rotulado com $A$ e seus filhos, da esquerda para a direita, são rotulados com $X_1$, $X_2$, ..., $X_n$, então deve existir uma regra de produção $A \rightarrow X_1 X_2 ... X_n$ na gramática.

A concatenação das folhas da árvore, lidas da esquerda para a direita, forma a sentença gerada, também conhecida como _yield_ da árvore. A árvore de derivação captura a estrutura sintática essencial da sentença, tornando explícitas as relações entre suas subpartes.

**Exemplo 2:** Gramática Não Ambígua para Listas

Considere a gramática definida como:

- **Símbolos não-terminais (N)**: $\{L, E\}$
- **Símbolos terminais (Σ)**: $\{a, b, [, ], ,\}$
- **Símbolo inicial (S)**: $L$
- **Regras de produção (P)**:
  1. $L \rightarrow [E]$
  2. $L \rightarrow [\,]$
  3. $E \rightarrow E, a$
  4. $E \rightarrow E, b$
  5. $E \rightarrow a$
  6. $E \rightarrow b$

Vamos derivar a _string_ "[a, b, a]". Primeiro com a Derivação Mais à Esquerda (_Leftmost_):

$$
\begin{align}
L &\Rightarrow [E]                  &&\text{[regra 1: expandir L]}\\
  &\Rightarrow [E, a]               &&\text{[regra 3: expandir E]}\\
  &\Rightarrow [E, b, a]            &&\text{[regra 4: expandir E mais à esquerda]}\\
  &\Rightarrow [a, b, a]            &&\text{[regra 5: expandir E mais à esquerda]}
\end{align}
$$

Agora com a Derivação Mais à Direita (_Rightmost_):

$$
\begin{align}
L &\Rightarrow [E]                  &&\text{[regra 1: expandir L]}\\
  &\Rightarrow [E, a]               &&\text{[regra 3: expandir E]}\\
  &\Rightarrow [E, b, a]            &&\text{[regra 4: expandir E]}\\
  &\Rightarrow [a, b, a]            &&\text{[regra 5: expandir E]}
\end{align}
$$

A @fig-deriva1 apresenta duas árvores geradas por dois processos de derivação, um à esquerda e outro à direita, para a _string_ "[a, b, a]". A atenta leitora deve observar que as árvores são iguais.

:::{#fig-deriva1}
![](images/deriva1.webp)

Apresentação das árvores de derivação à esquerda e à direita da _string_ "[a, b, a]" de forma gráfica.
:::

**Exemplo 3**: Considerando a gramática do Exemplo 2, faça a derivação da _string_ "[b]"

Novamente, começando com a Derivação Mais à Esquerda (_Leftmost_):

$$
\begin{align}
L &\Rightarrow [E]                  &&\text{[regra 1: expandir L]}\\
  &\Rightarrow [b]                  &&\text{[regra 6: expandir E]}
\end{align}
$$

Finalmente, a Derivação Mais à Direita (Rightmost):

$$
\begin{align}
L &\Rightarrow [E]                  &&\text{[regra 1: expandir L]}\\
  &\Rightarrow [b]                  &&\text{[regra 6: expandir E]}
\end{align}
$$

A atenta leitora deve perceber que esta gramática não é ambígua. Considerando que:

1. **Estrutura hierárquica clara**:
   * $L$ gera apenas a estrutura de lista com colchetes;
   * $E$ gera apenas a sequência de elementos separados por vírgula.

2. **Associatividade única**:
   * As regras $E \rightarrow E, a$ e $E \rightarrow E, b$ forçam associatividade à esquerda;
   * Não há regras como $E \rightarrow a, E$ que criariam ambiguidade.

3. **Sem sobreposição de produções**:
   * Cada não-terminal tem um papel específico e não conflitante;
   * Lista vazia $[\,]$ é tratada separadamente, evitando ambiguidade.

**Nota**: A gramática usada nos exemplos 2 e 3 garante que elementos são adicionados sempre à direita da lista, construindo-a da esquerda para a direita. A estrutura $[a, b, a]$ só pode ser interpretada de uma forma: uma lista contendo três elementos na ordem especificada.

## O Problema da Ambiguidade em Gramáticas Formais

**Uma gramática G = (V, T, P, S) é ambígua** se e somente se existe pelo menos uma cadeia $w \in L(G)$ que possui duas ou mais árvores de derivação distintas. Em termos práticos, isto quer dizer que a mesma sentença pode ser construída através de diferentes sequências de aplicação de regras de produção. A existência de diferentes estruturas sintáticas implica na existência de interpretações semânticas diferentes. Neste caso, o parser não consegue determinar univocamente qual estrutura sintática representa a intenção do programador. 

**Exemplo 1**: Considerando a gramática $G_1$, mostre que a sentença `id + id * id` é ambígua.

$$G_1 = (\{E\}, \{+, *, (, ), id\}, P, E)$$

Na qual  $P$ consiste das produções:

$$
\begin{align}
E &\Rightarrow E + E \\
E &\Rightarrow E * E \\
E &\Rightarrow (E) \\
E &\Rightarrow id
\end{align}
$$

Analisando a sequência `id + id * id`, podemos observar que esta _string_ pode ser interpretada de duas maneiras diferentes, dependendo da ordem em que as operações são realizadas:

1. Derivação 1 (Multiplicação com maior precedência):

  $$
  \begin{align}
  E &\Rightarrow E + E \\
  &\Rightarrow id + E \\
  &\Rightarrow id + E * E \\
  &\Rightarrow id + id * E \\
  &\Rightarrow id + id * id
  \end{align}
  $$

  O resultado da primeira derivação permite que a _string_ `id + id * id` seja interpretada como `id + (id * id)`.

2. Derivação 2 (Adição com maior precedência):

  $$
  \begin{align}
  E &\Rightarrow E * E \\
    &\Rightarrow E + E * E \\
    &\Rightarrow id + E * E \\
    &\Rightarrow id + id * E \\
    &\Rightarrow id + id * id
  \end{align}
  $$

  O resultado da segunda derivação permite que a _string_ `id + id * id` seja interpretada como `(id + id) * id`.

### Características e Taxonomia da Ambiguidade

A ambiguidade em gramáticas formais manifesta-se de diferentes formas e com distintos níveis de penetração na estrutura da linguagem:

A **ambiguidade local** representa conflitos que afetam construções sintáticas específicas e bem delimitadas dentro da gramática. O exemplo mais emblemático desta categoria são os problemas de precedência de operadores, nos quais a gramática permite múltiplas interpretações para expressões como `id + id * id`. Neste caso, a ambiguidade está circunscrita às regras que governam operadores aritméticos e pode ser resolvida através da reescrita da gramática ou da especificação explícita de precedência e associatividade.

A **ambiguidade global**, por outro lado, permeia toda a estrutura da gramática e afeta a arquitetura fundamental da linguagem. O problema clássico do _dangling else_ exemplifica perfeitamente esta categoria. Quando uma construção como `if (c1) if (c2) s1 else s2` pode ser interpretada de duas formas distintas, a ambiguidade não está limitada a uma regra específica, mas emerge da interação complexa entre múltiplas produções que definem comandos condicionais aninhados.

As manifestações da ambiguidade podem ser classificadas em três categorias fundamentais que refletem diferentes aspectos da análise sintática.

A **ambiguidade de precedência** surge quando operadores de diferentes níveis hierárquicos não possuem uma ordem de avaliação claramente definida pela gramática. Esta forma de ambiguidade é particularmente comum em linguagens que incluem operadores aritméticos, lógicos e relacionais. A ausência de uma hierarquia bem estabelecida força o parser a tomar decisões arbitrárias sobre qual operação deve ser executada primeiro, podendo resultar em interpretações semânticas drasticamente diferentes.

A **ambiguidade de associatividade** manifesta-se quando operadores do mesmo nível de precedência podem ser agrupados tanto à esquerda quanto à direita, alterando o resultado da computação. Por exemplo, em uma expressão como `a - b - c`, a associatividade à esquerda produz `(a - b) - c`, enquanto a associatividade à direita resulta em `a - (b - c)`. Embora matematicamente equivalentes para operações comutativas como adição, essa distinção torna-se crítica para operações não-comutativas.

A **ambiguidade estrutural** representa a forma mais complexa e desafiadora de ambiguidade, na qual diferentes agrupamentos de construções sintáticas produzem estruturas hierárquicas distintas. Esta categoria transcende questões operacionais e atinge o núcleo da organização sintática da linguagem, afetando como blocos de código, estruturas de controle e definições de escopo são interpretados pelo compilador.

**Exemplo 2**: o Problema do _Dangling Else_

Considere a gramática $G_2$ (Ambígua para `if-then-else`) definida por:

$$G_2 = (\{S, E\}, \{if, then, else, (, ), c, s\}, P, S)$$

Na qual $P$ consiste das produções:

$$
\begin{align}
S &\Rightarrow if (E) S \\
&\Rightarrow if (E) S else S \\
&\Rightarrow s \\
E &\Rightarrow c
\end{align}
$$

Para entender a ambiguidade vamos analisar a sentença `if (c1) if (c2) s1 else s2`. 

1. Nesta interpretação, a estrutura `if (c2) s1 else s2` é tratada como um único comando $S$, que por sua vez está aninhado dentro do primeiro `if`. A derivação ocorre da seguinte forma:

$$
\begin{align}
S &\Rightarrow \text{if } (E) S \\
&\Rightarrow \text{if } (c_1) S \\
&\Rightarrow \text{if } (c_1) \text{ if } (E) S \text{ else } S \\
&\Rightarrow \text{if } (c_1) \text{ if } (c_2) S \text{ else } S \\
&\Rightarrow \text{if } (c_1) \text{ if } (c_2) s_1 \text{ else } S \\
&\Rightarrow \text{if } (c_1) \text{ if } (c_2) s_1 \text{ else } s_2
\end{align}
$$

Resultando em:

```
if (c1) {
    if (c2)
        s1
    else
        s2
}
```

2. Nesta interpretação, a estrutura `if (c2) s1` é o corpo do `if` principal, e o `else s2` está associado a este `if` mais externo.

$$
\begin{align}
S &\Rightarrow \text{if } (E) S \text{ else } S \\
&\Rightarrow \text{if } (c_1) S \text{ else } S \\
&\Rightarrow \text{if } (c_1) \text{ if } (E) S \text{ else } S \\
&\Rightarrow \text{if } (c_1) \text{ if } (c_2) S \text{ else } S \\
&\Rightarrow \text{if } (c_1) \text{ if } (c_2) s_1 \text{ else } S \\
&\Rightarrow \text{if } (c_1) \text{ if } (c_2) s_1 \text{ else } s_2
\end{align}
$$

```
if (c1) {
    if (c2)
        s1
}
else
    s2
```

Como existem duas derivações à esquerda distintas para a mesma sentença, a gramática $G_2$ é formalmente ambígua.

## Técnicas de Eliminação de Ambiguidade

A atenta leitora deve compreender que a ambiguidade em uma gramática não é uma fatalidade irreversível. Ao longo do desenvolvimento da teoria das linguagens formais, foram desenvolvidas técnicas sistemáticas para transformar gramáticas ambíguas em equivalentes não-ambíguas, preservando a linguagem gerada. Estas técnicas representam não apenas exercícios teóricos, mas ferramentas práticas essenciais no projeto de compiladores modernos.

### Reescrita Sistemática da Gramática

A técnica mais fundamental para eliminar ambiguidade consiste na **reescrita estruturada da gramática**. Esta abordagem requer uma análise cuidadosa das fontes de ambiguidade e sua eliminação através da introdução de níveis hierárquicos explícitos na estrutura gramatical.

Considere novamente a gramática ambígua $G_1$ para expressões aritméticas:

$$G_1 = (\{E\}, \{+, *, (, ), id\}, P_1, E)$$

Na qual $P_1$ consiste das produções:

* $E \rightarrow E + E$
* $E \rightarrow E * E$
* $E \rightarrow (E)$
* $E \rightarrow id$

A transformação desta gramática em sua versão não-ambígua $G_3$ segue um princípio arquitetural elegante: **a estratificação sintática**. Cada nível de precedência é codificado como um não-terminal distinto:

$$G_3 = (\{E, T, F\}, \{+, *, (, ), id\}, P_3, E)$$

Na qual $P_3$ consiste das produções:

* $E \rightarrow E + T \mid T$
* $T \rightarrow T * F \mid F$
* $F \rightarrow (E) \mid id$

Esta hierarquia de não-terminais estabelece inequivocamente que:

1. **Precedência operacional**: A multiplicação, governada por $T$, tem precedência sobre a adição, governada por $E$. Uma expressão como `id + id * id` só pode ser derivada como `id + (id * id)`.

2. **Associatividade uniforme**: Ambos operadores associam à esquerda devido à recursão à esquerda nas produções $E \rightarrow E + T$ e $T \rightarrow T * F$. A expressão `a + b + c` é interpretada como `(a + b) + c`.

3. **Preservação da linguagem**: $L(G_1) = L(G_3)$, ou seja, ambas gramáticas geram exatamente o mesmo conjunto de _strings_, diferindo apenas na estrutura de derivação.

### Declarações Explícitas de Precedência e Associatividade

O desenvolvimento de **geradores de parsers** como [YACC](https://pubs.opengroup.org/onlinepubs/9699919799/utilities/yacc.html) (**Y**et **A**nother **C**ompiler-**C**ompiler) e seu sucessor [GNU Bison](https://www.gnu.org/software/bison/) introduziu uma abordagem pragmática: permitir que o desenvolvedor especifique diretamente as regras de desambiguação sem reescrever a gramática.

```
%left '+'     /* menor precedência, associa à esquerda */
%left '*'     /* precedência intermediária */
%right '^'    /* maior precedência, associa à direita */
%%
expr: expr '+' expr
| expr '*' expr
| expr '^' expr
| '(' expr ')'
| ID;
```

Estas declarações instruem o gerador de parser a resolver conflitos _shift_/_reduce_ de forma determinística. Quando confrontado com `a + b * c`, o parser saberá que deve realizar o shift do `*` (maior precedência) antes de reduzir a adição. Assim, os eventuais problemas de ambiguidade decorrentes da gramática original podem ser resolvidos.

#### Resolução do Problema do *Dangling Else*

Para a ambiguidade estrutural do *dangling else*, a solução canônica adotada pela maioria das linguagens modernas é a **convenção do else mais próximo**. Esta pode ser formalizada através de uma gramática cuidadosamente estruturada:

$$G_{if} = (\{S, M, U\}, \{if, else, (, ), c, s\}, P_{if}, S)$$

na qual $P_{if}$ consiste das seguintes produções:

* $S \rightarrow M \mid U$;
* $M \rightarrow \text{if } (E) \text{ } M \text{ else } M \mid s$;
* $U \rightarrow \text{if } (E) \text{ } S \mid \text{if } (E) \text{ } M \text{ else } U$.

Nesta gramática, $M$ (matched) representa comandos completos nos quais todo `if` tem seu `else` correspondente, enquanto $U$ (_unmatched_) representa comandos com `if` sem `else`. A estrutura garante que um `else` sempre se associa ao `if` mais interno ainda não pareado.

### Detecção Sistemática de Ambiguidade

A identificação de ambiguidade em uma gramática arbitrária é, em geral, um **problema indecidível**. Ou seja, não existe um algoritmo que sempre termine e determine se qualquer gramática é ambígua. Entretanto, a atenta leitora pode empregar uma das seguintes estratégias práticas:

1. **Abordagens Algorítmicas**

   **Verificação exaustiva limitada**: Para gramáticas pequenas, pode-se enumerar sistematicamente todas as derivações para _strings_ até um comprimento $n$ fixo. Se duas árvores de derivação distintas forem encontradas para a mesma _string_, a ambiguidade está provada. Esta técnica, embora computacionalmente intensiva, é conclusiva quando encontra ambiguidade.

   **Análise de tabelas de parsing**: Ferramentas como YACC e Bison constroem tabelas de ação/transição para parsers $LR$. A presença de **conflitos shift/reduce** ou **reduce/reduce** nestas tabelas indica potencial ambiguidade. Um conflito shift/reduce ocorre quando o parser não consegue decidir entre consumir mais entrada ou aplicar uma redução; um conflito reduce/reduce surge quando múltiplas produções podem ser aplicadas.

   **Verificadores especializados**: Ferramentas como o [CFG checker](https://github.com/stepchowfun/cfg-checker) empregam técnicas sofisticadas de análise estática para detectar ambiguidade, incluindo aproximações conservativas que podem provar não-ambiguidade para classes específicas de gramáticas.

2. **Heurísticas e Padrões Problemáticos**

A experiência acumulada na construção de compiladores identificou padrões gramaticais que frequentemente indicam ambiguidade:

   **Produções recursivas bilaterais**: Regras da forma $A \rightarrow A \alpha A$ são quase sempre ambíguas, pois permitem múltiplas formas de agrupar três ou mais ocorrências de $A$.

   **Prefixos comuns extensos**: Quando múltiplas produções para o mesmo não-terminal compartilham longos prefixos, como $A \rightarrow \alpha\beta\gamma$ e $A \rightarrow \alpha\beta\delta$, a decisão sobre qual produção aplicar pode requerer _Lookahead_arbitrariamente longo.

   **Aninhamento sem delimitadores explícitos**: Construções que permitem aninhamento sem marcadores claros de início e fim, como o problema clássico do *dangling else*, invariavelmente levam a ambiguidade estrutural.

## Autômatos com Pilha: A Máquina por Trás das Linguagens Livres de Contexto

Como estabelecido anteriormente, os autômatos finitos, os reconhecedores para linguagens regulares, são fundamentalmente limitados por sua falta de memória. A capacidade de _lembrar_ está restrita ao conjunto finito de estados da máquina. Essa limitação os impede de reconhecer linguagens que exigem a correspondência de símbolos ou contagem, como a linguagem $L=\{a^n b^n \mid n \geq 0\}$.

Para reconhecer a classe mais ampla das linguagens livres de contexto, é necessário um modelo de computação mais poderoso. O **autômato de pilha** é esse modelo. Um Autômato de Pilha pode ser concebido como um autômato finito não determinístico ao qual foi adicionada uma memória auxiliar: uma **pilha**, do inglês _stack_.

A pilha é uma estrutura de dados com acesso restrito, operando no modo **LIFO** (_**L**ast-**I**n, **F**irst-**O**ut_), o que significa que o último elemento inserido é o primeiro a ser removido. As transições de um Autômato de Pilha são mais complexas que as de um Autômato Finito. A decisão de qual transição tomar depende de três fatores: o estado atual, o próximo símbolo na _string_ de entrada e o símbolo que está no topo da pilha.

Em cada transição, além de mudar de estado e, opcionalmente, consumir um símbolo de entrada, o Autômato de Pilha pode realizar uma de três operações na pilha:

1. **Empilhar (_Push_)**: Adicionar um ou mais símbolos ao topo da pilha;
2. **Desempilhar (_Pop_)**: Remover o símbolo do topo da pilha;
3. **Manter**: Não alterar o conteúdo da pilha.

Esta capacidade de armazenar e recuperar informações de forma estruturada confere ao Autômato de Pilha seu poder computacional superior.

### A Equivalência Fundamental

**O resultado mais importante da teoria das Linguagens Livres de Contexto é a equivalência formal entre Gramáticas Livres de Contexto e autômatos com pilha. Uma linguagem é livre de contexto se, e somente se, existe um autômato com pilha que a reconhece.**

Esta equivalência é a espinha dorsal da análise sintática. Ela garante que para qualquer sintaxe de linguagem de programação que possa ser descrita por uma Gramática Livre de Contexto, podemos construir um mecanismo computacional o Autômato de Pilha para reconhecer programas escritos nessa linguagem. É importante notar que a classe de linguagens reconhecidas por Autômatos de Pilha não determinísticos é estritamente maior que a classe reconhecida por Autômatos de Pilha Determinísticos. **São os Autômatos de Pilha não Determinísticos que são equivalentes em poder às Gramáticas Livres de Contexto em geral**.

A escolha de uma pilha como o mecanismo de memória para reconhecer Linguagens Livres de Contexto não é acidental. A estrutura **LIFO** de uma pilha espelha perfeitamente a natureza recursiva e aninhada das derivações em uma Gramática Livre de Contexto.
Considere novamente a gramática $S \rightarrow aSb$ para a linguagem $a^nb^n$. A derivação de `aabb` será dada por  $S\Rightarrow aSb \Rightarrow aaSbb \Rightarrow aabb$. Observe como a estrutura se expande simetricamente _de dentro para fora_. Um Autômato de Pilha para esta linguagem implementa essa simetria de forma operacional: ao ler um `a`, ele empilha um símbolo de marcador (ex.: $X$); ao ler o próximo `a`, empilha outro $X$. Quando começa a ler os `b`s, ele desempilha um $X$ para cada `b` lido. Se a entrada terminar exatamente quando a pilha se esvaziar, a _string_ é aceita. O processo de empilhar na primeira metade e desempilhar na ordem inversa na segunda metade é a encarnação mecânica da recursão gramatical. A pilha _lembra_ as obrigações sintáticas (gerar um `b` correspondente para cada `a`) e as descarrega na ordem correta, tornando-a a estrutura de dados canônica para processar estruturas livres de contexto.

## As Fronteiras do Contexto Livre: O Lema do Bombeamento

Assim como existe uma ferramenta para provar que uma linguagem não é regular, existe um análogo para as linguagens livres de contexto: o **Lema do Bombeamento para Linguagens Livres de Contexto**, também conhecido como Lema de Bar-Hillel. Sua principal aplicação é demonstrar, por contradição, que uma determinada linguagem **não** é livre de contexto.

A intuição por trás do lema está enraizada na estrutura finita das gramáticas e na natureza das árvores de derivação. Para uma Gramática Livre de Contexto com um número finito de não terminais, qualquer _string_ suficientemente longa gerada por ela deve ter uma árvore de derivação _alta_. Pelo princípio da casa dos pombos, um caminho longo da raiz a uma folha nessa árvore deve necessariamente conter pelo menos um não-terminal repetido. Essa repetição cria uma sub-árvore que pode ser podada ou duplicada, _bombeando_ a _string_ de uma maneira específica.

O lema afirma formalmente que para qualquer linguagem livre de contexto $L$, existe um inteiro $p \geq 1$, o _comprimento de bombeamento_, tal que qualquer _string_ $s \in L$ com comprimento $\mid s \mid  \geq p$ pode ser decomposta em cinco _substrings_, $s = uvxyz$, que devem satisfazer as seguintes três condições:

1. $\mid vxy \mid \leq p$: a _substring_ que contém as partes bombeáveis não é excessivamente longa;
2. $\mid vy \mid \geq 1$: pelo menos uma das duas _substrings_ bombeáveis ($v$ ou $y$) não é vazia. Isso garante que o bombeamento realmente altera a _string_;
3. $uv^nxy^nz \in L$ para todo inteiro $n \geq 0$: as duas _substrings_, $v$ e $y$, podem ser bombeadas (repetidas) em conjunto um número arbitrário de vezes, incluindo zero, o que corresponde a removê-las), e a _string_ resultante permanecerá na linguagem $L$.

### Aplicação Prática: Prova de que $L = \{a^nb^nc^n \mid n \geq 0\}$ não é Livre de Contexto

A linguagem $L=\{a^nb^nc^n \mid n \geq 0\}$ é o exemplo clássico de uma linguagem que está além do alcance das Gramáticas Livres de Contexto. Podemos provar isso rigorosamente usando o lema do bombeamento.

A prova segue por contradição:

1. **Suposição**: Suponha que $L$ é livre de contexto.
2. **Invocação do Lema**: Pelo lema, deve existir um comprimento de bombeamento $p$.
3. **Escolha da _string_**: Selecionamos a _string_ $s=a^pb^pc^p$. Claramente, $s \in L$ e seu comprimento, $3p$, é maior ou igual a $p$.
4. **Análise da Decomposição**: O lema garante que $s$ pode ser decomposta como $s=uvxyz$, sujeita às condições do lema. A condição $\mid vxy \mid \leq p$ é a chave. Dada a estrutura de $s$ (um bloco de `a`s, seguido por um bloco de `b`s, seguido por um bloco de `c`s), esta condição implica que a sub_string_ $vxy$ não pode conter ocorrências de todos os três símbolos (`a`, `b` e `c`). Ela pode estar inteiramente dentro do bloco de `a`s, inteiramente dentro do de `b`s, ou abranger a fronteira entre `a`s e `b`s, ou entre `b`s e `c`s.
5. **Contradição**: A condição $\mid vy \mid \geq 1$ garante que o bombeamento adicionará (ou removerá) pelo menos um símbolo. Vamos considerar o bombeamento para cima, com $n=2$, resultando na _string_ $s′=uv^2xy^2z$.
   * Se $vxy$ contivesse apenas `a`s, então $v$ e $y$ conteriam apenas `a`s. A _string_ $s′$ teria mais `a`s do que `b`s e `c`s, violando a condição da linguagem.
   * Se $vxy$ contivesse uma mistura de `a`s e `b`s, então $v$ e $y$ poderiam conter `a`s e `b`s, mas nenhum `c`. A _string_ $s′$ teria um número aumentado de `a`s e/ou `b`s, mas o número de `c`s permaneceria $p$. Novamente, a igualdade $n=n=n$ seria quebrada.
   * O mesmo raciocínio se aplica a todas as outras localizações possíveis de $vxy$. Em nenhum caso, o bombeamento pode aumentar o número de `a`s, `b`s e `c`s na mesma proporção.
6. **Conclusão**: A _string_ bombeada $s′$ não pertence a $L$. Isso contradiz a terceira condição do lema. Portanto, a suposição inicial de que $L$ é livre de contexto deve ser falsa.

A estrutura de bombeamento duplo ($v$ e $y$) do lema não é arbitrária. Ela revela a limitação fundamental das Linguagens Livres de Contexto a dependências de, no máximo, _dois pontos_. Uma Gramática Livre de Contexto, por meio de recursão como $S→aSb$, pode correlacionar duas partes de uma _string_ (os `a`s no início e os `b`s no fim), que correspondem às partes $v$ e $y$ que são bombeadas em conjunto. A linguagem $a^nb^nc^n$ exige uma dependência de _três pontos_. Um autômato com pilha _gasta_ sua memória para verificar a correspondência entre `a`s e `b`s, não restando capacidade para verificar os `c`s contra a contagem original. O lema do bombeamento formaliza essa limitação, mostrando que o bombeamento inevitavelmente quebra essa dependência tripla.

## Os Analisadores Sintáticos

A análise sintática, ou *parsing*, é a segunda fase do processo de compilação, posicionada entre a análise léxica e a análise semântica. O analisador sintático atua como o guardião da gramática da linguagem. Enquanto o analisador léxico verifica a _ortografia_, se as palavras, ou _tokens_, são válidas, o analisador sintático verifica a _gramática_, se a sequência de _tokens_ forma sentenças estruturalmente corretas. O objetivo principal do analisador sintático é determinar se o fluxo de _tokens_ de entrada pode ser gerado pela gramática livre de contexto que define a linguagem e, em caso afirmativo, construir uma representação explícita dessa estrutura.

A entrada para o analisador sintático é o fluxo de _tokens_ produzido pelo analisador léxico. A saída, para um programa sintaticamente correto, é uma estrutura de dados que representa a estrutura hierárquica do código. Embora a árvore de derivação seja a representação teórica direta, na prática, os compiladores constroem uma **Árvore Sintática Abstrata**. A Árvore Sintática Abstrata é uma versão condensada e mais abstrata da árvore de derivação, que omite detalhes sintáticos intermediários, como parênteses para agrupamento ou não terminais que apenas passam a derivação adiante, e captura a estrutura lógica e semântica essencial do programa, tornando-a mais adequada para as fases subsequentes de análise e geração de código.

A maneira como a árvore de derivação é construída em relação à entrada define as duas principais estratégias de parsing, cada uma com suas próprias características, pontos fortes e limitações:

1. **Análise Descendente (*Top-Down Parsing*)**: A construção da árvore de derivação começa no topo, a raiz, que é o símbolo inicial da gramática, e avança para baixo, em direção às folhas, a _string_ de _tokens_ de entrada. Este método tenta encontrar a derivação mais à esquerda para a entrada.  
2. **Análise Ascendente (*Bottom-Up Parsing*)**: A construção da árvore de derivação começa na base, as folhas, que são a _string_ de _tokens_ de entrada, e avança para cima, em direção à raiz, o símbolo inicial. Este método efetivamente reverte uma derivação mais à direita.

Um erro sintático detectado pelo parser é um erro fatal que interrompe o processo de compilação. Sem uma estrutura sintática válida e inequívoca representada pela Árvore Sintática Abstrata, as fases subsequentes, que dependem dessa estrutura para realizar a verificação de tipos e a geração de código, não podem prosseguir.

### Estratégias de Análise Descendente (_Top-Down_)

A análise descendente tenta construir uma árvore de derivação para a _string_ de entrada começando pela raiz (símbolo inicial) e criando os nós da árvore em pré-ordem. Isso equivale a encontrar uma derivação mais à esquerda para a _string_ de entrada.

#### **Analisadores de Descida Recursiva (_Recursive-Descent_)**

Uma das implementações mais diretas e intuitivas de um parser descendente é o analisador de descida recursiva. Nesta abordagem, um conjunto de procedimentos mutuamente recursivos é escrito, geralmente um para cada não-terminal na gramática. O procedimento associado a um não-terminal A é responsável por reconhecer na entrada uma sub_string_ que pode ser derivada de A. A simplicidade e a facilidade de implementação manual tornam esta técnica atraente. No entanto, analisadores de descida recursiva ingênuos podem ser ineficientes. Estes analisadores podem exigir retrocesso, *backtracking*, se a escolha de uma produção se revelar incorreta. Além disso, eles não conseguem lidar com gramáticas que contêm recursão à esquerda, regras da forma $A\rightarrow A \beta$. Isso levaria a uma recursão infinita.

#### **Analisadores Preditivos ($LL$)**

Para superar as desvantagens do retrocesso, foi desenvolvida uma classe de parsers descendentes chamada de **analisadores preditivos**. Estes são parsers que podem _prever_ qual produção aplicar a um não-terminal olhando para a frente na _string_ de entrada, sem precisar adivinhar e retroceder. O tipo mais comum é o

**parser $LL(1)$**. A notação $LL(1)$ significa:

* **L** (primeiro): A entrada é lida da esquerda, _**L**eft_ para a direita;
* **L** (segundo): O parser constrói uma derivação mais à esquerda, _**L**eftmost_;
* **(1)**: Ele usa **1** símbolo de _Lookahead_ (antecipação) para tomar suas decisões.

Um parser $LL(1)$ opera com três componentes: uma **pilha de análise**, um **ponteiro de entrada** e uma **tabela de análise**. A tabela de análise é uma matriz na qual as linhas correspondem aos não terminais e as colunas aos terminais. Cada célula $M[A,a]$ contém a regra de produção que deve ser usada se o não-terminal A estiver no topo da pilha e o terminal a for o próximo símbolo de entrada, o _lookahead_. O algoritmo é determinístico: para cada par, não-terminal no topo da pilha, símbolo de _Lookahead_, há no máximo uma ação a ser tomada. Se a célula estiver vazia, um erro sintático é detectado.

Os parsers $LL$ são _ansiosos_. No momento em que um não-terminal $A$ está no topo da pilha, eles devem se comprometer _imediatamente_ com uma única regra $A\rightarrow \beta$, baseando sua decisão exclusivamente no próximo _token_ de entrada. Essa necessidade de uma decisão precoce e inequívoca é a razão pela qual as gramáticas $LL(1)$ não podem ter ambiguidades, recursão à esquerda ou prefixos comuns, duas produções para o mesmo não-terminal que começam com o mesmo símbolo. Tais características tornam impossível para o parser fazer uma escolha determinística com apenas um símbolo de _Lookahead_.

### Estratégias de Análise Ascendente (Bottom-Up)

Em contraste com a abordagem descendente, a análise ascendente constrói a árvore de derivação a partir das folhas, a _string_ de entrada, em direção à raiz, o símbolo inicial da gramática. O processo pode ser visto como uma tentativa de _reduzir_ a _string_ de entrada de volta ao símbolo inicial, essencialmente traçando uma derivação mais à direita ao contrário.

#### A Abordagem _Shift-Reduce_

O mecanismo fundamental por trás da maioria dos parsers ascendentes é o _**shift-reduce**_ (*deslocar-reduzir*). O parser utiliza uma pilha para armazenar símbolos da gramática e toma uma de quatro ações possíveis em cada passo:

1. **Shift (Deslocar)**: o próximo símbolo de entrada é movido (deslocado) para o topo da pilha; 
2. **Reduce (Reduzir)**: o parser reconhece que uma sequência de símbolos $\beta$ no topo da pilha corresponde ao lado direito de uma regra de produção $A\rightarrow \beta$. Ele então substitui (reduz) $\beta$ na pilha pelo não-terminal $A$; 
3. **Accept (Aceitar)**: a análise é concluída com sucesso. Isso ocorre quando a entrada foi totalmente consumida e a pilha contém apenas o símbolo inicial. 
4. **Error (Erro)**: um erro sintático é encontrado, e o parser não pode continuar.

A principal dificuldade em um parser _shift-reduce_ é decidir quando deslocar e quando reduzir (um **conflito shift/reduce**) ou, ao decidir reduzir, qual regra usar se múltiplas corresponderem (um **conflito reduce/reduce**).

#### A Família de Analisadores $LR$

A classe mais poderosa e amplamente utilizada de parsers ascendentes é a família **$LR$**. Eles são capazes de analisar uma classe de gramáticas significativamente maior do que os parsers $LL$. A notação $LR$ significa:

* **L**: A entrada é lida da esquerda, _**L**eft_ para a direita;
* **R**: O parser constrói uma derivação mais à direita, _**R**ightmost_, ao contrário.

Existem várias variantes de parsers $LR$, que diferem principalmente na forma como suas tabelas de análise são construídas e na quantidade de informação de _Lookahead_ que utilizam para resolver conflitos:

* **LR(0)**: o mais simples, não usa _Lookahead_;
* **SLR (Simple LR)**: usa os conjuntos FOLLOW do não-terminal para decidir sobre as reduções;
* **LALR(1) (Look-Ahead LR)**: uma versão otimizada do $LR(1)$ com tabelas menores, mas poder de reconhecimento ligeiramente reduzido. É a base para ferramentas como YACC e Bison;
* **LR(1) Canônico**: o mais poderoso da família, mas que gera tabelas de análise muito grandes.

Ao contrário dos parsers $LL$ _ansiosos_, os parsers $LR$ são _pacientes_. Eles não precisam decidir qual regra de produção usar no momento em que veem o primeiro símbolo de seu lado direito. Em vez disso, eles continuam a deslocar símbolos para a pilha até que o lado direito *completo* de uma produção, conhecido como _**handle**_, esteja no topo da pilha. Somente então eles realizam a redução. Essa capacidade de adiar a decisão até que mais contexto esteja disponível é a fonte de seu maior poder e de sua capacidade de lidar com uma gama mais ampla e natural de gramáticas sem a necessidade de reescritas extensivas.

O **parser LALR(1)** (_**L**ook-**A**head **LR**_) merece destaque. Esta família de _parsers_ representa uma solução engenhosa para um problema prático fundamental: os parsers LR(1), embora teoricamente mais poderosos, geram tabelas de análise de tamanho proibitivo para gramáticas reais de linguagens de programação. O LALR(1) resolve essa limitação através de uma técnica de **fusão de estados** (_state merging_). Durante a construção do autômato LR(1), estados que possuem o mesmo _core_ — isto é, a mesma coleção de itens LR(0), mas diferentes conjuntos de _lookahead_, são consolidados em um único estado. O conjunto de _lookahead_ do estado resultante torna-se a união dos conjuntos originais. Esta fusão reduz drasticamente o número de estados, frequentemente de milhares para centenas, tornando as tabelas de análise praticamente viáveis sem sacrificar significativamente o poder de reconhecimento.

A diferença fundamental entre LALR(1) e LR(1) manifesta-se tanto em termos de eficiência quanto de capacidade de reconhecimento. **Em termos de eficiência**, o LR(1) pode gerar tabelas com dezenas de milhares de estados para gramáticas complexas, consumindo megabytes de memória e tornando o parser lento, enquanto o LALR(1) produz tabelas compactas com centenas de estados, adequadas para uso industrial. **Em termos de poder de reconhecimento**, o LR(1) aceita rigorosamente todas as gramáticas LR(1), enquanto o LALR(1) constitui um subconjunto próprio dessa classe, algumas gramáticas LR(1) podem se tornar ambíguas durante o processo de fusão de estados, gerando conflitos _reduce/reduce_. Contudo, esta limitação raramente impacta gramáticas práticas de linguagens de programação, razão pela qual ferramentas como YACC e Bison adotaram LALR(1) como padrão, estabelecendo um equilíbrio ótimo entre expressividade teórica e viabilidade implementacional.

### Impacto da Ambiguidade no Processo de Compilação {#sec-impacto-ambiguidade-compilacao}

A atenta leitora deve compreender que a ambiguidade em uma gramática não é uma mera curiosidade teórica; ela representa uma falha fundamental na especificação de uma linguagem, com consequências diretas e severas em todas as principais fases do processo de compilação. Um compilador é uma ferramenta determinística que deve traduzir um único código-fonte em um único programa executável. A ambiguidade quebra essa premissa, introduzindo incerteza que se propaga desde a análise sintática até a geração do código final.

#### Fase de Análise Sintática

A análise sintática é a primeira e mais direta vítima da ambiguidade. O objetivo do _parser_ é construir uma única árvore de derivação que represente a estrutura do código-fonte. Quando a gramática é ambígua, essa tarefa torna-se impossível sem regras externas de desambiguação.

* **Geração de Múltiplas Árvores de Derivação**: Para uma sentença ambígua, o analisador sintático é inerentemente não-determinístico. Ele pode construir duas ou mais árvores de derivação distintas, cada uma representando uma interpretação estrutural válida. Sem um critério para escolher entre elas, o _parser_ não pode prosseguir de forma unívoca.

* **Conflitos em Parsers LR**: Em analisadores sintáticos _bottom-up_, como os da família LR (LR, SLR, LALR), a ambiguidade manifesta-se diretamente como **conflitos shift/reduce** ou **reduce/reduce**. Por exemplo, na gramática ambígua de expressões, ao processar `id + id * id`, o _parser_ chegará a um estado no qual encontrou `E + E`. Neste ponto, ele não sabe se deve reduzir a expressão pela regra $E \rightarrow E+E$ (um _reduce_) ou aguardar o próximo símbolo `*` para processar a multiplicação primeiro (um _shift_). Ferramentas geradoras de _parsers_, como o YACC ou Bison, relatarão esses conflitos como erros fatais durante a geração do analisador.

* **Ineficiência em Parsers Top-Down**: Para analisadores sintáticos _top-down_, como os de descida recursiva, a ambiguidade pode levar a um **backtracking excessivo**. O _parser_ pode ser forçado a explorar múltiplos caminhos de derivação, apenas para descobrir que um deles falha mais tarde. Em casos mal projetados, especialmente com recursão à esquerda, isso pode levar a _loops_ infinitos, paralisando a compilação.

#### Fase de Análise Semântica

Supondo que a fase sintática tenha conseguido, de alguma forma, produzir uma árvore, talvez escolhendo arbitrariamente uma das opções, os problemas de ambiguidade se transformam em problemas semânticos. A árvore sintática é a base para toda a análise de significado, e diferentes árvores levam a semânticas drasticamente diferentes.

* **Atribuição de Tipos Inconsistente**: A estrutura da árvore de derivação dita como os tipos são verificados e inferidos. Na expressão `(id_float + id_int) * id_int`, a adição ocorreria primeiro, possivelmente promovendo o `id_int` a `float`. Na interpretação `id_float + (id_int * id_int)`, a multiplicação de inteiros ocorreria primeiro. As duas árvores podem resultar em tipos finais diferentes ou até mesmo em erros de tipo distintos.

* **Resolução de Escopo Ambígua**: A ambiguidade estrutural, como a do _dangling else_, afeta diretamente a determinação do escopo. Uma variável declarada dentro de um bloco `if` terá seu escopo e tempo de vida definidos pela árvore sintática. Se não está claro a qual `if` um bloco `else` pertence, também não estará claro o escopo das construções dentro daquele bloco.

* **Ordem de Avaliação Indefinida**: A semântica de uma linguagem define a ordem de avaliação de subexpressões. Isso é fundamental quando há **efeitos colaterais** (por exemplo, `f() + g()`, no quais `f` e `g` modificam uma variável global). Uma gramática ambígua que permite duas árvores de derivação para esta expressão deixa indefinido qual função será chamada primeiro, tornando o resultado da expressão imprevisível.

#### Fase de Geração de Código

Finalmente, as inconsistências estruturais e semânticas culminam em um processo de geração de código que é, na melhor das hipóteses, não-determinístico e, na pior, incorreto.

* **Geração de Código Não-Determinístico**: Cada Árvore Sintática Abstrata distinta mapeia para uma sequência diferente de código intermediário ou de máquina. A expressão `(a+b)*c` gera uma instrução de `ADD` seguida por uma de `MUL`. Já `a+(b*c)` gera uma `MUL` seguida por uma `ADD`. Se o compilador escolher uma árvore arbitrariamente, o código gerado será igualmente arbitrário.

* **Impossibilidade de Otimização**: As otimizações de código dependem de uma análise rigorosa e inequívoca do fluxo de controle e de dados, que é derivada da Árvore Sintática Abstrata. Se o compilador não pode garantir a estrutura correta do programa (por exemplo, a precedência de operadores), ele não pode aplicar otimizações de forma segura. Otimizar uma estrutura que não representa a intenção do programador pode alterar drasticamente a lógica do programa.

* **Comportamento Imprevisível do Programa**: A consequência final é a mais grave. Um programa compilado a partir de uma gramática ambígua pode exibir **comportamentos diferentes** dependendo da versão do compilador ou até mesmo de fatores aparentemente não relacionados. Isso viola o princípio fundamental de que a compilação deve ser um processo reprodutível e confiável, tornando a linguagem inadequada para qualquer aplicação séria.

### Automatizando a Construção: Ferramentas Geradoras de Parsers

A construção manual de um analisador sintático, especialmente um parser $LR$, é uma tarefa complexa, tediosa e propensa a erros. Para mitigar essa complexidade, foram desenvolvidas ferramentas especializadas conhecidas como **geradores de parsers**. Essas ferramentas automatizam o processo de criação de um parser a partir de uma especificação de alto nível da gramática da linguagem.

#### YACC e GNU Bison**

As ferramentas mais conhecidas e influentes nesta categoria são YACC e seu sucessor, Bison.

* **YACC (Yet Another Compiler-Compiler)**: desenvolvido na Bell Labs, YACC é a ferramenta canônica para gerar parsers $LALR(1)$. Ele se tornou um padrão de fato em sistemas Unix.
* **GNU Bison**: É a implementação do projeto GNU de um gerador de parsers. É amplamente compatível com YACC, mas oferece recursos adicionais, como a geração de parsers $GLR$, _Generalized LR_ para lidar com gramáticas ambíguas e a geração de código em múltiplas linguagens (C, C++, Java).

O fluxo de trabalho com YACC ou Bison pode ser resumido da seguinte forma:

1. **Entrada**: o desenvolvedor cria um arquivo de especificação, geralmente com a extensão `.y`. Este arquivo contém três seções: declarações, a gramática da linguagem escrita em uma notação semelhante à **$BNF$**, e uma seção de código auxiliar. O desenvolvedor pode associar **ações** de código (em C ou C++) a cada regra da gramática.
2. **Processamento**: A ferramenta (YACC ou Bison) lê o arquivo de especificação. Ela analisa a gramática, constrói o autômato $LR$ e a tabela de análise correspondente, e verifica a existência de conflitos (_shift/reduce_ ou _reduce/reduce_).
3. **Saída**: Se a gramática for adequada (ex.: $LALR(1)$), a ferramenta gera um arquivo de código fonte (ex.: `y.tab.c`) que implementa a função do parser (tipicamente chamada `yyparse`). Esta função implementa o algoritmo _shift-reduce_ dirigido pela tabela gerada. As ações de código fornecidas pelo desenvolvedor são incorporadas à função e são executadas sempre que a regra correspondente é reduzida. Essas ações são tipicamente usadas para construir a Árvore Sintática Abstrata.

#### Sinergia com Lex/Flex

YACC/Bison são projetados para lidar com a análise sintática e quase sempre são usados em conjunto com um gerador de analisador léxico, como **Lex** ou seu sucessor **Flex**. O Flex recebe uma especificação de padrões de _tokens_, usando expressões regulares, e gera um _scanner_ (a função `yylex`). O parser gerado pelo Bison chama `yylex` para obter o próximo _token_ da entrada, formando um pipeline coeso que transforma o texto fonte em uma Árvore Sintática Abstrata.

O uso de ferramentas como YACC e Bison representa uma mudança de paradigma fundamental na construção de compiladores: da programação _imperativa_ para a _declarativa_. Em vez de implementar manualmente o complexo algoritmo de um parser shift-reduce, a abordagem imperativa, o desenvolvedor fornece uma especificação de alto nível da gramática da linguagem, a abordagem declarativa. A ferramenta se encarrega de gerar a implementação de baixo nível. Essa separação de interesses, o _quê_, a sintaxe da linguagem, do _como_, o algoritmo de parsing, aumenta drasticamente a produtividade, a robustez e a manutenibilidade do compilador.

Além disso, a utilização de geradores de parsers permite que os desenvolvedores se concentrem na lógica da linguagem que estão implementando, em vez de se perderem nos detalhes de implementação do parser. Isso resulta em um desenvolvimento mais rápido e menos propenso a erros, uma vez que a ferramenta cuida de muitos dos aspectos técnicos e repetitivos da construção do parser.
