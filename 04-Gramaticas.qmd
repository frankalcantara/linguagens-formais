# Linguagens Livres de Contexto {#sec-lingagens-livres-de-contexto}

No campo da ciência da computação teórica, a Hierarquia de Chomsky serve como o principal sistema de classificação para linguagens formais. Proposta por [Noam Chomsky](https://en.wikipedia.org/wiki/Noam_Chomsky), esta hierarquia organiza as linguagens em uma série de quatro níveis aninhados, cada um sendo um superconjunto do nível anterior. A classificação baseia-se na complexidade das regras gramaticais necessárias para gerar as _string_s de uma linguagem, estabelecendo uma correspondência direta entre a classe da linguagem, o tipo de gramática que a gera e o modelo de autômato capaz de reconhecê-la.1 Os níveis são:

* **Tipo 3: Linguagens Regulares**, geradas por gramáticas regulares e reconhecidas por autômatos finitos. 
* **Tipo 2: Linguagens Livres de Contexto**, geradas por gramáticas livres de contexto e reconhecidas por autômatos com pilha. 
* **Tipo 1: Linguagens Sensíveis ao Contexto**, geradas por gramáticas sensíveis ao contexto e reconhecidas por autômatos linearmente limitados. 
* **Tipo 0: Linguagens Recursivamente Enumeráveis**, geradas por gramáticas irrestritas e reconhecidas por Máquinas de Turing.

Há uma história rica e complexa por trás do desenvolvimento da Hierarquia de Chomsky, que reflete as mudanças nas necessidades e nas compreensões da linguística e da computação ao longo do tempo.

## Hierarquia de Chomsky: História e Contexto

A Hierarquia de Chomsky representa um dos marcos mais elegantes da ciência do século XX, estabelecendo pontes entre linguística, matemática e ciência da computação. **Desenvolvida por Noam Chomsky entre 1956 e 1959**, esta classificação não apenas transformou os estudos linguísticos de uma disciplina descritiva em uma ciência formal rigorosa, mas também **forneceu as bases teóricas para a construção de compiladores e o processamento computacional de linguagens**. A hierarquia emergiu de uma convergência única de fatores históricos: a revolução cognitiva dos anos 1950, o desenvolvimento da teoria da computação, e a genialidade de um jovem linguista que soube sintetizar influências matemáticas diversas em uma estrutura unificada que perdura até hoje.

A história da Hierarquia de Chomsky começa com um encontro transformador que ocorreu em 1947, quando Chomsky, então com 19 anos e considerando abandonar a universidade, conheceu [Zellig S. Harris](https://en.wikipedia.org/wiki/Zellig_Harris) na Universidade da Pensilvânia. Harris era um dos fundadores da linguística estrutural americana e tinha estabelecido o primeiro departamento moderno de linguística dos Estados Unidos em 1946. Sob a orientação de Harris, Chomsky também estudou matemática com [Nathan Fine](https://en.wikipedia.org/wiki/Nathan_Fine) em Harvard. **Essas influências foram cruciais para moldar sua abordagem aos sistemas formais e metodologia científica**. Sua tese de mestrado de 1951, _A Morfofonêmica do Hebraico Moderno_, e especialmente seu trabalho em _A Estrutura Lógica da Teoria Linguística_ (LSLT), escrito enquanto era fellow júnior em Harvard (1951-55), começaram a transformar a abordagem estrutural de Harris em algo inteiramente novo.

### A revolução cognitiva e o ambiente do MIT

Os anos 1950 representavam um período de fermentação intelectual que se tornaria conhecido como a **Revolução Cognitiva**. [George Miller](https://en.wikipedia.org/wiki/George_Armitage_Miller), uma das figuras-chave, datou o início dessa revolução em 11 de setembro de 1956, quando pesquisadores de psicologia experimental, ciência da computação e linguística teórica apresentaram trabalhos sobre temas relacionados à ciência cognitiva em uma reunião do _Special Interest Group in Information Theory_ no MIT.

Durante esse período, vários paradigmas dominantes estavam sendo desafiados. O behaviorismo era a doutrina dominante em psicologia e linguística, enfatizando organização de dados e classificação taxonômica enquanto rejeitava o estudo de estados mentais internos. A linguística estrutural, liderada por figuras como Leonard Bloomfield, tratava a linguagem como um fenômeno social estudado por meio de análise de corpus.

A Hierarquia de Chomsky emergiu de uma convergência sofisticada de lógica matemática, teoria dos autômatos e teoria das funções recursivas desenvolvidas entre as décadas de 1930 e 1950. Chomsky construiu sistematicamente sobre trabalhos matemáticos anteriores, criando uma síntese notável.

**O trabalho de [Emil Post](https://en.wikipedia.org/wiki/Emil_Leon_Post) (1936-1944)** foi fundamental. Post desenvolveu _sistemas canônicos de Post_ usando técnicas de reescrita de _string_s que se tornaram fundamentais para a abordagem de Chomsky. Seu trabalho de 1936 _Processos combinatórios finitos – Formulação 1_ criou modelos computacionais essencialmente equivalentes às máquinas de Turing. Post queria _derivar mecanicamente inferências de uma sentença axiomática inicial_, e Chomsky aplicou diretamente essa estrutura lógica para descrever conjuntos de _string_s na linguagem humana.

**O modelo de [McCulloch-Pitt](https://www.historyofinformation.com/detail.php?entryid=782)s de 1943** forneceu a ponte computacional. Seu trabalho _Um Cálculo Lógico das Ideias Imanentes na Atividade Nervosa_ ofereceu o primeiro modelo matemático conectando redes neurais à computação, levando diretamente à noção de autômatos finitos que se tornaram o Tipo-3 na Hierarquia de Chomsky. Eles demonstraram que neurônios poderiam funcionar como portas lógicas, estabelecendo a conexão entre sistemas biológicos e modelos computacionais formais.

**O trabalho de [Stephen Cole Kleene](https://en.wikipedia.org/wiki/Stephen_Cole_Kleene)** estabeleceu a equivalência fundamental entre expressões regulares e máquinas de estado finito (Teorema de Kleene), fornecendo a base matemática para as gramáticas do Tipo-3. Suas propriedades de fechamento para linguagens regulares sob união, concatenação e operação estrela de Kleene criaram o framework teórico necessário.

A estrutura teórica se baseou explicitamente na **teoria das funções recursivas**. Chomsky declarou que a gramática gerativa se desenvolveu dentro de _uma teoria matemática particular, a saber, a teoria das funções recursivas_. Isso foi importante porque a teoria das funções recursivas nas décadas de 1930-1940 formalizou a noção de _computação_, construindo **_sobre a tese de Church-Turing_** e as provas de equivalência entre diferentes modelos de computação.

### A elegância matemática dos quatro tipos

O desenvolvimento dos **quatro tipos específicos de gramáticas emergiu da aplicação de restrições matemáticas crescentes**. A classificação não foi arbitrária, mas resultou de uma lógica matemática rigorosa que Chomsky articulou em seus trabalhos seminais de 1956 (_Três Modelos para a Descrição da Linguagem_) e 1959 (_Sobre Certas Propriedades Formais das Gramáticas_).

**A estrutura hierárquica aninhada** representa uma beleza matemática notável. Cada classe é um subconjunto próprio da próxima classe menos restritiva: $\text{Tipo 3} ⊂ \text{Tipo 2} ⊂ \text{Tipo 1} ⊂ \text{Tipo 0}$. Cada tipo de gramática corresponde exatamente a uma classe de autômatos com poder computacional correspondente, criando uma correspondência perfeita entre restrições gramaticais e modelos computacionais.

- O **Tipo 0 (Gramáticas Irrestritas)** não possui restrições nas regras de produção ($\alpha \rightarrow \beta$), sendo equivalente às máquinas de Turing e capaz de gerar linguagens recursivamente enumeráveis.

- O **Tipo 1 (Gramáticas Sensíveis ao Contexto)** adiciona a propriedade não-contrativa ($\mid \alpha \mid ≤ \mid \beta \mid$), correspondendo aos autômatos linearmente limitados. 

- O **Tipo 2 (Gramáticas Livres de Contexto)(Gramáticas Livres de Contextos)** re_string_e o lado esquerdo a um único não-terminal ($A \rightarrow γ$), equivalendo aos autômatos de pilha. 

- O **Tipo 3 (Gramáticas Regulares)** impõe as restrições mais específicas em padrões terminal/não-terminal, correspondendo aos autômatos finitos.

**A elegância conceitual reside na relação inversa** entre liberdade gramatical e complexidade computacional: gramáticas mais restritas (Tipo 3) requerem menor poder computacional (autômatos finitos), enquanto gramáticas menos restritas (Tipo 0) necessitam do maior poder computacional (máquinas de Turing).

Na ciência da computação, a hierarquia estabeleceu **as bases matemáticas para a teoria das linguagens formais**, criando uma classificação sistemática que permanece fundamental para a ciência da computação teórica. A realização de Chomsky de que _a descrição de um tipo de autômato e de um tipo de linguagem estão relacionadas_ conectou campos anteriormente desconectados da linguística e computação.

### Aplicações práticas em compiladores e processamento de linguagem

Na análise léxica, gramáticas do Tipo-3 (regulares) e autômatos finitos tornaram-se fundamentais para _tokenizar_ código fonte, identificando palavras-chave, _string_s e identificadores. Na análise sintática, gramáticas do Tipo-2 (livres de contexto) formam a base teórica para analisar a maioria das linguagens de programação, lidando com estruturas aninhadas como parênteses, chamadas de função e estruturas de controle.

**Geradores de parser modernos usam especificações de Gramáticas Livres de Contextos** para gerar automaticamente parsers, com ferramentas implementando diretamente conceitos da Hierarquia de Chomsky. Subconjuntos de gramáticas livres de contexto (**LL**, **LR**) tornaram-se padrão para parsing eficiente, tornando a compilação tratável mantendo poder expressivo suficiente.

A **Forma de Backus-Naur (BNF)** desenvolvida por [John Backus](https://en.wikipedia.org/wiki/John_Backus) e [Peter Naur](https://en.wikipedia.org/wiki/Peter_Naur) para ALGOL 60 foi criada com conhecimento explícito do trabalho anterior de Chomsky sobre Gramáticas Livres de Contexto, tornando-se o padrão da indústria para definir sintaxe de linguagens de programação e implementando diretamente conceitos de gramática formal de Chomsky.

Não bastasse o que a atenta leitora viu até o momento, cada nível da hierarquia possui propriedades de complexidade computacional bem definidas. Linguagens regulares (Tipo 3) podem ser reconhecidas em tempo linear $O(n)$ com espaço constante. Linguagens livres de contexto (Tipo 2) são decidíveis em tempo polinomial $O(n^3)$ usando o algoritmo CYK, formando a base para a maioria dos compiladores de linguagens de programação. Linguagens sensíveis ao contexto (Tipo 1) são PSPACE-completas, com complexidade de tempo exponencial no pior caso, mas complexidade de espaço linear. Linguagens recursivamente enumeráveis (Tipo 0) são indecidíveis em geral.

:::{callout-tip}
**Algoritmo CYK (Cocke-Younger-Kasami)**

O algoritmo CYK é um método de programação dinâmica para determinar se uma _string_ pertence a uma linguagem livre de contexto. Desenvolvido independentemente por Cocke, Younger e Kasami, o algoritmo requer que a gramática esteja na Forma Normal de Chomsky, onde todas as produções têm a forma $A \rightarrow BC$ ou $A \rightarrow a$.

O algoritmo constrói uma tabela triangular de tamanho $n \times n$, onde $n$ é o comprimento da _string_ de entrada. Cada entrada $(i,j)$ da tabela indica quais não-terminais podem derivar a sub_string_ que inicia na posição $i$ com comprimento $j$. O preenchimento ocorre bottom-up: primeiro as sub_string_s de comprimento 1, depois 2, e assim sucessivamente até $n$.

A complexidade $O(n^3)$ resulta de três loops aninhados: dois para percorrer a tabela ($O(n^2)$ entradas) e um terceiro para testar todas as possíveis divisões de cada sub_string_ ($O(n)$ divisões por entrada). Esta complexidade polinomial torna o CYK prático para parsing de linguagens de programação, sendo fundamental na construção de parsers bottom-up.
:::

:::{callout-tip}
**PSPACE-Completas**

PSPACE é a classe de complexidade que contém todos os problemas de decisão solucionáveis por uma máquina de Turing determinística usando espaço polinomial. Um problema é PSPACE-completo se pertence ao PSPACE e qualquer problema em PSPACE pode ser reduzido a ele em tempo polinomial.

As linguagens sensíveis ao contexto (Tipo 1) são PSPACE-completas porque seu reconhecimento requer espaço proporcional ao comprimento da entrada, mas pode demandar tempo exponencial. Isso ocorre porque um autômato linearmente limitado (que reconhece essas linguagens) possui número finito de configurações possíveis, mas esse número cresce exponencialmente com o tamanho da entrada.

A aparente contradição entre "espaço linear" e "PSPACE-completo" se resolve considerando que PSPACE inclui problemas que usam espaço polinomial, e espaço linear é um caso particular de espaço polinomial ($O(n) \subset O(n^k)$). O tempo exponencial surge porque, mesmo com espaço limitado, o número de estados alcançáveis pode crescer exponencialmente, exigindo exploração exaustiva no pior caso.
:::

Além disso, os lemas de bombeamento para cada classe fornecem ferramentas matemáticas para provar que linguagens NÃO pertencem às classes inferiores. As propriedades de fechamento variam sistematicamente: **linguagens regulares são fechadas sob todas as operações padrão, linguagens livres de contexto são fechadas sob união, concatenação e estrela de Kleene mas NÃO sob interseção e complemento**.

### O Poder e as Limitações das Linguagens Regulares (Tipo 3)

As linguagens regulares representam a classe mais fundamental e restrita da hierarquia. Elas são formalmente definidas como o conjunto de linguagens que podem ser descritas por expressões regulares ou, equivalentemente, geradas por gramáticas regulares.

A estrutura de uma gramática regular é estritamente limitada. Suas regras de produção, que ditam como os símbolos podem ser reescritos, devem aderir a um formato rígido. Em uma gramática linear à direita, por exemplo, todas as regras devem ser da forma $A \rightarrow tN$ ou $A \rightarrow t$, onde $A$ e $N$ são símbolos não terminais e $t$ é uma _string_ de símbolos terminais que pode ser vazia. Uma restrição similar se aplica às gramáticas lineares à esquerda. Esta limitação estrutural não é meramente uma convenção; ela é a fonte da principal limitação computacional das linguagens regulares: a incapacidade de modelar dependências aninhadas ou recursivas. 

O autômato finito, o reconhecedor para a classe das linguagens regulares, opera sem uma memória externa; seu único _histórico_ está contido no estado atual em que se encontra. Consequentemente, ele não pode _lembrar_ ou _contar_ ocorrências de símbolos para garantir correspondências em uma _string_. O exemplo canônico das limitações de um autômato finito são os parênteses aninhados. Um autômato finito não pode verificar se uma _string_ contém um número igual de parênteses abertos e fechados se estes parênteses puderem ser aninhados.

### A Ascensão às Linguagens Livres de Contexto (Tipo 2)

A necessidade de modelar estruturas sintáticas mais complexas, como expressões aritméticas com parênteses aninhados, blocos de código delimitados (´begin´...´end´) ou estruturas de dados recursivas, revela a insuficiência das linguagens regulares. Para superar essas limitações, ascendemos na hierarquia para as linguagens livres de contexto.

As Linguagens Livres de Contexto formam um superconjunto estrito das linguagens regulares; toda linguagem regular é, por definição, livre de contexto, mas o inverso não é verdadeiro. O poder expressivo adicional das Linguagens Livres de Contexto emana diretamente de uma flexibilização nas regras de produção de suas gramáticas. Em uma gramática livre de contexto (Gramáticas Livres de Contextos), uma regra de produção tem a forma

A→β, onde A é um único símbolo não-terminal e β é uma _string_ qualquer de símbolos terminais e não terminais. A ausência de restrições sobre a posição dos não terminais em β permite a definição de recursão central, como na regra S→aSb, que é a chave para modelar estruturas aninhadas.2

### Exemplos Práticos que Distinguem as Duas Classes

A distinção teórica entre essas duas classes é melhor ilustrada pelo exemplo canônico da linguagem $L={a^n b^n \mid n \geq 0}$, que consiste em _strings_ com um número de `a`s seguido pelo mesmo número de `b`s.

Esta linguagem não é regular. A tentativa de construir um autômato finito para reconhecê-la falha porque a máquina precisaria de uma quantidade infinita de estados para _lembrar_ o número exato de `a`s lidos e garantir que o número de `b`s corresponda. Uma prova formal de que $L$ não é  uma linguagem regular pode ser rigorosamente construída usando o Lema do Bombeamento para linguagens regulares. Este lema afirma que, para qualquer _string_ suficientemente longa em uma linguagem regular, existe uma sub_string_ que pode ser _bombeada_ (repetida um número arbitrário de vezes) e a nova _string_ resultante ainda pertencerá à linguagem. Ao aplicar o lema à _string_ $apbp$,  no qual $p$ é o comprimento de bombeamento, a sub_string_ bombeada consistirá inteiramente de `a`s, quebrando o equilíbrio entre `a`s e `b`s e provando que a linguagem não pode ser regular. No entanto, $L$ é uma linguagem livre de contexto por excelência. Ela pode ser gerada pela gramática extremamente simples e elegante:

$$S \rightarrow aSb \mid \epsilon$$

Nesta gramática o símbolo $\epsilon$ representa a _string_ vazia. A regra recursiva $S \rightarrow aSb$ encapsula perfeitamente a capacidade de aninhamento que define as Linguagens Livres de Contexto. Outros exemplos práticos que exigem o poder das Linguagens Livres de Contexto incluem a linguagem dos palíndromos (ex.: _radar_) e a validação estrutural de documentos XML, no qual as _tags_ de abertura e fechamento devem ser corretamente aninhadas, uma tarefa impossível para expressões regulares.

A atenta leitora deve observar que essa progressão não é apenas um exercício teórico. A necessidade prática de analisar a sintaxe de linguagens de programação, que são repletas de construções aninhadas como laços, condicionais e chamadas de função, é a força motriz que torna as linguagens regulares insuficientes e as linguagens livres de contexto absolutamente essenciais para a ciência da computação. A Tabela #tbl-resumo1 condensa as diferenças entre linguagens regulares e linguagens livres de contexto.

| Característica | Linguagens Regulares (Tipo 3) | Linguagens Livres de Contexto (Tipo 2) |
| :---- | :---- | :---- |
| **Tipo de Gramática** | Gramática Regular | Gramática Livre de Contexto |
| **Formato das Regras** | Restrito (ex.: $A \rightarrow aB$ ou $A\rightarrow a$) | Irrestrito (ex.: $A \rightarrow \beta$, onde $\beta$ é qualquer _string_) |
| **Autômato Reconhecedor** | Autômato Finito (AF) | Autômato com Pilha (AP) |
| **Capacidade de _Memória_** | Nenhuma (limitada a estados finitos) | Ilimitada (via pilha LIFO) |
| **Exemplo Característico** | $a(ba)^∗$ | $a^n b^n$ para $n≥0$ |
| **Exemplo Não-Pertencente** | $a^n b^n$ para $n≥0$ | $a^n b^n c^n$ para $n≥0$ |

: Resumo das características e diferenças das linguagens regulares e livres de contexto.{#tbl-resumo1}

## A Anatomia das Gramáticas Livres de Contexto {#sec-anatomia-gramaticas-livres-de-contexto}

A atenta leitora verá que para analisar e processar linguagens com estruturas aninhadas, é imprescindível o uso de um formalismo matemático preciso. Uma Gramática Livre de Contexto fornece a base formal necessária.

O termo _livre de contexto_ é fundamental para a compreensão da natureza das gramáticas que a atenta leitora estudará neste capítulo. Esse termo significa que a aplicação de uma regra de produção $A\rightarrow \beta$ a um símbolo não-terminal $A$ é incondicional; ela pode ocorrer independentemente dos símbolos que cercam $A$, seu _contexto_, em uma forma sentencial intermediária. Esta propriedade simplifica a análise sintática em comparação com gramáticas mais complexas, como as sensíveis ao contexto (Tipo 1).

Formalmente, uma Gramática Livre de Contexto é definida como uma quádrupla $G=(N,\Sigma,P,S)$, na qual cada componente é um conjunto e tem um papel específico na geração das _string_s da linguagem. Os quatro componentes da tupla que define uma Gramáticas Livres de Contextos são:

1. **N: O Conjunto de Símbolos Não Terminais.** Este é um conjunto finito de variáveis que representam as diferentes construções sintáticas ou categorias gramaticais da linguagem. Por exemplo, em uma gramática para uma linguagem de programação, os não terminais podem incluir EXPRESSÃO, COMANDO e DECLARAÇÃO. Eles são os elementos que podem ser substituídos ou expandidos durante o processo de derivação e que aqui, neste livro, serão representados por letras maiúsculas do alfabeto latino.

2. **$\Sigma$: O Conjunto de Símbolos Terminais.** Este é um conjunto finito, disjunto de *N*, que constitui o alfabeto da linguagem. Os terminais são os símbolos literais, os _átomos_ que compõem as _string_s finais da linguagem e não podem ser mais decompostos. Exemplos incluem palavras-chave (`if`, `while`, etc.), operadores (`+`, `\`, `*`, etc.) e identificadores (`contador`, `salario`, etc.).

3. **P: O Conjunto de Regras de Produção.** Este é um conjunto finito de regras que definem como os não terminais podem ser substituídos. Cada regra tem a forma $A \rightarrow β$, onde $A\in N$ é um único não-terminal (a _cabeça_ da produção) e $\beta \in (N∪Σ)^∗$ é uma _string_, possivelmente vazia, de símbolos terminais e/ou não terminais que chamaremos de _corpo_ da produção. Essas regras são o motor do sistema gerativo.

4. **S: O Símbolo Inicial.** Um não-terminal especial, $S \in N$, que serve como ponto de partida para todas as derivações. Ele geralmente representa a construção sintática mais abrangente da linguagem, como um programa ou uma sentença.

A linguagem gerada por uma gramática $G$, denotada por $L(G)$, é o conjunto de todas as _string_s de símbolos terminais que podem ser derivadas a partir do símbolo inicial $S$ por meio da aplicação sucessiva das regras de produção em $P$.

Embora a definição formal de uma Gramáticas Livres de Contextos seja inerentemente gerativa, descrevendo como construir sentenças válidas a partir de $S$, sua aplicação primária em compiladores é reconhecedora. O objetivo de um analisador sintático não é gerar programas aleatórios, mas sim verificar se uma dada sequência de _tokens_, produzida pelo programador, pode ser gerada pela gramática. A gramática, portanto, atua como a especificação formal contra a qual o processo de reconhecimento é executado. O parser, na prática, tenta reverter o processo de derivação para validar a estrutura do programa fonte.

### Exemplo Canônico: A Linguagem dos Palíndromos

Para solidificar esses conceitos abstratos, vamos construir uma Gramáticas Livres de Contextos para a linguagem dos palíndromos sobre o alfabeto $\Sigma = \{0,1\}$. Só para refrescar a memória: um palíndromo é uma _string_ que se lê da mesma forma da esquerda para a direita e da direita para a esquerda.

A estrutura recursiva dos palíndromos pode ser definida da seguinte forma:

1. **Casos Base**: A _string_ vazia ($\epsilon$), `0` e `1` são palíndromos. 
2. **Passo Indutivo**: Se $w$ é um palíndromo, então $0w0$ e $1w1$ também são palíndromos.

Esta definição recursiva pode ser traduzida diretamente em um conjunto de regras de produção para uma Gramáticas Livres de Contextos. Seja $K$ o nosso símbolo não-terminal para _palíndromo_:

1. $K \rightarrow \epsilon$  
2. $K \rightarrow 0$  
3. $K \rightarrow 1$  
4. $K \rightarrow 0K0$  
5. $K \rightarrow 1K1$

Neste exemplo, a gramática completa $G_{pal}$ é definida pela quádrupla:

* $N = \{K\}$
* $\Sigma = \{0,1\}$
* $P$ é o conjunto das cinco regras listadas acima.
* $S = K$

Esta gramática pode gerar qualquer palíndromo sobre $\{0,1\}$. Por exemplo, a _string_ _0110_ pode ser derivada da seguinte forma: 

A derivação da _string_ `0110` a partir do símbolo inicial $K$ é feita aplicando-se as regras da gramática sequencialmente.

1. **Passo 1: Iniciar com o símbolo inicial.**
    * Começamos com o símbolo inicial da gramática, que é $K$.
    * $K$

2. **Passo 2: Aplicar a Regra 4 ($K \rightarrow 0K0$).**
    * Para gerar uma _string_ que começa e termina com `0`, aplicamos a regra 4.
    * $K \Rightarrow 0K0$

3. **Passo 3: Aplicar a Regra 5 ($K \rightarrow 1K1$).**
    * Agora, precisamos gerar a parte interna do palíndromo. Substituímos o $K$ restante pela regra 5 para obter os `1`s internos.
    * $0K0 \Rightarrow 0(1K1)0 = 01K10$

4. **Passo 4: Aplicar a Regra 1 ($K \rightarrow \epsilon$).**
    * O centro do palíndromo `0110` é vazio. Para finalizar a derivação, substituímos o último $K$ pela cadeia vazia, $\epsilon$ (epsilon), usando a regra 1.
    * $01K10 \Rightarrow 01(\epsilon)10 = 0110$

A sequência completa da derivação será:

$$K \Rightarrow 0K0 \Rightarrow 01K10 \Rightarrow 01\epsilon10 \Rightarrow 0110$$

### Exercícios de Derivação {#sec-execicios-derivacao}

#### Exercício 1: Palíndromo Ímpar

Dada a gramática de palíndromos $G_{pal}$:

- $N = \{K\}$
- $\Sigma = \{0,1\}$
- $P = \{ K \rightarrow \epsilon, K \rightarrow 0, K \rightarrow 1, K \rightarrow 0K0, K \rightarrow 1K1 \}$
- $S = K$

Faça a derivação da *string* `101`.

#### Exercício 2: Expressão Aritmética Simples

Considere uma gramática simplificada para expressões aritméticas, $G_{exp}$ dada por:

* $N = \{E\}$
* $\Sigma = \{id, +, *, (, )\}$
* $P = \{ E \rightarrow E + E, E \rightarrow E * E, E \rightarrow (E), E \rightarrow id \}$
* $S = E$

Faça a derivação da *string* `id * id + id`.

#### Exercício 3: Palíndromo de Comprimento Par e Aninhado

Usando a gramática de palíndromos $G_{pal}$ dada a seguir, faça a derivação da *string* `011110`.

- $N = \{K\}$
- $\Sigma = \{0,1\}$
- $P = \{ K \rightarrow \epsilon, K \rightarrow 0, K \rightarrow 1, K \rightarrow 0K0, K \rightarrow 1K1 \}$
- $S = K$

#### Exercício 4: Linguagem $a^nb^n$

Considere a gramática $G_{ab}$ que gera *strings* com um número de `a``s seguido pelo mesmo número de `b``s:

* $N = \{S\}$
* $\Sigma = \{a, b\}$
* $P = \{ S \rightarrow aSb, S \rightarrow \epsilon \}$
* $S = S$

Faça a derivação da *string* `aaabbb`.

#### Exercício 5: Comando Condicional `if-else`

Seja uma gramática para um comando `if-else` simplificado, $G_{if}$:

* $N = \{C, A\}$
* $\Sigma = \{ \text{if}, \text{then}, \text{else}, id, :=, 0 \}$
* $P = \{ C \rightarrow \text{if } id \text{ then } A \text{ else } A, A \rightarrow id := 0 \}$
* $S = C$

Faça a derivação da *string* `if id then id := 0 else id := 0`.

#### Exercício 6: Parênteses Balanceados

Considere a gramática $G_{par}$ para gerar sequências de parênteses balanceados:

* $N = \{B\}$
* $\Sigma = \{ (, ) \}$
* $P = \{ B \rightarrow (B), B \rightarrow BB, B \rightarrow \epsilon \}$
* $S = B$

Faça a derivação da *string* `()(())`.

#### Exercício 7: Palíndromo Vazio

Usando a gramática de palíndromos $G_{pal}$ do primeiro exercício, faça a derivação da *string* vazia, $\epsilon$.

## Geração de Sentenças: Derivações, Árvores e Ambiguidade

Uma **derivação** é a sequência de passos que transforma o símbolo inicial em uma _string_ final de terminais por meio da aplicação das regras de produção. Exatamente o processo que a esforçada leitora aprendeu na seção @sec-anatomia-gramaticas-livres-de-contexto. Em cada passo, um não-terminal é escolhido e substituído pelo corpo de uma da regras de produção deste não-terminal. Como uma forma sentencial intermediária pode conter múltiplos não terminais, precisaremos de uma  convenção para determinar qual deles expandir. Isso leva a duas estratégias de derivação:

1. **Derivação Mais à Esquerda (*Leftmost Derivation*)**: em cada passo, o símbolo não-terminal que aparece mais à esquerda na forma sentencial é sempre o escolhido para ser substituído.  
2. **Derivação Mais à Direita (*Rightmost Derivation*)**: em cada passo, o símbolo não-terminal que aparece mais à direita é o escolhido para ser substituído.

Para uma gramática não ambígua, embora as sequências de passos sejam diferentes, tanto a derivação mais à esquerda quanto a mais à direita para uma dada sentença resultarão na mesma estrutura sintática. Como exemplo, a atenta leitora pode estudar o exemplo a seguir:

**Exemplo 1**: Gramática Não Ambígua para Expressões Aritméticas

Considere a seguinte gramática:

* **Símbolos não-terminais (N)**: $\{E, T, F\}$
* **Símbolos terminais (\Sigma)**: $\{id, +, *, (, )\}$
* **Símbolo inicial (S)**: $E$
* **Regras de produção (P)**:
  1. $E \rightarrow E + T$
  2. $E \rightarrow T$
  3. $T \rightarrow T * F$
  4. $T \rightarrow F$
  5. $F \rightarrow (E)$
  6. $F \rightarrow id$

Vamos fazer a Derivação da string "id + id * id", primeiro usando a derivação Mais à Esquerda (_Leftmost_):

$$
\begin{align}
E &\Rightarrow E + T                &&\text{[regra 1: expandir E]}\
  &\Rightarrow T + T                &&\text{[regra 2: expandir E mais à esquerda]}\
  &\Rightarrow F + T                &&\text{[regra 4: expandir T mais à esquerda]}\
  &\Rightarrow id + T               &&\text{[regra 6: expandir F mais à esquerda]}\
  &\Rightarrow id + T * F           &&\text{[regra 3: expandir T]}\
  &\Rightarrow id + F * F           &&\text{[regra 4: expandir T mais à esquerda]}\
  &\Rightarrow id + id * F          &&\text{[regra 6: expandir F mais à esquerda]}\
  &\Rightarrow id + id * id         &&\text{[regra 6: expandir F]}
\end{align}
$$

Agora vamos derivar a mesma _string_ usando a derivação Mais à Direita (_Rightmost_):

$$
\begin{align}
E &\Rightarrow E + T                &&\text{[regra 1: expandir E]}\
  &\Rightarrow E + T * F            &&\text{[regra 3: expandir T mais à direita]}\
  &\Rightarrow E + T * id           &&\text{[regra 6: expandir F mais à direita]}\
  &\Rightarrow E + F * id           &&\text{[regra 4: expandir T mais à direita]}\
  &\Rightarrow E + id * id          &&\text{[regra 6: expandir F mais à direita]}\
  &\Rightarrow T + id * id          &&\text{[regra 2: expandir E]}\
  &\Rightarrow F + id * id          &&\text{[regra 4: expandir T]}\
  &\Rightarrow id + id * id         &&\text{[regra 6: expandir F]}
\end{align}
$$

Observando as duas derivações é possível perceber que a gramática usada neste exemplo, não é ambígua. Observe que:

1. **Precedência clara**: A multiplicação $(*)$ tem precedência maior que a adição $(+)$
   * $T$ (termo) gera multiplicações
   * $E$ (expressão) gera adições de termos

2. **Associatividade definida**: Operadores associam à esquerda
   * $E \rightarrow E + T$ (não $E \rightarrow T + E$)
   * $T \rightarrow T * F$ (não $T \rightarrow F * T$)

3. **Estrutura única**: Para qualquer string válida, existe exatamente uma árvore de derivação,
   independentemente da ordem de derivação (leftmost ou rightmost).

A atenta leitora deve notar que embora a sequência de passos seja diferente nas duas derivações, ambas produzem a mesma estrutura sintática: $id + (id * id)$, onde a multiplicação tem precedência sobre a adição.

### A Construção de Árvores de Derivação

O processo mais intuitivo de visualizar a estrutura hierárquica imposta por uma gramática a uma sentença é por meio de uma **árvore de derivação**, ou **árvore sintática**. Uma árvore de derivação é uma representação gráfica de uma derivação que abstrai a ordem em que as produções foram aplicadas. Suas propriedades são definidas por:

1. A raiz da árvore é rotulada com o símbolo inicial $S$.
2. Cada nó interno é rotulado com um símbolo não-terminal.
3. Cada folha é rotulada com um símbolo terminal ou com $\epsilon$.
4. Se um nó interno é rotulado com $A$ e seus filhos, da esquerda para a direita, são rotulados com $X_1$, $X_2$, ..., $X_n$, então deve existir uma regra de produção $A \rightarrow X_1 X_2 ... X_n$ na gramática.

A concatenação das folhas da árvore, lidas da esquerda para a direita, forma a sentença gerada, também conhecida como _yield_ da árvore. A árvore de derivação captura a estrutura sintática essencial da sentença, tornando explícitas as relações entre suas subpartes.

**Exemplo 2:** Gramática Não Ambígua para Listas {#sec-derivacao-listas-exemplo2}

Considere a gramática definida como:

- **Símbolos não-terminais (N)**: $\{L, E\}$
- **Símbolos terminais (Σ)**: $\{a, b, [, ], ,\}$
- **Símbolo inicial (S)**: $L$
- **Regras de produção (P)**:
  1. $L \rightarrow [E]$
  2. $L \rightarrow [\,]$
  3. $E \rightarrow E, a$
  4. $E \rightarrow E, b$
  5. $E \rightarrow a$
  6. $E \rightarrow b$

Vamos derivar a _string_ "[a, b, a]". Primeiro com a Derivação Mais à Esquerda (_Leftmost_):

$$
\begin{align}
L &\Rightarrow [E]                  &&\text{[regra 1: expandir L]}\\
  &\Rightarrow [E, a]               &&\text{[regra 3: expandir E]}\\
  &\Rightarrow [E, b, a]            &&\text{[regra 4: expandir E mais à esquerda]}\\
  &\Rightarrow [a, b, a]            &&\text{[regra 5: expandir E mais à esquerda]}
\end{align}
$$

Agora com a Derivação Mais à Direita (_Rightmost_):

$$
\begin{align}
L &\Rightarrow [E]                  &&\text{[regra 1: expandir L]}\\
  &\Rightarrow [E, a]               &&\text{[regra 3: expandir E]}\\
  &\Rightarrow [E, b, a]            &&\text{[regra 4: expandir E]}\\
  &\Rightarrow [a, b, a]            &&\text{[regra 5: expandir E]}
\end{align}
$$

A @fig-deriva1 apresenta as duas árvores de derivação para a string "[a, b, a]".

:::{#fig-deriva1}
![](images/deriva1.webp)

Apresentação das duas árvores de derivação de "[a, b, a]" de forma gráfica.
:::

**Exemplo 3**: Considerando a gramática do Exemplo 2 @sec-derivacao-listas-exemplo2, faça a derivação da string "[b]"

Novamente, começando com a Derivação Mais à Esquerda (_Leftmost_):

$$
\begin{align}
L &\Rightarrow [E]                  &&\text{[regra 1: expandir L]}\\
  &\Rightarrow [b]                  &&\text{[regra 6: expandir E]}
\end{align}
$$

Finalmente, a Derivação Mais à Direita (Rightmost):

$$
\begin{align}
L &\Rightarrow [E]                  &&\text{[regra 1: expandir L]}\\
  &\Rightarrow [b]                  &&\text{[regra 6: expandir E]}
\end{align}
$$

A atenta leitora deve perceber que esta gramática não é ambígua. Considerando que:

1. **Estrutura hierárquica clara**:
   * $L$ gera apenas a estrutura de lista com colchetes;
   * $E$ gera apenas a sequência de elementos separados por vírgula.

2. **Associatividade única**:
   * As regras $E \rightarrow E, a$ e $E \rightarrow E, b$ forçam associatividade à esquerda;
   * Não há regras como $E \rightarrow a, E$ que criariam ambiguidade.

3. **Sem sobreposição de produções**:
   * Cada não-terminal tem um papel específico e não conflitante;
   * Lista vazia $[\,]$ é tratada separadamente, evitando ambiguidade.

**Nota**: A gramática usada nos exemplos 2 e 3 garante que elementos são adicionados sempre à direita da lista, construindo-a da esquerda para a direita. A estrutura $[a, b, a]$ só pode ser interpretada de uma forma: uma lista contendo três elementos na ordem especificada.

### **O Problema da Ambiguidade**

**Uma gramática é considerada ambígua se existe pelo menos uma sentença em sua linguagem que pode ser gerada por duas ou mais árvores de derivação distintas**. Equivalentemente, uma gramática é ambígua se alguma sentença possui duas ou mais derivações mais à esquerda, ou mais à direita, distintas.

A ambiguidade é um problema grave no projeto de linguagens de programação. A existência de uma ambiguidade implica que uma única sentença pode ter múltiplas interpretações sintáticas e, consequentemente, múltiplos significados semânticos.

* Exemplo Clássico (Expressões Aritméticas): Considere a gramática $G_{expr}$:

  $E \rightarrow E + E \mid E * E \mid id$

  A sentença _id + id * id_ pode ser derivada de duas maneiras 23:  
  1. $E \Rightarrow E + E \Rightarrow id + E \Rightarrow id + E * E \Rightarrow \ldots \Rightarrow id + id * id$ (Interpretação: $id + (id * id)$)  
  2. $E \Rightarrow E * E \Rightarrow E + E * E \Rightarrow \ldots \Rightarrow id + id * id$ (Interpretação: $(id + id) * id$)  

  Cada derivação corresponde a uma árvore de derivação diferente, uma impondo a precedência da multiplicação e a outra, a da adição.

**Exemplo 1**: em algumas linguagens de programação, a construção `if-then-else` pode levar à ambiguidade. Para a sentença `if (c1) if (c2) s1 else s2`, não fica claro se o `else` está associado ao primeiro `if` ou ao segundo. Uma gramática ambígua permite ambas as interpretações, o que pode levar a erros lógicos graves e difíceis de depurar em tempo de execução. Chamamos a este problema de _Dangling Else_.

A ambiguidade sintática é a causa raiz da ambiguidade semântica. A estrutura da árvore de derivação não é um mero artefato teórico; ela dita diretamente a ordem de avaliação e, portanto, o *significado* do programa. As fases subsequentes do compilador, como a análise semântica e a geração de código, operam sobre a estrutura hierárquica fornecida pela árvore. Se múltiplas árvores são possíveis, o compilador não tem como determinar a intenção do programador, tornando impossível a geração de um código correto e determinístico. Por essa razão, a eliminação da ambiguidade, tipicamente por meio da reescrita da gramática para impor regras de precedência e associatividade de operadores, não é uma formalidade, mas um pré-requisito absoluto para a construção de um compilador funcional. A sintaxe precede e comanda a semântica.

## Autômatos com Pilha: A Máquina por Trás das Linguagens Livres de Contexto

Como estabelecido anteriormente, os autômatos finitos, os reconhecedores para linguagens regulares, são fundamentalmente limitados por sua falta de memória. A capacidade de _lembrar_ está restrita ao conjunto finito de estados da máquina. Essa limitação os impede de reconhecer linguagens que exigem a correspondência de símbolos ou contagem, como a linguagem $L=\{a^n b^n \mid n \geq 0\}$.

Para reconhecer a classe mais ampla das linguagens livres de contexto, é necessário um modelo de computação mais poderoso. O **autômato com pilha (AP)** é esse modelo. Um AP pode ser concebido como um autômato finito não determinístico (AFND) ao qual foi adicionada uma memória auxiliar: uma **pilha** (*stack*).

A pilha é uma estrutura de dados com acesso restrito, operando no modo LIFO (*Last-In, First-Out*), o que significa que o último elemento inserido é o primeiro a ser removido. As transições de um AP são mais complexas que as de um AF. A decisão de qual transição tomar depende de três fatores: o estado atual, o próximo símbolo na _string_ de entrada e o símbolo que está no topo da pilha.

Em cada transição, além de mudar de estado e consumir um símbolo de entrada (opcionalmente), o AP pode realizar uma de três operações na pilha:

1. **Empilhar (*Push*)**: Adicionar um ou mais símbolos ao topo da pilha.
2. **Desempilhar (*Pop*)**: Remover o símbolo do topo da pilha.
3. **Manter**: Não alterar o conteúdo da pilha.

Esta capacidade de armazenar e recuperar informações de forma estruturada confere ao AP seu poder computacional superior.

### A Equivalência Fundamental

O resultado mais importante da teoria das Linguagens Livres de Contexto é a equivalência formal entre gramáticas livres de contexto e autômatos com pilha. Uma linguagem é livre de contexto se, e somente se, existe um autômato com pilha que a reconhece.

Esta equivalência é a espinha dorsal da análise sintática. Ela garante que para qualquer sintaxe de linguagem de programação que possa ser descrita por uma Gramáticas Livres de Contextos, podemos construir um mecanismo computacional o Autômato de Pilha para reconhecer programas escritos nessa linguagem. É importante notar que a classe de linguagens reconhecidas por Autômatos de Pilha não determinísticos é estritamente maior que a classe reconhecida por Autômatos de Pilha determinísticos. São os Autômatos de Pilha não determinísticos que são equivalentes em poder às Gramáticas Livres de Contextos em geral.

A escolha de uma pilha como o mecanismo de memória para reconhecer Linguagens Livres de Contexto não é acidental. A estrutura LIFO de uma pilha espelha perfeitamente a natureza recursiva e aninhada das derivações em uma Gramáticas Livres de Contextos. Considere novamente a gramática S→aSb para a linguagem anbn. A derivação de aabb é S⇒aSb⇒aaSbb⇒aabb. Observe como a estrutura se expande simetricamente _de dentro para fora_. Um AP para esta linguagem implementa essa simetria de forma operacional: ao ler um `a`, ele empilha um símbolo de marcador (ex.: X); ao ler o próximo `a`, empilha outro X. Quando começa a ler os `b`s, ele desempilha um X para cada `b` lido. Se a entrada terminar exatamente quando a pilha se esvaziar, a _string_ é aceita. O processo de empilhar na primeira metade e desempilhar na ordem inversa na segunda metade é a encarnação mecânica da recursão gramatical. A pilha _lembra_ as obrigações sintáticas (gerar um `b` correspondente para cada `a`) e as descarrega na ordem correta, tornando-a a estrutura de dados canónica para processar estruturas livres de contexto.

## As Fronteiras do Contexto Livre: O Lema do Bombeamento

Assim como existe uma ferramenta para provar que uma linguagem não é regular, existe um análogo para as linguagens livres de contexto: o **Lema do Bombeamento para Linguagens Livres de Contexto**, também conhecido como Lema de Bar-Hillel.32 Sua principal aplicação é demonstrar, por contradição, que uma determinada linguagem

*não* é livre de contexto.34

A intuição por trás do lema está enraizada na estrutura finita das gramáticas e na natureza das árvores de derivação. Para uma Gramáticas Livres de Contextos com um número finito de não terminais, qualquer _string_ suficientemente longa gerada por ela deve ter uma árvore de derivação _alta_. Pelo princípio da casa dos pombos, um caminho longo da raiz a uma folha nessa árvore deve necessariamente conter pelo menos um não-terminal repetido. Essa repetição cria uma sub-árvore que pode ser excisada ou duplicada, _bombeando_ a _string_ de uma maneira específica.35

O lema afirma formalmente que para qualquer linguagem livre de contexto L, existe um inteiro p≥1 (o _comprimento de bombeamento_) tal que qualquer _string_ s∈L com comprimento ∣s∣≥p pode ser decomposta em cinco sub_string_s, s=uvxyz, que devem satisfazer as seguintes três condições 32:

1. ∣vxy∣≤p: A sub_string_ que contém as partes bombeáveis não é excessivamente longa. 
2. ∣vy∣≥1: Pelo menos uma das duas sub_string_s bombeáveis (v ou y) não é vazia. Isso garante que o bombeamento realmente altera a _string_. 
3. uvnxynz∈L para todo inteiro n≥0: As duas sub_string_s v e y podem ser bombeadas (repetidas) em conjunto um número arbitrário de vezes (incluindo zero, o que corresponde a removê-las), e a _string_ resultante permanecerá na linguagem L.

### Aplicação Prática: Prova de que L \= {aⁿbⁿcⁿ | n ≥ 0} não é Livre de Contexto

A linguagem L={anbncn∣n≥0} é o exemplo canônico de uma linguagem que está além do alcance das Gramáticas Livres de Contextos. Podemos provar isso rigorosamente usando o lema do bombeamento.

A prova segue por contradição:

1. **Suposição**: Suponha que L é livre de contexto. 
2. **Invocação do Lema**: Pelo lema, deve existir um comprimento de bombeamento p. 
3. **Escolha da _string_**: Selecionamos a _string_ s=apbpcp. Claramente, s∈L e seu comprimento, 3p, é maior ou igual a p. 
4. **Análise da Decomposição**: O lema garante que s pode ser decomposta como s=uvxyz, sujeita às condições do lema. A condição ∣vxy∣≤p é a chave. Dada a estrutura de s (um bloco de `a`s, seguido por um bloco de `b`s, seguido por um bloco de `c`s), esta condição implica que a sub_string_ vxy não pode conter ocorrências de todos os três símbolos (`a`, `b` e `c`). Ela pode estar inteiramente dentro do bloco de `a`s, inteiramente dentro do de `b`s, ou abranger a fronteira entre `a`s e `b`s, ou entre `b`s e `c`s. 
5. **Contradição**: A condição ∣vy∣≥1 garante que o bombeamento adicionará (ou removerá) pelo menos um símbolo. Vamos considerar o bombeamento para cima, com n=2, resultando na _string_ s′=uv2xy2z. 
   * Se vxy contivesse apenas `a`s, então v e y conteriam apenas `a`s. A _string_ s′ teria mais `a`s do que `b`s e `c`s, violando a condição da linguagem. 
   * Se vxy contivesse uma mistura de `a`s e `b`s, então v e y poderiam conter `a`s e `b`s, mas nenhum `c`. A _string_ s′ teria um número aumentado de `a`s e/ou `b`s, mas o número de `c`s permaneceria p. Novamente, a igualdade n=n=n seria quebrada. 
   * O mesmo raciocínio se aplica a todas as outras localizações possíveis de vxy. Em nenhum caso, o bombeamento pode aumentar o número de `a`s, `b`s e `c`s na mesma proporção. 
6. **Conclusão**: A _string_ bombeada s′ não pertence a L. Isso contradiz a terceira condição do lema. Portanto, a suposição inicial de que L é livre de contexto deve ser falsa.

A estrutura de bombeamento duplo (v e y) do lema não é arbitrária. Ela revela a limitação fundamental das Linguagens Livres de Contexto a dependências de, no máximo, _dois pontos_. Uma Gramáticas Livres de Contextos, por meio de recursão como S→aSb, pode correlacionar duas partes de uma _string_ (os `a`s no início e os `b`s no fim), que correspondem às partes v e y que são bombeadas em conjunto. A linguagem anbncn exige uma dependência de _três pontos_. Um autômato com pilha _gasta_ sua memória para verificar a correspondência entre `a`s e `b`s, não restando capacidade para verificar os `c`s contra a contagem original. O lema do bombeamento formaliza essa limitação, mostrando que o bombeamento inevitavelmente quebra essa dependência tripla.

## Os Analisadores Sintáticos

A análise sintática, ou *parsing*, é a segunda fase do processo de compilação, posicionada entre a análise léxica e a análise semântica.16 Seu papel é fundamental: atua como o guardião da gramática da linguagem. Enquanto o analisador léxico verifica a _ortografia_ (se as palavras, ou _tokens_, são válidas), o parser verifica a _gramática_ (se a sequência de _tokens_ forma sentenças estruturalmente corretas).17 O objetivo principal do parser é determinar se o fluxo de _tokens_ de entrada pode ser gerado pela gramática livre de contexto que define a linguagem e, em caso afirmativo, construir uma representação explícita dessa estrutura.

A entrada para o analisador sintático é o fluxo de _tokens_ produzido pelo analisador léxico. A saída, para um programa sintaticamente correto, é uma estrutura de dados que representa a estrutura hierárquica do código. Embora a árvore de derivação seja a representação teórica direta, na prática, os compiladores constroem uma **Árvore Sintática Abstrata (AST)**. A AST é uma versão condensada e mais abstrata da árvore de derivação, que omite detalhes sintáticos intermediários (como parênteses para agrupamento ou não terminais que apenas passam a derivação adiante) e captura a estrutura lógica e semântica essencial do programa, tornando-a mais adequada para as fases subsequentes de análise e geração de código.16

A maneira como a árvore de derivação é construída em relação à entrada define as duas principais estratégias de parsing, cada uma com suas próprias características, pontos fortes e limitações 41:

1. **Análise Descendente (*Top-Down Parsing*)**: A construção da árvore de derivação começa no topo (a raiz, que é o símbolo inicial da gramática) e avança para baixo, em direção às folhas (a _string_ de _tokens_ de entrada). Este método tenta encontrar a derivação mais à esquerda para a entrada.41  
2. **Análise Ascendente (*Bottom-Up Parsing*)**: A construção da árvore de derivação começa na base (as folhas, que são a _string_ de _tokens_ de entrada) e avança para cima, em direção à raiz (o símbolo inicial). Este método efetivamente reverte uma derivação mais à direita.41

Um erro sintático detectado pelo parser é um erro fatal que interrompe o processo de compilação. Sem uma estrutura sintática válida e inequívoca representada pela AST, as fases subsequentes, que dependem dessa estrutura para realizar a verificação de tipos e a geração de código, não podem prosseguir.

### Estratégias de Análise Descendente (_Top-Down_)

A análise descendente tenta construir uma árvore de derivação para a _string_ de entrada começando pela raiz (símbolo inicial) e criando os nós da árvore em pré-ordem. Isso equivale a encontrar uma derivação mais à esquerda para a _string_ de entrada.

#### **8.2 Analisadores de Descida Recursiva (Recursive-Descent)**

Uma das implementações mais diretas e intuitivas de um parser descendente é o analisador de descida recursiva. Nesta abordagem, um conjunto de procedimentos mutuamente recursivos é escrito, geralmente um para cada não-terminal na gramática. O procedimento associado a um não-terminal A é responsável por reconhecer na entrada uma sub_string_ que pode ser derivada de A.43 A simplicidade e a facilidade de implementação manual tornam esta técnica atraente. No entanto, analisadores de descida recursiva ingênuos podem ser ineficientes. Estes analisadores podem exigir retrocesso ( *backtracking*) se a escolha de uma produção se revelar incorreta. Além disso, eles não conseguem lidar com gramáticas que contêm recursão à esquerda (regras da forma A→Aβ). Isso levaria a uma recursão infinita.

#### **8.3 Analisadores Preditivos (LL)**

Para superar as desvantagens do retrocesso, foi desenvolvida uma classe de parsers descendentes chamada de **analisadores preditivos**. Estes são parsers que podem _prever_ qual produção aplicar a um não-terminal olhando para a frente na _string_ de entrada, sem precisar adivinhar e retroceder.44 O tipo mais comum é o

**parser LL(1)**.45 A notação LL(1) significa:

* **L** (primeiro): A entrada é lida da **E**squerda (*Left*) para a direita. 
* **L** (segundo): O parser constrói uma derivação mais à **E**squerda (*Leftmost*). 
* **(1)**: Ele usa **1** símbolo de *lookahead* (antecipação) para tomar suas decisões.

Um parser LL(1) opera com três componentes: uma **pilha de análise**, um **ponteiro de entrada** e uma **tabela de análise**.42 A tabela de análise é uma matriz onde as linhas correspondem aos não terminais e as colunas aos terminais. Cada célula

M\[A,a\] contém a regra de produção que deve ser usada se o não-terminal A estiver no topo da pilha e o terminal a for o próximo símbolo de entrada (o *lookahead*). O algoritmo é determinístico: para cada par (não-terminal no topo da pilha, símbolo de lookahead), há no máximo uma ação a ser tomada. Se a célula estiver vazia, um erro sintático é detectado.

Os parsers LL são _ansiosos_. No momento em que um não-terminal A está no topo da pilha, eles devem se comprometer *imediatamente* com uma única regra A→β, baseando sua decisão exclusivamente no próximo token de entrada. Essa necessidade de uma decisão precoce e inequívoca é a razão pela qual as gramáticas LL(1) não podem ter ambiguidades, recursão à esquerda ou prefixos comuns (duas produções para o mesmo não-terminal que começam com o mesmo símbolo). Tais características tornam impossível para o parser fazer uma escolha determinística com apenas um símbolo de lookahead.

| Tabela de Análise LL(1) para uma Gramática de Expressões Simplificada |  
| :--- | id | \+ | \* | ( | ) | $ |  
| E | E→TE′ | | | E→TE′ | | |  
| E` | | E′→+TE′ | | | E′→ϵ | E′→ϵ |  
| T | T→FT′ | | | T→FT′ | | |  
| T` | | T′→ϵ | T′→∗FT′ | | T′→ϵ | T′→ϵ |  
| F | F→id | | | F→(E) | | |

### **Capítulo 9: Estratégias de Análise Ascendente (Bottom-Up)**

#### **9.1 Conceito**

Em contraste com a abordagem descendente, a análise ascendente constrói a árvore de derivação a partir das folhas (a _string_ de entrada) em direção à raiz (o símbolo inicial). O processo pode ser visto como uma tentativa de _reduzir_ a _string_ de entrada de volta ao símbolo inicial, essencialmente traçando uma derivação mais à direita ao contrário.41

#### **9.2 A Abordagem Shift-Reduce**

O mecanismo fundamental por trás da maioria dos parsers ascendentes é o **shift-reduce** (*deslocar-reduzir*). O parser utiliza uma pilha para armazenar símbolos da gramática e toma uma de quatro ações possíveis em cada passo 49:

1. **Shift (Deslocar)**: O próximo símbolo de entrada é movido (deslocado) para o topo da pilha. 
2. **Reduce (Reduzir)**: O parser reconhece que uma sequência de símbolos β no topo da pilha corresponde ao lado direito de uma regra de produção A→β. Ele então substitui (reduz) β na pilha pelo não-terminal A. 
3. **Accept (Aceitar)**: A análise é concluída com sucesso. Isso ocorre quando a entrada foi totalmente consumida e a pilha contém apenas o símbolo inicial. 
4. **Error (Erro)**: Um erro sintático é encontrado, e o parser não pode continuar.

A principal dificuldade em um parser shift-reduce é decidir quando deslocar e quando reduzir (um **conflito shift/reduce**) ou, ao decidir reduzir, qual regra usar se múltiplas corresponderem (um **conflito reduce/reduce**).49

#### **9.3 A Família de Analisadores LR**

A classe mais poderosa e amplamente utilizada de parsers ascendentes é a família **LR**. Eles são capazes de analisar uma classe de gramáticas significativamente maior do que os parsers LL.50 A notação LR significa:

* **L**: A entrada é lida da **E**squerda (*Left*) para a direita. 
* **R**: O parser constrói uma derivação mais à di**R**eita (*Rightmost*) ao contrário.

Existem várias variantes de parsers LR, que diferem principalmente na forma como suas tabelas de análise são construídas e na quantidade de informação de *lookahead* que utilizam para resolver conflitos 48:

* **LR(0)**: O mais simples, não usa lookahead. 
* **SLR (Simple LR)**: Usa os conjuntos FOLLOW do não-terminal para decidir sobre as reduções. 
* **LALR(1) (Look-Ahead LR)**: Uma versão otimizada do LR(1) com tabelas menores, mas poder de reconhecimento ligeiramente reduzido. É a base para ferramentas como YACC e Bison. 
* **LR(1) Canônico**: O mais poderoso da família, mas que gera tabelas de análise muito grandes.

Ao contrário dos parsers LL _ansiosos_, os parsers LR são _pacientes_. Eles não precisam decidir qual regra de produção usar no momento em que veem o primeiro símbolo de seu lado direito. Em vez disso, eles continuam a deslocar símbolos para a pilha até que o lado direito *completo* de uma produção (conhecido como *handle*) esteja no topo da pilha. Somente então eles realizam a redução. Essa capacidade de adiar a decisão até que mais contexto esteja disponível é a fonte de seu maior poder e de sua capacidade de lidar com uma gama mais ampla e natural de gramáticas sem a necessidade de reescritas extensivas.

| Tabela de Ação/Desvio (Action/Goto) para um Parser LR |  
| :--- | AÇÃO | DESVIO |  
| Estado | id | \+ | \* | $ | E | T |  
| 0 | s2 | | | | 1 | 3 |  
| 1 | | s4 | | acc | | |  
| 2 | | r3 | r3 | r3 | | |  
| 3 | | r1 | s5 | r1 | | |  
| 4 | s2 | | | | 6 | 3 |  
| ... |... |... |... |... |... |... |  
(s \= shift, r \= reduce, acc \= accept)

### **Capítulo 10: Automatizando a Construção: Ferramentas Geradoras de Parsers**

#### **10.1 O Conceito de _Compilador de Compiladores_**

A construção manual de um analisador sintático, especialmente um parser LR, é uma tarefa complexa, tediosa e propensa a erros. Para mitigar essa complexidade, foram desenvolvidas ferramentas especializadas conhecidas como **geradores de parsers** ou, mais ambiciosamente, _compiladores de compiladores_.52 Essas ferramentas automatizam o processo de criação de um parser a partir de uma especificação de alto nível da gramática da linguagem.

#### **10.2 YACC e GNU Bison**

As ferramentas mais conhecidas e influentes nesta categoria são YACC e seu sucessor, Bison.

* **YACC (Yet Another Compiler-Compiler)**: Desenvolvido na Bell Labs, YACC é a ferramenta canónica para gerar parsers LALR(1).52 Ele se tornou um padrão de fato em sistemas Unix. 
* **GNU Bison**: É a implementação do projeto GNU de um gerador de parsers. É amplamente compatível com YACC, mas oferece recursos adicionais, como a geração de parsers GLR (Generalized LR) para lidar com gramáticas ambíguas e a geração de código em múltiplas linguagens (C, C++, Java).53

#### **10.3 Funcionamento**

O fluxo de trabalho com YACC ou Bison é o seguinte 52:

1. **Entrada**: O desenvolvedor cria um arquivo de especificação (geralmente com a extensão .y). Este arquivo contém três seções: declarações, a gramática da linguagem escrita em uma notação semelhante à BNF, e uma seção de código auxiliar. O desenvolvedor pode associar **ações** de código (em C ou C++) a cada regra da gramática. 
2. **Processamento**: A ferramenta (YACC ou Bison) lê o arquivo de especificação. Ela analisa a gramática, constrói o autômato LR e a tabela de análise correspondente, e verifica a existência de conflitos (shift/reduce ou reduce/reduce). 
3. **Saída**: Se a gramática for adequada (ex.: LALR(1)), a ferramenta gera um arquivo de código fonte (ex.: y.tab.c) que implementa a função do parser (tipicamente chamada yyparse). Esta função implementa o algoritmo shift-reduce dirigido pela tabela gerada. As ações de código fornecidas pelo desenvolvedor são incorporadas à função e são executadas sempre que a regra correspondente é reduzida. Essas ações são tipicamente usadas para construir a Árvore Sintática Abstrata.

#### **10.4 Sinergia com Lex/Flex**

YACC/Bison são projetados para lidar com a análise sintática e quase sempre são usados em conjunto com um gerador de analisador léxico, como **Lex** ou seu sucessor **Flex**. O Flex recebe uma especificação de padrões de _tokens_ (usando expressões regulares) e gera um scanner (a função yylex). O parser gerado pelo Bison chama yylex para obter o próximo token da entrada, formando um pipeline coeso que transforma o texto fonte em uma AST.52

O uso de ferramentas como YACC e Bison representa uma mudança de paradigma fundamental na construção de compiladores: da programação *imperativa* para a *declarativa*. Em vez de implementar manualmente o complexo algoritmo de um parser shift-reduce (a abordagem imperativa), o desenvolvedor fornece uma especificação de alto nível da gramática da linguagem (a abordagem declarativa). A ferramenta se encarrega de gerar a implementação de baixo nível. Essa separação de interesses — o *quê* (a sintaxe da linguagem) do *como* (o algoritmo de parsing) — aumenta drasticamente a produtividade, a robustez e a manutenibilidade do compilador.

#### **Referências citadas**

1. terminologia \- O que é uma linguagem livre de contexto? \- Stack ..., acessado em agosto 19, 2025, [https://pt.stackoverflow.com/questions/180927/o-que-%C3%A9-uma-linguagem-livre-de-contexto](https://pt.stackoverflow.com/questions/180927/o-que-%C3%A9-uma-linguagem-livre-de-contexto)  
2. Dúvida acerca da Hierarquia de Chomsky \- Stack Overflow em Português, acessado em agosto 19, 2025, [https://pt.stackoverflow.com/questions/372178/d%C3%BAvida-acerca-da-hierarquia-de-chomsky](https://pt.stackoverflow.com/questions/372178/d%C3%BAvida-acerca-da-hierarquia-de-chomsky)  
3. Linguagem regular – Wikipédia, a enciclopédia livre, acessado em agosto 19, 2025, [https://pt.wikipedia.org/wiki/Linguagem\_regular](https://pt.wikipedia.org/wiki/Linguagem_regular)  
4. Gramática regular – Wikipédia, a enciclopédia livre, acessado em agosto 19, 2025, [https://pt.wikipedia.org/wiki/Gram%C3%A1tica\_regular](https://pt.wikipedia.org/wiki/Gram%C3%A1tica_regular)  
5. Linguagens Livres de Contexto \- Marcus Vinícius Midena Ramos, acessado em agosto 19, 2025, [https://www.marcusramos.com.br/univasf/livro-lfa-slides/cap4.pdf](https://www.marcusramos.com.br/univasf/livro-lfa-slides/cap4.pdf)  
6. Lema do Bombeamento | DECOM-UFOP, acessado em agosto 19, 2025, [http://www.decom.ufop.br/anderson/BCC242/LemaBombeamento.pdf](http://www.decom.ufop.br/anderson/BCC242/LemaBombeamento.pdf)  
7. Lema do Bombeamento \- Aplicação para Linguagens Regulares e Livres de Contexto \- facom/ufu, acessado em agosto 19, 2025, [https://www.facom.ufu.br/\~madriana/TC/LemaBomb.pdf](https://www.facom.ufu.br/~madriana/TC/LemaBomb.pdf)  
8. teoria da computação unidade 2: autômatos e linguagens aula 1: lema do bombeamento professor \- CIn UFPE, acessado em agosto 19, 2025, [https://www.cin.ufpe.br/\~lfsc/cursos/teoriadainformacao/unidade%202/cap%201\_9%20-%20lema%20do%20bombeamento.pdf](https://www.cin.ufpe.br/~lfsc/cursos/teoriadainformacao/unidade%202/cap%201_9%20-%20lema%20do%20bombeamento.pdf)  
9. Lema do Bombeamento \- Douglas O. Cardoso, acessado em agosto 19, 2025, [https://docardoso.github.io/project/lfa/04-bombeamento.pdf](https://docardoso.github.io/project/lfa/04-bombeamento.pdf)  
10. Linguagens Formais e Autômatos \- Gitea \-- aleph0, acessado em agosto 19, 2025, [https://aleph0.info/cursos/lf/notas/lfa.pdf](https://aleph0.info/cursos/lf/notas/lfa.pdf)  
11. Exercícios Resolvidos, acessado em agosto 19, 2025, [http://wiki.icmc.usp.br/images/f/f1/Gramáticas Livres de Contextos.pdf](http://wiki.icmc.usp.br/images/f/f1/Gramáticas Livres de Contextos.pdf)  
12. Linguagens formais e autômatos, acessado em agosto 19, 2025, [http://cm-kls-content.s3.amazonaws.com/201702/INTERATIVAS\_2\_0/LINGUAGENS\_FORMAIS\_E\_AUTOMATOS/U1/LIVRO\_UNICO.pdf](http://cm-kls-content.s3.amazonaws.com/201702/INTERATIVAS_2_0/LINGUAGENS_FORMAIS_E_AUTOMATOS/U1/LIVRO_UNICO.pdf)  
13. As linguagens livres de contexto são geradas por gramáticas livres de contexto? \- Academia EITCA \- EITCA Academy, acessado em agosto 19, 2025, [https://pt.eitca.org/c%C3%ADber-seguran%C3%A7a/eitc-%C3%A9-cctf-fundamentos-da-teoria-da-complexidade-computacional/gram%C3%A1ticas-e-linguagens-livres-de-contexto/introdu%C3%A7%C3%A3o-a-gram%C3%A1ticas-e-linguagens-livres-de-contexto/s%C3%A3o-linguagens-livres-de-contexto-geradas-por-gram%C3%A1ticas-livres-de-contexto/](https://pt.eitca.org/c%C3%ADber-seguran%C3%A7a/eitc-%C3%A9-cctf-fundamentos-da-teoria-da-complexidade-computacional/gram%C3%A1ticas-e-linguagens-livres-de-contexto/introdu%C3%A7%C3%A3o-a-gram%C3%A1ticas-e-linguagens-livres-de-contexto/s%C3%A3o-linguagens-livres-de-contexto-geradas-por-gram%C3%A1ticas-livres-de-contexto/)  
14. Gramática, acessado em agosto 19, 2025, [http://www.din.uem.br/yandre/TC/gramatica-mini.pdf](http://www.din.uem.br/yandre/TC/gramatica-mini.pdf)  
15. Símbolos terminais e não terminais – Wikipédia, a enciclopédia livre, acessado em agosto 19, 2025, [https://pt.wikipedia.org/wiki/S%C3%ADmbolos\_terminais\_e\_n%C3%A3o\_terminais](https://pt.wikipedia.org/wiki/S%C3%ADmbolos_terminais_e_n%C3%A3o_terminais)  
16. Análise sintática (computação) – Wikipédia, a enciclopédia livre, acessado em agosto 19, 2025, [https://pt.wikipedia.org/wiki/An%C3%A1lise\_sint%C3%A1tica\_(computa%C3%A7%C3%A3o)](https://pt.wikipedia.org/wiki/An%C3%A1lise_sint%C3%A1tica_\(computa%C3%A7%C3%A3o\))  
17. Fases Da Compilação | PDF \- Scribd, acessado em agosto 19, 2025, [https://id.scribd.com/document/92994660/Fases-da-compilacao](https://id.scribd.com/document/92994660/Fases-da-compilacao)  
18. Qual é a diferença entre uma derivação mais à esquerda e uma derivação mais à direita? \- EITCA Academy, acessado em agosto 19, 2025, [https://pt.eitca.org/c%C3%ADber-seguran%C3%A7a/eitc-%C3%A9-cctf-fundamentos-da-teoria-da-complexidade-computacional/gram%C3%A1ticas-e-linguagens-livres-de-contexto/introdu%C3%A7%C3%A3o-a-gram%C3%A1ticas-e-linguagens-livres-de-contexto/exame-revis%C3%A3o-introdu%C3%A7%C3%A3o-ao-contexto-gram%C3%A1ticas-e-idiomas-livres/qual-%C3%A9-a-diferen%C3%A7a-entre-uma-deriva%C3%A7%C3%A3o-mais-%C3%A0-esquerda-e-uma-deriva%C3%A7%C3%A3o-mais-%C3%A0-direita/](https://pt.eitca.org/c%C3%ADber-seguran%C3%A7a/eitc-%C3%A9-cctf-fundamentos-da-teoria-da-complexidade-computacional/gram%C3%A1ticas-e-linguagens-livres-de-contexto/introdu%C3%A7%C3%A3o-a-gram%C3%A1ticas-e-linguagens-livres-de-contexto/exame-revis%C3%A3o-introdu%C3%A7%C3%A3o-ao-contexto-gram%C3%A1ticas-e-idiomas-livres/qual-%C3%A9-a-diferen%C3%A7a-entre-uma-deriva%C3%A7%C3%A3o-mais-%C3%A0-esquerda-e-uma-deriva%C3%A7%C3%A3o-mais-%C3%A0-direita/)  
19. Gramáticas Livres de Contexto, acessado em agosto 19, 2025, [https://www.inf.ufrgs.br/\~johann/comp/aula06.Gramáticas Livres de Contextos.pdf](https://www.inf.ufrgs.br/~johann/comp/aula06.Gramáticas Livres de Contextos.pdf)  
20. Compiladores Capítulo 3: Análise Sintática 3.1 \- Introdução \- Facom-UFMS, acessado em agosto 19, 2025, [http://www.facom.ufms.br/\~ricardo/Courses/CompilerI-2009/Materials/Sintatic\_Analisys.pdf](http://www.facom.ufms.br/~ricardo/Courses/CompilerI-2009/Materials/Sintatic_Analisys.pdf)  
21. Árvores de Derivação | LL e LR | Ambiguidade | Compiladores \- YouTube, acessado em agosto 19, 2025, [https://www.youtube.com/watch?v=MLyFS7ClMf0](https://www.youtube.com/watch?v=MLyFS7ClMf0)  
22. Gramáticas Independentes de Contexto \- Autómatos e Linguagens de Programação, acessado em agosto 19, 2025, [https://home.uevora.pt/\~fc/alp/03-gramaticas\_automatos\_pilha/03.01-gramaticas\_independentes\_contexto.html](https://home.uevora.pt/~fc/alp/03-gramaticas_automatos_pilha/03.01-gramaticas_independentes_contexto.html)  
23. Ambigüidade, acessado em agosto 19, 2025, [https://www.inf.ufes.br/\~tavares/labcomp2000/aula63.htm](https://www.inf.ufes.br/~tavares/labcomp2000/aula63.htm)  
24. Introdução a Gramáticas e Linguagens, acessado em agosto 19, 2025, [https://web.icmc.usp.br/SCATUSU/RT/Notas\_Didaticas/nd\_10.pdf](https://web.icmc.usp.br/SCATUSU/RT/Notas_Didaticas/nd_10.pdf)  
25. Curso de Teoria da Computação \- Árvore de derivação e Ambiguidade \- YouTube, acessado em agosto 19, 2025, [https://www.youtube.com/watch?v=bbWgP\_khQZg](https://www.youtube.com/watch?v=bbWgP_khQZg)  
26. Estrutura de um Compilador · Compiladores para Humanos, acessado em agosto 19, 2025, [https://johnidm.gitbooks.io/compiladores-para-humanos/content/part1/structure-of-a-compiler.html](https://johnidm.gitbooks.io/compiladores-para-humanos/content/part1/structure-of-a-compiler.html)  
27. Compiladores \- Ciência da Computação \- Resumo da Prova PosComp, acessado em agosto 19, 2025, [https://cienciadacomputacao.wiki.br/19\_Tecnologia\_Compiladores.html](https://cienciadacomputacao.wiki.br/19_Tecnologia_Compiladores.html)  
28. Autômato com pilha – Wikipédia, a enciclopédia livre, acessado em agosto 19, 2025, [https://pt.wikipedia.org/wiki/Aut%C3%B4mato\_com\_pilha](https://pt.wikipedia.org/wiki/Aut%C3%B4mato_com_pilha)  
29. SCC-5832 \- Capítulo 2 Linguagens Livres de Contexto e Autômatos de Pilha \- USP, acessado em agosto 19, 2025, [http://wiki.icmc.usp.br/images/a/a2/SCC5832Cap2.pdf](http://wiki.icmc.usp.br/images/a/a2/SCC5832Cap2.pdf)  
30. Autômatos e Pilha & Gramáticas Livres de Contexto \- YouTube, acessado em agosto 19, 2025, [https://www.youtube.com/watch?v=1Xe2TUvah1I](https://www.youtube.com/watch?v=1Xe2TUvah1I)  
31. Aula 13 – Autômato com Pilha \- Universidade Federal de Alfenas, acessado em agosto 19, 2025, [https://www.bcc.unifal-mg.edu.br/\~humberto/disciplinas/2011\_1\_lfa/aulas/aula\_13\_AutomatoComPilha.pdf](https://www.bcc.unifal-mg.edu.br/~humberto/disciplinas/2011_1_lfa/aulas/aula_13_AutomatoComPilha.pdf)  
32. Lema do bombeamento para linguagens livre de contexto ..., acessado em agosto 19, 2025, [https://pt.wikipedia.org/wiki/Lema\_do\_bombeamento\_para\_linguagens\_livre\_de\_contexto](https://pt.wikipedia.org/wiki/Lema_do_bombeamento_para_linguagens_livre_de_contexto)  
33. Lema do bombeamento para linguagens livres de contexto \- Wikipédia, acessado em agosto 19, 2025, [https://pt.wikipedia.org/wiki/Lema\_do\_bombeamento\_para\_linguagens\_livres\_de\_contexto](https://pt.wikipedia.org/wiki/Lema_do_bombeamento_para_linguagens_livres_de_contexto)  
34. Teoria da Computação 40 \- Lema do Bombeamento para linguagens regulares \- YouTube, acessado em agosto 19, 2025, [https://www.youtube.com/watch?v=0hbLhEdxuss](https://www.youtube.com/watch?v=0hbLhEdxuss)  
35. Linguagens Livres de Contexto: Lema do Bombeamento e Propriedades de Fechamento \- Andrei Rimsa Alvares, acessado em agosto 19, 2025, [http://rimsa.com.br/documents/lectures/decom035/c2/lessons/Aula10.pdf](http://rimsa.com.br/documents/lectures/decom035/c2/lessons/Aula10.pdf)  
36. \*c\*} \= {abncn, n _\!$\# %`& (\*) \+ , \-. /+ / ` `/+-. ão fechadas sob 1, acessado em agosto 19, 2025, [https://www.cos.ufrj.br/\~rps/monitoria/lfa/com162\[1\]\_list2.pdf](https://www.cos.ufrj.br/~rps/monitoria/lfa/com162[1]_list2.pdf)  
37. Qual é a relação entre linguagens decidíveis e linguagens livres de contexto?, acessado em agosto 19, 2025, [https://pt.eitca.org/c%C3%ADber-seguran%C3%A7a/eitc-%C3%A9-cctf-fundamentos-da-teoria-da-complexidade-computacional/gram%C3%A1ticas-e-linguagens-livres-de-contexto/exemplos-de-gram%C3%A1ticas-livres-de-contexto/exemplos-de-revis%C3%A3o-de-exame-de-gram%C3%A1ticas-livres-de-contexto/qual-%C3%A9-a-rela%C3%A7%C3%A3o-entre-linguagens-decid%C3%ADveis-e-linguagens-livres-de-contexto/](https://pt.eitca.org/c%C3%ADber-seguran%C3%A7a/eitc-%C3%A9-cctf-fundamentos-da-teoria-da-complexidade-computacional/gram%C3%A1ticas-e-linguagens-livres-de-contexto/exemplos-de-gram%C3%A1ticas-livres-de-contexto/exemplos-de-revis%C3%A3o-de-exame-de-gram%C3%A1ticas-livres-de-contexto/qual-%C3%A9-a-rela%C3%A7%C3%A3o-entre-linguagens-decid%C3%ADveis-e-linguagens-livres-de-contexto/)  
38. Lema do Bombeamento Linguagens Livres de Contexto \- DECOM-UFOP |, acessado em agosto 19, 2025, [http://www.decom.ufop.br/anderson/2\_2011/BCC244/LemaDoBombeamentoFCPL.pdf](http://www.decom.ufop.br/anderson/2_2011/BCC244/LemaDoBombeamentoFCPL.pdf)  
39. As fases de um compilador, acessado em agosto 19, 2025, [http://wiki.icmc.usp.br/images/3/32/SCC\_605\_Estrutura\_de\_um\_Compilador.pdf](http://wiki.icmc.usp.br/images/3/32/SCC_605_Estrutura_de_um_Compilador.pdf)  
40. Compiladores/Projecto de Compiladores/Fases Desenvolvimento \- Wiki\*\*3, acessado em agosto 19, 2025, [https://web.tecnico.ulisboa.pt/\~david.matos/w/pt/index.php/Compiladores/Projecto\_de\_Compiladores/Fases\_Desenvolvimento](https://web.tecnico.ulisboa.pt/~david.matos/w/pt/index.php/Compiladores/Projecto_de_Compiladores/Fases_Desenvolvimento)  
41. compiladores \- O que é e como funciona a análise sintática ..., acessado em agosto 19, 2025, [https://pt.stackoverflow.com/questions/181635/o-que-%C3%A9-e-como-funciona-a-an%C3%A1lise-sint%C3%A1tica-ascendente-e-descendente](https://pt.stackoverflow.com/questions/181635/o-que-%C3%A9-e-como-funciona-a-an%C3%A1lise-sint%C3%A1tica-ascendente-e-descendente)  
42. Top-Down Parsing \- Wiki\*\*3, acessado em agosto 19, 2025, [https://web.tecnico.ulisboa.pt/\~david.matos/w/pt/index.php/Top-Down\_Parsing](https://web.tecnico.ulisboa.pt/~david.matos/w/pt/index.php/Top-Down_Parsing)  
43. Análise Sintática \- IC-Unicamp, acessado em agosto 19, 2025, [https://ic.unicamp.br/\~sandro/cursos/mc910/2009/slides/cap3-parser.pdf](https://ic.unicamp.br/~sandro/cursos/mc910/2009/slides/cap3-parser.pdf)  
44. Parsers LL(1) o mundo obscuro da análise sintática \- Frank de Alcantara, acessado em agosto 19, 2025, [https://frankalcantara.com/parsers-ll(1)/](https://frankalcantara.com/parsers-ll\(1\)/)  
45. Análise Sintática Descendente, acessado em agosto 19, 2025, [https://erinaldosn.files.wordpress.com/2011/06/aula-10-anc3a1lise-sintc3a1tica-descendente.pdf](https://erinaldosn.files.wordpress.com/2011/06/aula-10-anc3a1lise-sintc3a1tica-descendente.pdf)  
46. Compiladores 9 \- Algoritmo de Análise Sintática LL(1) ou Analisador Sintático LL(1) ou Parser LL(1) \- YouTube, acessado em agosto 19, 2025, [https://www.youtube.com/watch?v=yTgrbMkKk6M](https://www.youtube.com/watch?v=yTgrbMkKk6M)  
47. Como funciona a análise sintática ascendente (bottom-up parsing)? : r/golang \- Reddit, acessado em agosto 19, 2025, [https://www.reddit.com/r/golang/comments/kwju6j/how\_the\_bottom\_up\_parsing\_works/?tl=pt-br](https://www.reddit.com/r/golang/comments/kwju6j/how_the_bottom_up_parsing_works/?tl=pt-br)  
48. Bottom-Up Parsing \- Wiki\*\*3, acessado em agosto 19, 2025, [https://web.tecnico.ulisboa.pt/\~david.matos/w/pt/index.php/Bottom-Up\_Parsing](https://web.tecnico.ulisboa.pt/~david.matos/w/pt/index.php/Bottom-Up_Parsing)  
49. Análise Sintática (Cap. 04\) Análise Sintática Ascendente BottomUp \- Facom-UFMS, acessado em agosto 19, 2025, [http://www.facom.ufms.br/\~ricardo/Courses/CompilerI-2009/Lectures/CompilersI\_Lec06\_SyntaticAnalysis%283%29.pdf](http://www.facom.ufms.br/~ricardo/Courses/CompilerI-2009/Lectures/CompilersI_Lec06_SyntaticAnalysis%283%29.pdf)  
50. Compiladores (CC3001) Aula 6: Análise sintática bottom-up \- DCC/FCUP, acessado em agosto 19, 2025, [https://www.dcc.fc.up.pt/\~pbv/aulas/compiladores/teoricas/aula06.pdf](https://www.dcc.fc.up.pt/~pbv/aulas/compiladores/teoricas/aula06.pdf)  
51. Compila10 \- Análise sintática LR, Transição e fechamento \- YouTube, acessado em agosto 19, 2025, [https://www.youtube.com/watch?v=mGD7YpFSX-c](https://www.youtube.com/watch?v=mGD7YpFSX-c)  
52. Yacc \- Wikipedia, acessado em agosto 19, 2025, [https://en.wikipedia.org/wiki/Yacc](https://en.wikipedia.org/wiki/Yacc)  
53. GNU Bison \- Wikipedia, acessado em agosto 19, 2025, [https://en.wikipedia.org/wiki/GNU\_Bison](https://en.wikipedia.org/wiki/GNU_Bison)  
54. Bison \- GNU Project \- Free Software Foundation, acessado em agosto 19, 2025, [https://www.gnu.org/software/bison/](https://www.gnu.org/software/bison/)  
55. bison.pdf \- GNU, acessado em agosto 19, 2025, [https://www.gnu.org/s/bison/manual/bison.pdf](https://www.gnu.org/s/bison/manual/bison.pdf)

## chomsky referências

@article{chomsky1959formal,
  title={On certain formal properties of grammars},
  author={Chomsky, Noam},
  journal={Information and Control},
  volume={2},
  number={2},
  pages={137--167},
  year={1959},
  publisher={Elsevier}
}

@book{chomsky1957syntactic,
  title={Syntactic structures},
  author={Chomsky, Noam},
  year={1957},
  publisher={Mouton}
}

@article{mcculloch1943logical,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={The bulletin of mathematical biophysics},
  volume={5},
  number={4},
  pages={115--133},
  year={1943},
  publisher={Springer}
}

@article{kleene1956representation,
  title={Representation of events in nerve nets and finite automata},
  author={Kleene, Stephen Cole},
  journal={Automata studies},
  volume={34},
  pages={3--41},
  year={1956},
  publisher={Princeton University Press}
}

@article{post1936finite,
  title={Finite combinatory processes—formulation 1},
  author={Post, Emil L},
  journal={Journal of Symbolic Logic},
  volume={1},
  number={3},
  pages={103--105},
  year={1936},
  publisher={Cambridge University Press}
}

@incollection{chomsky1956three,
  title={Three models for the description of language},
  author={Chomsky, Noam},
  booktitle={IRE Transactions on information theory},
  volume={2},
  number={3},
  pages={113--124},
  year={1956},
  publisher={IEEE}
}

@article{harris1951methods,
  title={Methods in structural linguistics},
  author={Harris, Zellig S},
  year={1951},
  publisher={University of Chicago Press}
}

@article{chomsky1959review,
  title={A review of BF Skinner`s Verbal Behavior},
  author={Chomsky, Noam},
  journal={Language},
  volume={35},
  number={1},
  pages={26--58},
  year={1959},
  publisher={Linguistic Society of America}
}

@misc{wikipedia_chomsky_hierarchy,
  title={Chomsky hierarchy},
  author={{Wikipedia contributors}},
  year={2024},
  howpublished={\url{https://en.wikipedia.org/wiki/Chomsky_hierarchy}},
  note={Accessed: 2025-08-19}
}

@misc{britannica_chomsky,
  title={Noam Chomsky | Biography, Theories, Books, Psychology, \& Facts},
  author={{Encyclopædia Britannica}},
  year={2024},
  howpublished={\url{https://www.britannica.com/biography/Noam-Chomsky}},
  note={Accessed: 2025-08-19}
}

@article{pmc_formal_language_theory,
  title={Formal language theory: refining the Chomsky hierarchy},
  author={Jäger, Gerhard and Rogers, James},
  journal={Philosophical Transactions of the Royal Society B},
  volume={367},
  number={1598},
  pages={1956--1970},
  year={2012},
  publisher={The Royal Society}
}

@misc{devopedia_chomsky,
  title={Chomsky Hierarchy},
  author={{Devopedia}},
  year={2024},
  howpublished={\url{https://devopedia.org/chomsky-hierarchy}},
  note={Accessed: 2025-08-19}
}

@article{neural_networks_chomsky,
  title={Neural Networks and the Chomsky Hierarchy},
  author={Deletang, Grégoire and Ruoss, Anian and Mediano, Pedro AM and Jaggi, Martin and others},
  journal={arXiv preprint arXiv:2207.02098},
  year={2022}
}

@book{sipser2012introduction,
  title={Introduction to the Theory of Computation},
  author={Sipser, Michael},
  year={2012},
  publisher={Cengage Learning},
  edition={3rd}
}

@book{hopcroft2006introduction,
  title={Introduction to automata theory, languages, and computation},
  author={Hopcroft, John E and Motwani, Rajeev and Ullman, Jeffrey D},
  year={2006},
  publisher={Pearson/Addison Wesley},
  edition={3rd}
}

@misc{number_analytics_chomsky,
  title={Ultimate Guide to the Chomsky Hierarchy},
  author={{Number Analytics}},
  year={2024},
  howpublished={\url{https://www.numberanalytics.com/blog/ultimate-guide-chomsky-hierarchy}},
  note={Accessed: 2025-08-19}
}

@misc{cognitive_revolution,
  title={The Cognitive Revolution},
  author={{UCF Pressbooks}},
  year={2024},
  howpublished={\url{https://pressbooks.online.ucf.edu/lumenpsychology/chapter/reading-the-cognitive-revolution-and-multicultural-psychology/}},
  note={Accessed: 2025-08-19}
}

@article{backus1959syntax,
  title={The syntax and semantics of the proposed international algebraic language of the Zurich ACM-GAMM Conference},
  author={Backus, John W},
  journal={Proceedings of the International Conference on Information Processing},
  pages={125--132},
  year={1959},
  organization={UNESCO}
}

@book{naur1963revised,
  title={Revised report on the algorithmic language ALGOL 60},
  author={Naur, Peter and others},
  journal={Communications of the ACM},
  volume={6},
  number={1},
  pages={1--17},
  year={1963},
  publisher={ACM}
}