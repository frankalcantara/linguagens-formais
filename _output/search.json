[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Linguagens Formais e Autômatos",
    "section": "",
    "text": "1 Disciplina de Linguagens Formais\nEste é o material de suporte para a disciplina de Linguagens Formais, que abrange conceitos fundamentais de teoria da computação, autômatos, gramáticas e linguagens formais. O curso é dividido em várias partes, cada uma focando em aspectos específicos do tema.\nAlém deste texto o aluno deve consultar os livros e artigos recomendados no plano de ensino da disciplina listados a seguir:\nBibliografia Básica:\n\nMEIRA, Silvio Romero de Lemos. Introdução à programação funcional. Campinas: Ed. da UNICAMP, 1988. 290 p.;\nCASANOVA, M.A.; GIORNO, F.A.; FURTADO, A.L..: Programação em Lógica e a Linguagem Prolog. São Paulo: Edard Blücher,1987. 461 p.;\nSEBESTA, Robert W. Conceitos de linguagens de programação. 4. ed. Porto Alegre: Bookman, 2000.\n\nBibliografia Complementar:\n\nTHOMPSON, Simon. Haskell: the craft of functional programming. Harlow: Addison-Wesley, 1996. 500 p. Prentice-Hall;\nRUSSELL, Stuart J., NORVIG, Peter. Inteligência artificial. Rio de Janeiro, Elsevier, 2013;\nCOPPIN, Bem. Inteligência Artificial. Rio de Janeiro, LTC, 2010;\nSCHWICHTENBERG, Helmut;, Marktoberdorf, Alemanhan. Berlin: UFRJ, c1995. 470 p.;\nARAÚJO, Stenio Longo; ACIÓLY, Benedito Melo. Introdução ao Haskell. Vitória da Conquista: Edições UESB, 2008. 86 p..",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Disciplina de Linguagens Formais</span>"
    ]
  },
  {
    "objectID": "01-lexico.html",
    "href": "01-lexico.html",
    "title": "2  Analisadores Léxicos",
    "section": "",
    "text": "2.1 Máquinas de Estado Finito: História e Conceito\nOs estudos da Ciência da Computação são frequentemente organizados em uma hierarquia de capacidade computacional. Uma hierarquia em que a Máquina de Estados Finitos ocupa a posição de modelo mais fundamental e, em certos aspectos, o mais simples. Uma Máquina de Estado Finito é um modelo para sistemas que possuem uma quantidade finita e limitada de memória.\nO modelo que chamamos de Máquina de Estado Finito é composto por um conjunto finito de estados, um estado inicial, um conjunto de transições que definem como a máquina muda de estado com base em entradas discretas, e um conjunto de estados de aceitação que determinam se uma cadeia de símbolos de entrada é aceita ou rejeitada.\nA origem da máquina de estado finito está no produto de um esforço colaborativo que envolveu biólogos, psicólogos, matemáticos, engenheiros e alguns dos primeiros cientistas da computação, todos unidos por um interesse comum: modelar o processo do pensamento humano, seja no cérebro ou em uma máquina. Isso, curiosamente, liga as máquinas de estados finitos às ferramentas de Inteligência Artificial e Aprendizagem de Máquina. A história das Máquinas de Estados Finitos começa com a tentativa de entender o funcionamento do cérebro humano e como este processa informações.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analisadores Léxicos</span>"
    ]
  },
  {
    "objectID": "01-lexico.html#máquinas-de-estado-finito-história-e-conceito",
    "href": "01-lexico.html#máquinas-de-estado-finito-história-e-conceito",
    "title": "2  Analisadores Léxicos",
    "section": "",
    "text": "2.1.1 O Ponto de Partida: O Modelo de McCulloch e Pitts\nO trabalho pioneiro que deu origem à teoria dos autômatos foi o artigo de 1943 dos neurofisiologistas Warren McCulloch e Walter Pitts, intitulado A Logical Calculus Immanent in Nervous Activity(1)1.\nMcCulloch e Pitts propuseram um modelo matemático para o neurônio biológico, caracterizando-o como uma unidade de processamento binária. No modelo de McCulloch e Pitts, um neurônio dispararia, produzindo um sinal de saída, se, e somente se, um número fixo de sinapses de entrada fosse excitado dentro de um período de tempo discreto, excedendo assim um limiar predefinido (1)]. Este é um modelo discreto e binário, no qual cada neurônio pode estar em um conjunto finito de estados, por exemplo, disparando ou quieto, e cuja transição de estado é governada por entradas também discretas. O modelo de McCulloch e Pitts representa a primeira formalização conhecida de um Autômato Finito. O artigo demonstrou que uma rede destes neurônios formava um sistema capaz de computações lógicas complexas, como as operações da lógica proposicional, indicando que comportamentos complexos poderiam emergir de componentes simples e finitos.\n\n\n\n\n\n\nNote\n\n\n\nUm Autômato Finito é uma Máquina de Estados Finitos do tipo reconhecedor cuja função é reconhecer padrões em cadeias de símbolos, ou seja, determinar se um string de entrada pertence a uma determinada linguagem. Este modelo é fundamental para a teoria da computação e serve como base para muitos conceitos mais avançados, como autômatos de pilha e máquinas de Turing. Todo Autômato Finito é uma Máquina de Estados Finitos. Mas, nem toda Máquina de Estados Finitos é um Autômato Finito. A diferença está no propósito: enquanto os Autômatos Finitos são projetados para reconhecer linguagens, as Máquinas de Estados Finitos podem ser usadas para modelar qualquer sistema que possa ser descrito por estados discretos e as transições entre eles.\n\n\nA partir da inspiração biológica do modelo de McCulloch e Pitts, o conceito foi rapidamente abstraído para um modelo matemático puro, que ficou conhecido como Máquina de Estados Finitos. No domínio da linguagem computacional, uma Máquina de Estado Finito, em sua forma mais interessante para nosso propósito, funcionando como um artefato capaz de identificar uma linguagem, será formalmente definida como uma 5-tupla:\n\\[M=(Q, \\Sigma, \\delta, q_0, F)\\]\nna qual:\n\n\\(Q\\) é o conjunto de estados;\n\\(\\Sigma\\) é o alfabeto;\n\\(\\delta\\) é a função de transição;\n\\(q_0\\) é o estado inicial;\n\\(F\\) é o conjunto de estados de aceitação.\n\nO conceito de estado é o coração da Máquina de Estado Finito.\nUm estado é uma abstração que resume todo o histórico de entradas que a máquina processou até um determinado momento, contendo apenas a informação necessária para decidir sobre as futuras transições e saídas. Uma Máquina de Estados Finitos transita de um estado para outro com base na entrada atual. Este comportamento é frequentemente visualizado por meio de um diagrama de transição de estados. Este diagrama é um grafo em que os nós representam os estados e as arestas direcionadas e rotuladas representam as transições. Como pode ser visto na Figure 2.1.\nUma cadeia de entrada será aceita pela Máquina de Estado Finito se, começando no estado inicial \\(q_0\\), a sequência de transições correspondente à cadeia de entrada termina em um dos estados de aceitação que estejam no conjunto \\(F\\), no caso da Figure 2.1, o estado de aceitação é \\(q_2\\).\n\n\n\n\n\n\nFigure 2.1: Diagrama de transição de uma máquina de estados finitos.\n\n\n\nA origem da Máquina de Estado Finito ilustra um dos ciclos virtuosos de inovação da Ciência da Computação. O processo começou com a observação de um sistema complexo do mundo real, o cérebro, e continua até os dias de hoje. McCulloch e Pitts não estavam tentando inventar um modelo de computação, tentavam entender a lógica da atividade nervosa. O passo de gênio subsequente, liderado por figuras como Stephen Kleene, foi reconhecer que o princípio computacional subjacente, um sistema com memória finita que muda de estado com base em entradas discretas, pode ser divorciado de sua inspiração biológica.\nEste divórcio foi o passo que permitiu a generalização do conceito e sua aplicação a domínios completamente diferentes. Domínios que abrangem desde o design de circuitos digitais até a análise de textos em um compilador.\n\n\n2.1.2 A Era da Formalização — Kleene, Moore e Mealy\nA década de 1950 marcou um período de intensa formalização e expansão da teoria dos Autômatos Finitos. Três figuras destacam-se nesta época: Stephen Kleene, que estabeleceu a ligação fundamental entre autômatos e uma nova notação de padrões, e Edward F. Moore e George H. Mealy, que estenderam o modelo para além do simples reconhecimento, dotando-o da capacidade de produzir saídas.\nSeguindo as ideias pioneiras de McCulloch e Pitts, o matemático americano Stephen Cole Kleene publicou em 1956 seu artigo Representation of Events in Nerve Nets and Finite Automata(2)2. Seu trabalho estava inserido em um contexto mais amplo de investigação sobre os limites da computação, que funções podem ser computadas e que problemas são decidíveis.\nA contribuição mais notável de Kleene neste artigo foi a criação das expressões regulares (REGEX). Kleene introduziu esta notação algébrica como uma forma concisa e poderosa para descrever conjuntos de sequências de entrada, ou eventos, que hoje conhecemos como Linguagens Regulares. Na sua obra, Kleene definiu formalmente as três operações fundamentais que formam a base de todas as expressões regulares:\n\nUnião (Alternância): representada por \\(+\\), \\(|\\) ou \\(\\cup\\), dependendo da área da matemática onde a união é usada, denota uma escolha entre dois padrões.\nConcatenação: representada pela justaposição de dois padrões, denota a sequência de um padrão seguido por outro. A concatenação é representada simplesmente pela junção dos símbolos, como em \\(ab\\), que denota a sequência do símbolo \\(a\\) seguido pelo símbolo \\(b\\). Ou pelo uso da notação do produto escalar \\(\\cdot\\), como em \\(a \\cdot b\\).\n\nFechamento de Kleene (Kleene Star): representado pelo asterisco \\(*\\), denota zero ou mais ocorrências do padrão precedente. Considerando o padrão \\(p\\), o fechamento de Kleene é denotado por \\(p^*\\) e representa a linguagem que contém todas as cadeias que podem ser formadas concatenando zero ou mais cópias de \\(p\\). Por exemplo, o fechamento de Kleene do símbolo \\(a\\) é a linguagem \\(\\{\\epsilon, a, aa, aaa, \\ldots\\}\\), no qual \\(\\epsilon\\) representa a cadeia vazia.\n\nEstas operações, aplicadas a símbolos de um alfabeto, formam a Álgebra de Kleene, um sistema formal que se tornou onipresente na ciência da computação, com aplicações que vão desde a verificação de programas até a análise de algoritmos.\n\n2.1.2.1 O Teorema de Kleene: A Grande Unificação\nA genialidade do trabalho de Kleene não reside apenas na invenção das expressões regulares, mas na prova de sua profunda ligação com os Autômatos Finitos. O Teorema de Kleene estabelece uma equivalência fundamental: a classe de linguagens que podem ser descritas por expressões regulares é precisamente a mesma classe de linguagens que podem ser reconhecidas por Autômatos Finitos. A prova deste teorema é construtiva e, por isso, de enorme importância prática.\n\n\n\n\n\n\nNote\n\n\n\nA curiosa leitora deve notar que uma prova construtiva é um tipo de prova matemática que não apenas demonstra a existência de um objeto matemático, mas também fornece um método explícito para construí-lo ou encontrá-lo. Neste caso, em vez de provar que algo deve existir por meio de uma contradição, a prova construtiva mostra o objeto.\nExemplo: Provar que entre quaisquer dois números racionais distintos, \\(x\\) e \\(y\\), existe um outro número racional, \\(z\\).\nProva por Construção:\nAssumindo \\(x, y \\in \\mathbb{Q}\\) e \\(x &lt; y\\).\n\nConstrução: Vamos construir \\(z\\) pegando a média de \\(x\\) e \\(y\\): \\[z = \\frac{x+y}{2}\\]\nVerificação:\n\nComo a soma e a divisão de números racionais resulta em um número racional, \\(z\\) é garantidamente um número racional (\\(z \\in \\mathbb{Q}\\)).\nComo \\(x &lt; y\\), pode-se provar que \\(x &lt; z &lt; y\\).\n\n\nA prova funciona porque nós construímos um valor específico para \\(z\\) e demonstramos que ele satisfaz as condições exigidas.\n\n\nNo caso do Teorema de Kleene, a prova construtiva é dividida em duas partes fundamentais, cada uma correspondendo a uma direção da equivalência.\n\nDe Expressão Regular para Autômato Finito: esta parte da prova demonstra que, para qualquer expressão regular, é possível construir um Autômato Finito, especificamente, um Autômato Finito Não-Determinístico (AFN) com transições épsilon, ou AFN*-\\(\\epsilon\\), que aceita a mesma linguagem. A construção deste Autômato é indutiva sobre a estrutura da expressão regular. Começa-se com autômatos simples para os casos base, a linguagem vazia \\(\\emptyset\\), a linguagem contendo a cadeia vazia \\(\\{\\epsilon\\}\\), e a linguagem contendo um único símbolo \\(\\{a\\}\\)) e depois mostram-se métodos para combinar autômatos existentes para corresponder às operações de união, concatenação e fechamento de Kleene.\nDe Autômato Finito para Expressão Regular: a segunda parte demonstra que, para qualquer Autômato Finito, é possível derivar uma expressão regular que descreve a linguagem que ele aceita. Este processo é mais complexo e envolve a eliminação progressiva de estados do autômato, enquanto as etiquetas das transições são substituídas por expressões regulares cada vez mais complexas que representam os caminhos que foram eliminados.\n\nO Teorema de Kleene é a pedra angular teórica que sustenta os programas geradores de Analisadores Léxicos modernos como o Lex (3) e o Flex (4). O Teorema de Kleene garante que os programadores podem usar a notação declarativa e legível das expressões regulares para especificar os padrões dos tokens, com a confiança de que estas especificações podem ser automaticamente compiladas por um sistema eficiente de reconhecimento, o Autômato Finito.\nSim, a leitora entendeu corretamente. Existem programas capazes de gerar Analisadores Léxicos a partir de especificações da linguagem. O Lex e o Flex são exemplos clássicos desses programas. Porém, o que queremos neste livro é entender linguagens formais e compiladores, e não apenas usar ferramentas prontas.\n\n\n2.1.2.2 A Introdução de Saídas: Máquinas Transdutoras\nOs autômatos de Kleene, tal como os de McCulloch e Pitts, eram artefatos para o reconhecimento, ou aceitação de padrões. Sua única função era emitir um veredito binário: o string de entrada pertence ou não à linguagem. Todavia, muitas aplicações, desde circuitos de controle a sistemas de Inteligência Artificial, necessitam gerar uma sequência de saídas em resposta às entradas.\nAs Máquinas de Estados Finitos que produzem saídas são chamadas de transdutores de estados finitos. Um Transdutor de Estados Finitos estende a definição da Máquina de Estado Finito para uma 6-tupla,\n\\[(Q, \\Sigma, \\Gamma, \\delta, \\lambda, q_0)\\]\nna qual \\(\\Gamma\\) é um alfabeto finito de símbolos de saída e \\(\\lambda\\) é uma função de saída.\nEm meados da década de 1950, G.H. Mealy e E.F. Moore, trabalhando de forma independente, propuseram dois modelos distintos para os Transdutores de Estados Finitos, generalizando a teoria para englobar máquinas muito mais poderosas.\nNo seu artigo “Gedanken-experiments on Sequential Machines”(5)3, Edward F. Moore introduziu um modelo de transdutor no qual a saída é determinada exclusivamente pelo estado atual da máquina. A função de saída é, portanto, definida como:\n\\[\\lambda: Q \\to \\Gamma\\]\nEsta definição implica que a saída é estável enquanto a máquina permanece em um determinado estado. Isso quer dizer que uma mudança na saída só ocorre quando há uma transição para um novo estado. Em implementações de hardware, isto significa que as saídas são síncronas com as transições de estado, que por sua vez são frequentemente sincronizadas por um sinal de relógio, um clock. Nos diagramas de estado, a saída de uma máquina de Moore é tipicamente associada ao próprio estado, sendo escrita dentro do círculo que o representa. Uma máquina de Moore tende a necessitar de mais estados do que uma máquina de Mealy para realizar a mesma tarefa. Neste caso, um estado pode ser necessário apenas para gerar uma saída específica.\nUm ano antes, George H. Mealy, em seu artigo “A Method for Synthesizing Sequential Circuits”(6)4 publicado no Bell System Technical Journal, propôs um modelo alternativo. Em uma máquina de Mealy, a saída depende tanto do estado atual como da entrada atual. A função de saída é definida como\n\\[\\lambda: Q \\times \\Sigma \\to \\Gamma\\]\nComo a saída pode mudar instantaneamente com uma mudança na entrada, mesmo sem uma transição de estado, as saídas de uma máquina de Mealy são consideradas assíncronas. Isto pode permitir uma resposta mais rápida do sistema, mas também introduz a possibilidade de problemas de temporização em circuitos sequenciais. Nos diagramas de estado, a saída é associada à transição, sendo escrita no arco da transição, tipicamente separada da entrada por uma barra (ex.: a/b indica que a máquina transita de um estado a outro com a entrada a e produz a saída b). Esta característica permite que as máquinas de Mealy sejam mais compactas, frequentemente necessitando de menos estados do que a máquina de Moore equivalente.\nA tabela Table 2.1 resume as diferenças entre os dois modelos.\n\n\n\nTable 2.1: Comparação Detalhada entre Máquinas de Moore e Mealy\n\n\n\n\n\n\n\n\n\n\nCaracterística\nMáquina de Moore\nMáquina de Mealy\n\n\n\n\nFunção de Saída (\\(\\lambda\\))\n\\(\\lambda: Q \\to \\Gamma\\)\n\\(\\lambda: Q \\times \\Sigma \\to \\Gamma\\)\n\n\nDependência da Saída\nApenas do estado atual\nDo estado atual e da entrada atual\n\n\nTiming da Saída\nSíncrona com o estado\nAssíncrona com a entrada\n\n\nRepresentação em Diagrama\nAssociada ao nó do estado\nAssociada ao arco da transição\n\n\nNúmero de Estados\nGeralmente necessita de mais estados\nGeralmente necessita de menos estados\n\n\nAplicação Típica\nAtivação de um conjunto de ações estáveis num estado\nDesencadear eventos ou sinais em resposta a transições\n\n\n\n\n\n\nA esforçada leitora deve ter em mente que o Teorema de Kleene não é apenas um resultado matemático elegante; ele estabelece uma dicotomia que serve como alicerce para grande parte da engenharia de software moderna. Essa dicotomia se manifesta de forma clara:\n\nAs expressões regulares são uma linguagem declarativa: descrevem o que é o padrão;\nOs Autômatos Finitos são um modelo operacional: descrevem como reconhecer o padrão.\n\nA genialidade do Teorema de Kleene, amável leitora, reside na garantia de que podemos traduzir informações, sem perdas, do mundo declarativo, que é mais fácil para os humanos, para o mundo operacional, que é eficiente para as máquinas. Ou vice-versa.\nEste padrão, traduzir uma especificação de alto nível em uma implementação de baixo nível, é a descrição sintética do que um compilador faz. Além disso, o trabalho de Kleene pode ser visto como o arquétipo da poderosa ideia de separação de preocupações, um princípio de design tecnológico que permite criar abstrações elegantes sem sacrificar a performance.\n\n\n\n2.1.3 O Poder do Não-Determinismo\nEnquanto Kleene, Moore e Mealy solidificavam e expandiam a teoria dos autômatos determinísticos, uma nova ideia despontava no horizonte. Esta ideia, o não-determinismo, parecia à primeira vista conceder um poder quase mágico às máquinas, mas acabaria por se revelar uma das ferramentas conceituais mais úteis para simplificar o design e a teoria dos autômatos.\nEm 1959, Michael O. Rabin e Dana Scott publicaram seu artigo clássico e profundamente influente, Finite Automata and Their Decision Problems(7). Por este trabalho, que introduziu formalmente o conceito de Autômato Finito Não-Determinístico, foram agraciados com o Prêmio Turing, a mais alta honra da ciência da computação.\nUm Autômato Finito Não-Determinístico relaxa as restrições de um Autômato Finito Determinístico de três formas cruciais:\n\nMúltiplas Transições: para um dado estado e um símbolo de entrada, um Autômato Finito Não-Determinístico pode ter zero, uma ou múltiplas transições possíveis. A função de transição mapeia para um conjunto de estados, não para um único estado. Formalmente, \\(\\delta: Q \\times \\Sigma \\to \\mathcal{P}(Q)\\), em que \\(\\mathcal{P}(Q)\\) é o conjunto das partes de \\(Q\\).\nTransições-Épsilon (\\(\\epsilon\\)-transitions): um Autômato Finito Não-Determinístico pode mudar de estado sem consumir qualquer símbolo da entrada. Estas transições, rotuladas com \\(\\epsilon\\), permitem que a máquina salte espontaneamente entre estados. A função de transição é então \\(\\delta: Q \\times (\\Sigma \\cup \\{\\epsilon\\}) \\to \\mathcal{P}(Q)\\).\nTransições Ausentes: para um determinado par, estado e símbolo de entrada, o conjunto de próximos estados pode ser o conjunto vazio, significando que a computação nesse ramo termina.\n\nO modelo de computação de um Autômato Finito Não-Determinístico permite a existência de múltiplas transições possíveis a partir de um mesmo estado para um mesmo símbolo de entrada. Essa característica resulta em um modelo de computação inerentemente paralelo, no qual o Autômato Finito Não-Determinístico explora todos os caminhos de transição possíveis simultaneamente ao processar uma cadeia de entrada. A cadeia é considerada aceita se pelo menos um destes caminhos terminar em um estado de aceitação após a leitura de toda a cadeia.\n\n2.1.3.1 A Construção do Conjunto das Partes (Powerset Construction)\nA conclusão mais surpreendente e poderosa do artigo de Rabin e Scott é que, apesar de sua aparente flexibilidade e poder acrescido, os Autômatos Finitos Não-Determinísticos não são mais expressivos do que os Autômatos Finitos Determinísticos. Isto é uma forma elegante de dizer que para qualquer Autômato Finito Não-Determinístico, existe um Autômato Finito Determinístico equivalente que reconhece exatamente a mesma linguagem.\nA prova da equivalência entre Autômatos Finitos Não-Determinísticos e Determinísticos, mais uma vez, construtiva, por meio de um algoritmo conhecido como a construção do conjunto das partes (powerset construction). A ideia central desse algoritmo é simular o comportamento paralelo do Autômato Finito Não-Determinístico de forma determinística. Para tanto, cada estado no Autômato Finito Determinístico construído corresponde a um conjunto de estados nos quais o Autômato Finito Não-Determinístico poderia estar em um mesmo momento. O algoritmo da construção do conjunto das partes funciona da seguinte forma:\n\nEstado Inicial do Autômato Finito Determinístico: o estado inicial do Autômato Finito Determinístico (AFD) é o conjunto de todos os estados do Autômato Finito Não-Determinístico que são alcançáveis a partir do estado inicial do Autômato Finito Não-Determinístico usando apenas transições-\\(\\epsilon\\). Este conjunto é conhecido como o fechamento-\\(\\epsilon\\) (epsilon-closure) do estado inicial do Autômato Finito Não-Determinístico.\nTransições do Autômato Finito Determinístico: para cada estado do Autômato Finito Determinístico, que corresponde a um conjunto de estados do Autômato Finito Não-Determinístico, e para cada símbolo do alfabeto, a transição do Autômato Finito Determinístico é calculada em duas etapas: primeiro, determina-se o conjunto de todos os estados do Autômato Finito Não-Determinístico que podem ser alcançados a partir do conjunto atual ao ler esse símbolo; segundo, calcula-se o fechamento-\\(\\epsilon\\) desse novo conjunto. O resultado é o novo estado do Autômato Finito Determinístico.\nEstados de Aceitação do Autômato Finito Determinístico: um estado no Autômato Finito Determinístico é um estado de aceitação se seu conjunto correspondente de estados do Autômato Finito Não-Determinístico contiver pelo menos um dos estados de aceitação originais do Autômato Finito Não-Determinístico.\n\nEste processo é repetido até que não sejam gerados novos estados no Autômato Finito Determinístico. Como o número de subconjuntos de um conjunto finito de estados é finito, embora potencialmente grande, o processo garante a terminação.\nA existência da construção do conjunto das partes estabelece uma relação de compromisso (trade-off) entre Autômatos Finitos Não-Determinísticos e Determinísticos, que é de extrema importância na prática da engenharia de compiladores.\n\nConstrução e Tamanho: os Autômatos Finitos Não-Determinísticos são geralmente muito mais fáceis e intuitivos de construir diretamente a partir de uma especificação de uma linguagem, como uma expressão regular. Um Autômato Finito Não-Determinístico para uma dada linguagem é tipicamente muito menor, em termos de número de estados e transições, do que o Autômato Finito Determinístico equivalente. A conversão de um Autômato Finito Não-Determinístico com \\(n\\) estados para um Autômato Finito Determinístico pode, no pior dos casos, resultar em um Autômato Finito Determinístico com até \\(2^n\\) estados, um fenômeno conhecido como explosão de estados.\nSimulação e Eficiência: por outro lado, os Autômatos Finitos Determinísticos são muito mais fáceis de simular. Para uma cadeia de entrada de comprimento \\(k\\), um Autômato Finito Determinístico executa exatamente \\(k\\) transições, resultando em uma execução em tempo linear. A simulação direta de um Autômato Finito Não-Determinístico é mais complexa. Esta simulação pode exigir o rastreamento de múltiplos caminhos de computação em paralelo, tornando-a mais lenta.\n\nA tabela Table 2.2 resume esses compromissos práticos.\n\n\n\nTable 2.2: Autômato Finito Não-Determinístico vs. Autômato Finito Determinístico.\n\n\n\n\n\n\n\n\n\n\nCritério\nAutômato Finito Não-Determinístico\nAutômato Finito Determinístico\n\n\n\n\nFunção de Transição\n\\(\\delta: Q \\times (\\Sigma \\cup \\{\\epsilon\\}) \\to \\mathcal{P}(Q)\\)\n\\(\\delta: Q \\times \\Sigma \\to Q\\)\n\n\nTransições-\\(\\epsilon\\)\nPermitidas\nNão permitidas\n\n\nNúmero de Estados\nGeralmente pequeno (\\(n\\))\nPotencialmente grande (até \\(2^n\\))\n\n\nFacilidade de Construção\nAlta (fácil de construir a partir de RE)\nBaixa (difícil de construir diretamente)\n\n\nVelocidade de Execução\nLenta (simulação direta)\nRápida (execução em tempo linear)\n\n\nUtilização na Prática\nPasso intermédio na compilação de RE\nProduto final para analisadores léxicos eficientes\n\n\n\n\n\n\nO não-determinismo, tal como introduzido por Rabin e Scott, não é um recurso computacional físico. Nenhuma máquina real adivinha o caminho correto, ou verifica todos os caminhos no mesmo instante. Em vez disso, o não-determinismo é uma poderosa ferramenta de abstração conceitual. O não-determinismo permite que os projetistas humanos pensem em termos de possibilidades: _existe algum caminho que leve à aceitação? Em vez de se prenderem aos detalhes físicos de uma única computação determinística. Além disso, a prova de que Autômatos Finitos Não-Determinísticos são equivalentes a Autômatos Finitos Determinísticos é mais do que um resultado teórico; é uma licença para usar a abstração mais conveniente, geralmente o Autômato Finito Não-Determinístico, para o design e a especificação, com a garantia de que ela pode ser convertida em uma forma eficiente e fisicamente executável, geralmente o Autômato Finito Determinístico.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analisadores Léxicos</span>"
    ]
  },
  {
    "objectID": "01-lexico.html#sec-analisador-lexico",
    "href": "01-lexico.html#sec-analisador-lexico",
    "title": "2  Analisadores Léxicos",
    "section": "2.2 Autômatos Finitos: O Analisador Léxico",
    "text": "2.2 Autômatos Finitos: O Analisador Léxico\nA teoria dos Autômatos Finitos, com suas provas de equivalência e algoritmos de conversão, poderia ter permanecido um tópico de interesse puramente acadêmico. No entanto, encontrou uma aplicação prática tão perfeita na construção de compiladores que se tornou um exemplo de como a teoria fundamental pode transformar a engenharia de software. Esta aplicação é a análise léxica.\nComo eu disse antes, a primeira fase de um compilador é o analisador léxico, também conhecido como scanner ou lexer. Sua tarefa é ler um fluxo de caracteres criados a partir do código fonte, e identificar conjuntos significativos de símbolos para uma dada linguagem, os lexemas. Para cada lexema, o analisador léxico produzirá um token, que é tipicamente uma estrutura de dados consistindo em uma classe de token (ex.: IDENTIFICADOR, NÚMERO, PALAVRA-CHAVE), o valor do lexema, a cadeia de símbolos real, e a posição do lexema no código fonte. Por exemplo, a linha de código if (i == j) seria transformada em uma sequência de tokens como (IF, “if”, 0, 0), (LPAREN, “(”, 0, 3), (ID, “i”, 0, 4), (EQ, “==”, 0, 6), (ID, “j”, 0, 9), (RPAREN, “)”, 0, 10). O fragmento de código Listing 2.1 ilustra uma estrutura de armazenamento para tokens.\n\n\n\nListing 2.1\n\n\n#include &lt;string&gt;\n#include &lt;string_view&gt;\n#include &lt;array&gt;\n\n// Enumeração para os tipos de token\nenum class TokenType {\n    // Palavras-chave\n    IF,\n    // outras palavras-chave como ELSE, WHILE, FOR, etc.\n    \n    // Identificadores e literais\n    IDENTIFICADOR,\n    NUMERO,\n    STRING_LITERAL,\n    \n    // Operadores\n    EQ,          // ==\n    // outros operadores como NEQ, LT, GT, LE, GE, ADD, SUB, MUL, DIV\n\n    // Delimitadores\n    LPAREN,      // (\n    RPAREN,      // )\n    // outros delimitadores como LBRACE, RBRACE, SEMICOLON, COMMA    \n    // Especiais\n    END_OF_FILE,\n    INVALID\n};\n\n// Estrutura para representar um token\nstruct Token {\n    TokenType tipo;           // Classe do token\n    std::string lexema;       // Valor do lexema (cadeia de caracteres real)\n    int linha;               // Número da linha\n    int posicao;             // Posição na linha\n    \n    // Construtor\n    Token(TokenType t, std::string_view lex, int lin, int pos)\n        : tipo(t), lexema(lex), linha(lin), posicao(pos) {}\n    \n    // Construtor padrão\n    Token() : tipo(TokenType::INVALID), linha(0), posicao(0) {}\n};\n\n// Exemplo de array de _tokens_ para o fragmento \"if (i == j)\"\nstd::array&lt;Token, 6&gt; exemplo__tokens_ = {{\n    Token(TokenType::IF, \"if\", 1, 0),\n    Token(TokenType::LPAREN, \"(\", 1, 3),\n    Token(TokenType::IDENTIFICADOR, \"i\", 1, 4),\n    Token(TokenType::EQ, \"==\", 1, 6),\n    Token(TokenType::IDENTIFICADOR, \"j\", 1, 9),\n    Token(TokenType::RPAREN, \")\", 1, 10)\n}};\n\n\n\nA questão fundamental que deve estar incomodando a curiosa leitora é: como reconhecer estes padrões?\nOs padrões que definem os tokens na maioria das linguagens de programação, como identificadores, números inteiros, e palavras-chave, são quase invariavelmente descritos por Linguagens Regulares. Como estabelecido pelo Teorema de Kleene, as Linguagens Regulares são precisamente aquelas que podem ser reconhecidas por Autômatos Finitos. Portanto, as Máquinas de Estados Finitos são o modelo computacional adequado e suficiente para a tarefa da análise léxica. Parece simples, contudo, na prática, a análise léxica enfrenta ambiguidades que devem ser resolvidas por regras claras:\n\nRegra da Correspondência Mais Longa (Maximal Munch): se uma porção do texto de entrada pode corresponder a múltiplos padrões de comprimentos diferentes, o analisador léxico escolhe sempre a correspondência mais longa possível. Por exemplo, no string &gt;=, o lexer não irá parar no token &gt;; ele continuará a ler para reconhecer o identificador &gt;=. Isto requer que o algoritmo tenha capacidade de lookahead, do inglês para olhar para a frente, lendo caracteres para além do final de um potencial lexema para garantir que não há uma correspondência mais longa.\nPrioridade das Regras: se dois padrões correspondem a um lexema do mesmo comprimento, por exemplo if poderia ser uma palavra-chave ou um identificador, a ambiguidade é resolvida dando prioridade à regra que aparece primeiro no arquivo de especificação das regras do analisador léxico. Por esta razão, as regras para palavras-chave são sempre listadas antes, e têm precedência, sobre a regra geral para identificadores.\nTratamento de Erros: se uma sequência de caracteres não corresponder a nenhum padrão conhecido, o analisador léxico deve sinalizar um erro. Normalmente, isso é feito emitindo um token especial, como INVALID, e registrando a posição do erro.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analisadores Léxicos</span>"
    ]
  },
  {
    "objectID": "01-lexico.html#extensões-modernas-e-conclusão",
    "href": "01-lexico.html#extensões-modernas-e-conclusão",
    "title": "2  Analisadores Léxicos",
    "section": "2.3 Extensões Modernas e Conclusão",
    "text": "2.3 Extensões Modernas e Conclusão\nEmbora o modelo clássico da máquina de estado finito seja perfeitamente adequado para a análise léxica, sua simplicidade pode tornar-se uma limitação ao modelar sistemas mais complexos. A história das Máquinas de Estados Finitos não termina com sua aplicação em compiladores; ela continua com extensões que procuram gerir a complexidade em domínios como sistemas reativos e de controle.\nUma das principais dificuldades ao aplicar Máquinas de Estados Finitos a sistemas complexos é o problema da explosão de estados. Se um sistema é composto por múltiplas variáveis ou componentes, o número total de estados na Máquina de Estado Finito que o descreve pode crescer exponencialmente, tornando o modelo intratável e incompreensível. Por exemplo, um sistema com \\(n\\) variáveis, cada uma podendo assumir \\(Z\\) valores, pode ter até \\(Z^n\\) estados possíveis. Uma Máquina de Estado Finito plana, na qual todos os estados existem ao mesmo nível, não escala bem para esses cenários.\nO termo máquina de estados finitos plana não é uma definição padrão na teoria formal da computação, mas pode ter algumas interpretações dependendo do contexto. Exemplo de representação plana:\nEstados: \\(Q = \\{q_0, q_1, q_2, q_3\\}\\) Transições: \\(q_0 \\xrightarrow{a} q_1, q_1 \\xrightarrow{b} q_2, q_2 \\xrightarrow{c} q_3\\)\nPara resolver esta limitação, David Harel introduziu em 1987 os StateCharts, uma poderosa extensão visual e formal das Máquinas de Estados Finitos. Os StateCharts enriquecem o formalismo clássico com duas noções fundamentais:\n\nHierarquia (Aninhamento de Estados ou Decomposição-OU): os estados podem conter subestados. Isto permite refinar o comportamento de um superestado em vários subestados mais detalhados, organizando a complexidade de forma hierárquica.\nConcorrência (Estados Ortogonais ou Decomposição-E): os StateCharts permitem que o sistema esteja em múltiplos estados de subsistemas diferentes no mesmo instante. Isto é ideal para modelar sistemas compostos por componentes paralelos e independentes, reduzindo drasticamente o número de estados explícitos necessários em comparação com uma máquina de estados finitos plana.\n\nExemplo hierárquico:\n\\[\\text{Estado\\_Principal} \\begin{cases}\n\\text{Subestado\\_A} \\{q_0, q_1\\} \\\\\n\\text{Subestado\\_B} \\{q_2, q_3\\}\n\\end{cases}\\]\nPara facilitar a compreensão da abstração dos StateCharts, considere um sistema de um player como mostrado na Figure 2.2.\n\n\n\n\n\n\nFigure 2.2\n\n\n\nEmbora as Máquinas de Estados Finitos clássicas sejam suficientes para a análise léxica, os StateCharts demonstram como o modelo fundamental pode ser estendido para lidar com a complexidade inerente a sistemas de controle de software, protocolos de comunicação e interfaces de usuário, mostrando a versatilidade e a relevância contínua do paradigma de estados.\n\n2.3.1 Um Olhar no Horizonte: Não-Determinismo e a Fronteira Quântica\nA amável leitora, ao se deparar com o não-determinismo de um Autômato Finito Não-Determinístico, pode se perguntar sobre outros modelos de computação que exploram múltiplos caminhos. Onde a computação quântica se encaixa nisso?\nA conexão teórica existe nos Autômatos Finitos Quânticos(8). Contudo, é fundamental distinguir os conceitos:\n\nNão-Determinismo Clássico: a máquina explora múltiplos caminhos computacionais em paralelo. Imagine seguir todas as saídas de um labirinto ao mesmo tempo. A cadeia de entrada é aceita se pelo menos um caminho leva a um estado de aceitação.\nParalelismo Quântico: a máquina evolui em um único estado de superposição quântica. Os estados não são caminhos separados, mas sim componentes de um vetor de estado em um espaço de Hilbert complexo. O resultado depende das probabilidades das amplitudes quânticas no momento da medição.\n\nEssa distinção leva a uma consequência surpreendente: os modelos mais básicos de Autômatos Finitos Quânticos são, na verdade, menos poderosos que os Autômatos Finitos Não-Determinísticos clássicos. Os modelos quânticos não conseguem reconhecer todas as Linguagens Regulares. A razão está nas restrições da mecânica quântica, como a reversibilidade das operações.\nOnde está a vantagem, então? Na eficiência de estados. Para certas linguagens específicas, um Autômato Finito Quântico pode ser exponencialmente mais compacto que qualquer autômato clássico. O exemplo canônico é a linguagem \\(L_p = \\{a^k \\mid k \\text{ é múltiplo de } p\\}\\), para um \\(p\\) primo. Um Autômato Finito Determinístico (AFD) precisa de \\(p\\) estados, enquanto um AFQ pode reconhecer a mesma linguagem com apenas \\(O(\\log p)\\) estados.\nEmbora a aplicação prática de Autômatos Finitos Quânticos em analisadores léxicos seja um campo puramente especulativo, eles demonstram como os modelos fundamentais que estudamos neste capítulo continuam a inspirar as fronteiras mais avançadas da teoria da computação.\n\n\n\n\n[1] MCCULLOCH, W. S.; PITTS, W. A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, v. 5, n. 4, p. 115–133, 1943. \n\n\n[2] KLEENE, S. C. Representation of events in nerve nets and finite automata. In: SHANNON, C. E.; MCCARTHY, J. (Eds.). Automata studies. Annals of mathematics studies. [s.l.] Princeton University Press, 1956. v. 34p. 3–41. \n\n\n[3] LESK, M. E. Lex – A Lexical Analyzer Generator. Murray Hill, NJ: Bell Laboratories, 1975. \n\n\n[4] PAXSON, V. Flex, a fast lexical analyzer generator. [s.l: s.n.]. \n\n\n[5] MOORE, E. F. Gedanken-experiments on sequential machines. In: SHANNON, C. E.; MCCARTHY, J. (Eds.). Automata studies. Princeton: Princeton University Press, 1956. p. 129–154. \n\n\n[6] MEALY, G. H. A method for synthesizing sequential circuits. The Bell System Technical Journal, v. 34, n. 5, p. 1045–1064, Sep. 1955. \n\n\n[7] RABIN, M. O.; SCOTT, D. Finite Automata and Their Decision Problems. IBM Journal of Research and Development, v. 3, n. 2, p. 114–125, 1959. \n\n\n[8] TIAN, Y. et al. Experimental demonstration of quantum finite automaton. npj Quantum Information, v. 5, 2019.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analisadores Léxicos</span>"
    ]
  },
  {
    "objectID": "01-lexico.html#footnotes",
    "href": "01-lexico.html#footnotes",
    "title": "2  Analisadores Léxicos",
    "section": "",
    "text": "em tradução livre, “Um Cálculo Lógico Immanente na Atividade Nervosa”.↩︎\nem tradução livre, “Representação de Eventos em Redes Nervosas e Autômatos Finitos”.↩︎\nem tradução livre, “Experimentos Mentais sobre Máquinas Sequenciais”.↩︎\nem tradução livre, “Um Método para Sintetizar Circuitos Sequenciais”.↩︎",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Analisadores Léxicos</span>"
    ]
  },
  {
    "objectID": "01a-lexico.html",
    "href": "01a-lexico.html",
    "title": "3  Alfabetos, Linguagens e Strings: Fundamentos Matemáticos",
    "section": "",
    "text": "3.1 Alfabetos: Os Blocos Fundamentais\nUm alfabeto constitui o artefato fundamental da teoria de linguagens formais. Matematicamente, um alfabeto é um conjunto finito não-vazio de símbolos, denotado convencionalmente pela letra grega \\(\\Sigma\\), o sigma maiúsculo. Neste contexto, um alfabeto será definido como:\n\\[\\Sigma = \\{a_1, a_2, \\ldots, a_n\\} \\text{ tal que } |\\Sigma| = n &lt; \\infty \\text{ e } n \\geq 1\\]\nComo esse é o nosso primeiro contato a cuidadosa leitora deve observar que a definição elegante e formal de alfabeto pode ser decomposta em partes mais simples:\nA natureza dos símbolos que compõem um alfabeto é irrelevante para a teoria matemática subjacente. Um símbolo pode ser uma letra, um dígito, um caractere especial, ou qualquer entidade atômica que possa ser distinguida de outras. A única exigência é que os símbolos sejam mutuamente distintos e que o conjunto seja finito.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Alfabetos, Linguagens e Strings: Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "01a-lexico.html#alfabetos-os-blocos-fundamentais",
    "href": "01a-lexico.html#alfabetos-os-blocos-fundamentais",
    "title": "3  Alfabetos, Linguagens e Strings: Fundamentos Matemáticos",
    "section": "",
    "text": "\\(\\Sigma = \\{a_1, a_2, \\ldots, a_n\\}\\): o alfabeto é um conjunto formado por \\(n\\) símbolos distintos (\\(a_1\\) até \\(a_n\\)).\n\\(|\\Sigma| = n\\): o tamanho do alfabeto (número de símbolos) é \\(n\\).\n\\(n &lt; \\infty\\): o alfabeto é finito (não possui infinitos símbolos).\n\\(n \\geq 1\\): o alfabeto é não vazio, ou seja, contém pelo menos um símbolo.\n\n\n\n3.1.1 Exemplos de Alfabetos\nA teoria ganha vida por meio de exemplos concretos. Considere os seguintes alfabetos frequentemente utilizados na ciência da computação:\n\nAlfabeto binário: \\(\\Sigma_{\\text{bin}} = \\{0, 1\\}\\). Este alfabeto fundamental aparece em representações binárias, lógica booleana e circuitos digitais. A cardinalidade deste alfabeto é \\(|\\Sigma_{\\text{bin}}| = 2\\).\nAlfabeto das letras minúsculas: \\(\\Sigma_{\\text{abc}} = \\{a, b, c, \\ldots, z\\}\\). Usado para modelar identificadores em linguagens de programação e texto em linguagem natural. Sua cardinalidade é \\(|\\Sigma_{\\text{abc}}| = 26\\).\nAlfabeto para representação de números decimais: \\(\\Sigma_{\\text{dec}} = \\{+, -, ., 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\\}\\). Fundamental para a representação de números em notação decimal. Incluindo números reais, positivos e negativos, considerando o símbolo ‘.’ como separador decimal. A cardinalidade é \\(|\\Sigma_{\\text{dec}}| = 13\\).\nAlfabeto ASCII para Expressões Aritméticas: \\(\\Sigma_{\\text{arit}} = \\{a, b, \\ldots, z, 0, 1, \\ldots, 9, +, -, *, /\\}\\). Este exemplo define um subconjunto concreto e útil do ASCII, adequado para modelar identificadores de uma única letra e operações aritméticas simples. O uso de \\(\\ldots\\) (elipses) aqui é uma notação concisa para representar todos os caracteres dentro de um intervalo sequencial bem definido, neste caso, as letras minúsculas de ‘a’ a ‘z’ e os dígitos de ‘0’ a ‘9’. Ao contrário de uma definição com uma cardinalidade ambígua, esta abordagem permite um cálculo exato do tamanho do alfabeto considerando letras = \\({a, b, \\ldots, z}\\) (26), dígitos = \\({0, 1, \\ldots, 9}\\) (10), operadores = \\({+, -, *, /}\\) seria:\n\n\\[|\\Sigma_{\\text{arit}}| = \\text{(letras)} + \\text{(dígitos)} + \\text{(operadores)} = 26 + 10 + 4 = 40\\]\n\n\n3.1.2 Propriedades Matemáticas dos Alfabetos\nA definição formal de alfabeto como conjunto finito implica que precisaremos considerar três propriedades importantes:\n\nFinitude: todo alfabeto possui um número finito de símbolos. Esta restrição não é uma limitação prática, mas sim uma necessidade teórica que garante a decidibilidade de muitos problemas computacionais. Decidibilidade, neste contexto, refere-se à capacidade de determinar se uma string pertence a uma linguagem definida sobre um alfabeto.\nDistinção: cada símbolo em um alfabeto é único e distinguível. Formalmente, se \\(a, b \\in \\Sigma\\) e \\(a \\neq b\\), então \\(a\\) e \\(b\\) são símbolos diferentes.\nNão-ordenação inerente: um alfabeto, sendo um conjunto matemático, não possui uma ordenação natural. Qualquer ordenação, como a ordem alfabética, é uma convenção externa imposta para conveniência.\n\n\n\n3.1.3 Exercícios 1\n\nDetermine a cardinalidade dos seguintes alfabetos:\n\n\\(\\Sigma_1 = \\{a, b, c, +, -, *, /, (, )\\}\\);\n\\(\\Sigma_2 = \\{0, 1, 2, \\ldots, 9, A, B, C, D, E, F\\}\\) (hexadecimal);\n\\(\\Sigma_3 = \\{\\text{verdadeiro}, \\text{falso}, \\land, \\lor, \\neg, (, )\\}\\).\n\nConstrua alfabetos apropriados para os seguintes contextos:\n\nExpressões lógicas booleanas simples com variáveis \\(p\\), \\(q\\), \\(r\\);\nNúmeros em notação científica (ex: \\(1.23 \\times 10^{-4}\\));\nCoordenadas cartesianas no formato \\((x, y)\\).\n\nDetermine quais dos seguintes conjuntos são alfabetos válidos segundo a definição formal. Justifique sua resposta:\n\n\\(A = \\emptyset\\);\n\\(B = \\{\\epsilon\\}\\);\n\\(C = \\{1, 2, 3, \\ldots\\}\\);\n\\(D = \\{a, b, a\\}\\).\n\nDado o conjunto de strings \\(S = \\{\\text{if}, \\text{then}, \\text{else}, \\text{fi}\\}\\), determine o menor alfabeto \\(\\Sigma\\) tal que todas as strings em \\(S\\) possam ser formadas usando símbolos de \\(\\Sigma\\).\nCompare os alfabetos \\(\\Sigma_A = \\{0, 1\\}\\) e \\(\\Sigma_B = \\{a, b, c\\}\\) em termos de:\n\nCardinalidade;\nNúmero de strings de comprimento 3 que podem ser formadas;\nAplicabilidade para representar números binários.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Alfabetos, Linguagens e Strings: Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "01a-lexico.html#strings-sequências-sobre-alfabetos",
    "href": "01a-lexico.html#strings-sequências-sobre-alfabetos",
    "title": "3  Alfabetos, Linguagens e Strings: Fundamentos Matemáticos",
    "section": "3.2 Strings: Sequências Sobre Alfabetos",
    "text": "3.2 Strings: Sequências Sobre Alfabetos\nUma string (ou cadeia) sobre um alfabeto \\(\\Sigma\\) é uma sequência finita de símbolos escolhidos de \\(\\Sigma\\). Formalmente, uma string \\(w\\) de comprimento \\(n\\) pode ser representada como:\n\\[w = a_1a_2\\ldots a_n \\text{ tal que } a_i \\in \\Sigma \\text{ para } i = 1, 2, \\ldots, n\\]\nNa qual, temos:\n\n\\(w = a_1a_2\\ldots a_n\\): uma string \\(w\\) é uma sequência finita de símbolos concatenados.\n\\(a_i \\in \\Sigma \\quad \\text{para} \\quad i = 1, 2, \\ldots, n\\): cada elemento \\(a_i\\) na string pertence ao alfabeto \\(\\Sigma\\).\nA string possui comprimento \\(|w| = n\\) (número de símbolos na sequência). Por exemplo, se \\(w = abc\\), então \\(|w| = 3\\).\n\n\n3.2.1 A string Vazia\nUm caso especial é a string vazia, denotada por \\(\\epsilon\\), letra grega épsilon minúsculo. Esta string existe, não contém símbolos e possui comprimento zero:\n\\[\\epsilon = \\text{string vazia tal que } |\\epsilon| = 0\\]\nA string vazia desempenha um papel análogo ao número zero na aritmética, servindo como elemento neutro para a operação de concatenação de strings.\n\n\n3.2.2 Operações com Strings\nAssim como a aritmética define operações como adição e multiplicação sobre o conjunto dos números, a teoria das linguagens formais estabelece operações para manipular e combinar strings. Essas operações formam uma espécie de álgebra sobre as strings, permitindo-nos construir sequências complexas a partir de componentes mais simples de maneira formal e precisa. A seguir, exploraremos as operações mais importantes, começando pela mais fundamental de todas, que serve como base para quase todas as outras: a concatenação.\n\n3.2.2.1 Concatenação\nA operação fundamental com strings é a concatenação. Para duas strings \\(x\\) e \\(y\\), a concatenação \\(xy\\) é a string formada pela justaposição de \\(x\\) seguida por \\(y\\).\nDefinição formal: se \\(x = a_1a_2\\ldots a_m\\) e \\(y = b_1b_2\\ldots b_n\\), então:\n\\[xy = a_1a_2\\ldots a_mb_1b_2\\ldots b_n\\]\nAlternativamente, a concatenação pode ser representada com a mesma notação do produto escalar. Assim, a concatenação de \\(x\\) e \\(y\\) pode ser denotada como \\(x \\cdot y\\).\nPropriedades da concatenação:\n\nAssociatividade: para strings \\(x\\), \\(y\\), \\(z\\), temos \\((xy)z = x(yz)\\).\nElemento neutro: para qualquer string \\(w\\), \\(w\\epsilon = \\epsilon w = w\\).\nNão-comutatividade: em geral, \\(xy \\neq yx\\).\n\n\n\n3.2.2.2 Potências de Strings\nPara uma string \\(w\\) e um inteiro não-negativo \\(n\\), a potência \\(w^n\\) é definida recursivamente:\n\\[w^0 = \\epsilon\\] \\[w^{n+1} = w^n \\cdot w \\text{ para } n \\geq 0\\]\nPor exemplo, se \\(w = ab\\), então \\(w^3 = ababab\\).\nAntes de avançarmos para operações com linguagens, a atenta leitora deve distinguir a operação de potência de uma string da operação de Fechamento de Kleene, que será detalhada adiante. Enquanto a potência \\(w^n\\) aplica-se a uma única string \\(w\\) para produzir como resultado outra única string, o Fechamento de Kleene, denotado por um asterisco, como em \\(L^*\\), é uma operação que se aplica a um conjunto de strings, uma linguagem, ou a um conjunto de símbolos, um alfabeto. O resultado do Fechamento de Kleene não é uma string, mas sim um novo conjunto de strings, geralmente infinito, representando todas as combinações possíveis de elementos do conjunto original. A potência é, portanto, uma operação sobre elementos individuais, enquanto o Fechamento de Kleene é uma operação sobre conjuntos.\n\n\n3.2.2.3 Reverso de Strings\nO reverso de uma string \\(w\\), denotado por \\(w^R\\), é a string obtida invertendo a ordem dos símbolos:\nDefinição recursiva:\n\n\\(\\epsilon^R = \\epsilon\\);\n\\((wa)^R = aw^R\\) para \\(w \\in \\Sigma^*\\) e \\(a \\in \\Sigma\\).\n\nPor exemplo, se \\(w = abc\\), então \\(w^R = cba\\).\n\n\n\n3.2.3 Substrings, Prefixos e Sufixos: Entendendo as Partes de uma String\nAo trabalhar com texto e linguagens de programação, raramente tratamos uma string como uma unidade indivisível. Quase sempre precisamos inspecionar, extrair ou analisar suas partes internas. Os conceitos de substring, prefixo e sufixo nos dão um vocabulário formal e preciso para descrever essas partes.\nEste vocabulário formado de termos para as partes em que podemos dividir strings será relevante quando: estivermos estudando analisadores léxicos e sintáticos. Estes analisadores frequentemente operam sobre essas partes para identificar padrões, validar estruturas e extrair informações significativas; criando ferramentas de busca e análise de texto, onde precisamos localizar ocorrências de padrões dentro de grandes volumes de texto; ou desenvolvendo algoritmos que manipulam ou processam strings de maneira eficiente; ou ainda, ao implementar funcionalidades de autocompletar em editores de texto e IDEs, onde o sistema sugere palavras ou comandos com base no que o usuário já digitou.\n\n3.2.3.1 Substring\nUma substring é simplesmente um pedaço ou um segmento contínuo de caracteres que está dentro de outra string. Formalmente dizemos que uma string $v$ é uma substring de $w$ se for possível encontrar duas outras strings, $x$ (a parte que vem antes) e $y$ (a parte que vem depois), de forma que $w = xvy$. As strings $x$ ou $y$ podem, inclusive, ser vazias.\nExemplo: Para a string $w = compilador$, as palavras pila, com, dor e compilador são todas substrings válidas. No caso de pila, $x = com$ e $y = dor$.\n\n\n3.2.3.2 Prefixo\nUm prefixo é um trecho que começa exatamente no início de uma string. Formalmente dizemos que uma string $v$ é um prefixo de $w$ se houver uma string $y$ (o resto da string) tal que $w = vy$.\nExemplo: Para $w = compilador$, alguns prefixos são c, com, compi e a própria string compilador.\n\n3.2.3.2.1 Prefixo Próprio\nDizemos que uma string é um prefixo próprio de outra string se for um prefixo que não seja a própria string inteira. Formalmente, um prefixo $v$ de $w$ é dito próprio se $v \\neq w$.\nExemplo: Para $w = compilador$, compi é um prefixo próprio, mas compilador não é.\n\n\n\n3.2.3.3 Sufixo\nUm sufixo é um trecho que termina exatamente no final de uma string. Formalmente dizemos que uma string $v$ é um sufixo de $w$ se houver uma string $x$ (a parte inicial) tal que $w = xv$.\nExemplo: Para $w = compilador$, alguns sufixos são r, dor, lador e a própria string compilador.\n\n3.2.3.3.1 Sufixo Próprio\nDizemos que uma string é um sufixo próprio se for qualquer sufixo que não seja a própria string inteira. Formalmente, um sufixo $v$ de $w$ é dito próprio se `\\(v \\neq w\\).\nExemplo: Para $w = compilador$, lador é um sufixo próprio, mas compilador não é.\nDefinir formalmente as partes de uma string é fundamental para a ciência da computação. Estas partes são a base para inúmeras operações e algoritmos:\n\n\n\n\n3.2.4 Exercícios 2\n\nSejam \\(x = ab\\) e \\(y = cd\\). Calcule:\n\n\\(xy\\) e \\(yx\\);\n\\(x^3\\) e \\(y^2\\);\n\\((xy)^2\\) e \\(x^2y^2\\);\n\\(|x^n|\\) em função de \\(n\\).\n\nPara as strings dadas, determine seus reversos:\n\n\\(w_1 = abcde\\);\n\\(w_2 = palíndromo\\);\n\\(w_3 = \\epsilon\\) (string vazia);\nProve que \\((\\epsilon)^R = \\epsilon\\).\n\nDemonstre as seguintes propriedades usando strings específicas:\n\nAssociatividade: \\((xy)z = x(yz)\\) para \\(x = a\\), \\(y = bc\\), \\(z = d\\);\nElemento neutro: \\(w\\epsilon = \\epsilon w = w\\) para \\(w = abc\\);\nNão-comutatividade: encontre \\(x\\) e \\(y\\) tais que \\(xy \\neq yx\\).\n\nPara a string \\(w = compilador\\):\n\nListe todos os prefixos próprios;\nListe todos os sufixos próprios;\nIdentifique todas as substrings de comprimento 4;\nDetermine quantos prefixos e sufixos \\(w\\) possui no total.\n\nSeja \\(w = aba\\). Calcule:\n\n\\((w^R)^2\\);\n\\((w^2)^R\\);\n\\(w^R w\\);\nVerifique se \\((w^2)^R = (w^R)^2\\).",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Alfabetos, Linguagens e Strings: Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "01a-lexico.html#linguagens-conjuntos-de-strings",
    "href": "01a-lexico.html#linguagens-conjuntos-de-strings",
    "title": "3  Alfabetos, Linguagens e Strings: Fundamentos Matemáticos",
    "section": "3.3 Linguagens: Conjuntos de Strings",
    "text": "3.3 Linguagens: Conjuntos de Strings\nUma linguagem sobre um alfabeto \\(\\Sigma\\) é simplesmente um conjunto de strings formadas a partir de símbolos de \\(\\Sigma\\). Formalmente:\n\\[L \\subseteq \\Sigma^*\\]\nna qual \\(\\Sigma^*\\) denota o conjunto de todas as strings possíveis sobre \\(\\Sigma\\), incluindo a string vazia \\(\\epsilon\\).\n\n3.3.1 O Conjunto Universal \\(\\Sigma^*\\)\nO conjunto \\(\\Sigma^*\\) (lê-se sigma estrela ou fechamento de Kleene de \\(\\Sigma\\)) é definido como:\n\\[\\Sigma^* = \\Sigma^0 \\cup \\Sigma^1 \\cup \\Sigma^2 \\cup \\Sigma^3 \\cup \\ldots\\]\nna qual:\n\n\\(\\Sigma^0 = \\{\\epsilon\\}\\);\n\\(\\Sigma^1 = \\Sigma = \\{a_1, a_2, \\ldots, a_n\\}\\);\n\\(\\Sigma^2 = \\{xy \\mid x, y \\in \\Sigma\\}\\);\n\\(\\Sigma^k = \\{w \\mid |w| = k \\text{ e } w \\text{ é string sobre } \\Sigma\\}\\).\n\nDessa forma, \\(\\Sigma^k\\) é o conjunto de todas as strings de comprimento \\(k\\) formadas por símbolos de \\(\\Sigma\\)\nPara o alfabeto binário \\(\\Sigma = \\{0, 1\\}\\):\n\\[\\Sigma^* = \\{\\epsilon, 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, 100, 101, 110, 111, \\ldots\\}\\]\n\n\n3.3.2 Exemplos de Linguagens\n\nLinguagem vazia: \\(L_{\\emptyset} = \\emptyset\\). A linguagem que não contém nenhuma string. É importante distinguir da linguagem que contém apenas a string vazia.\nLinguagem contendo apenas a string vazia: \\(L_{\\epsilon} = \\{\\epsilon\\}\\). Uma linguagem com exatamente um elemento, a string vazia.\nLinguagem de todas as strings binárias: \\(L_{\\text{todas}} = \\{0, 1\\}^*\\). Esta linguagem contém todas as strings possíveis sobre o alfabeto binário.\nLinguagem de strings binárias de comprimento par: \\[L_{\\text{par}} = \\{w \\in \\{0, 1\\}^* \\mid |w| \\text{ é par}\\}\\]\n\nLinguagem de identificadores válidos: para modelar identificadores em uma linguagem de programação: \\[L_{\\text{id}} = \\{w \\in \\{a, \\ldots, z, A, \\ldots, Z, 0, \\ldots, 9, \\_\\}^* \\mid w \\text{ inicia com letra ou } \\_\\}\\]\n\n\n3.3.3 Operações com Linguagens\nDa mesma forma que operamos com strings individuais, podemos realizar operações com linguagens inteiras para construir novas linguagens a partir de outras. Uma vez que uma linguagem é, por definição, um conjunto de strings (\\(L \\subseteq \\Sigma^*\\)), ela herda naturalmente as operações fundamentais da teoria dos conjuntos. Adicionalmente, estenderemos as operações que vimos para strings, como a concatenação, para que se apliquem a conjuntos inteiros de strings. Iniciaremos nossa exploração com as operações que derivam diretamente da natureza das linguagens como conjuntos.\n\n3.3.3.1 União de Linguagens\nA união representa a operação mais natural e intuitiva entre linguagens, capturando a essência da escolha ou alternativa. Quando unimos duas linguagens, criamos uma nova linguagem que aceita qualquer string que pertença a pelo menos uma das linguagens originais. É como estabelecer uma regra de aceitação do tipo ou isto, ou aquilo, ou ambos.\nEsta operação herda diretamente da teoria dos conjuntos, mas seu significado em linguagens formais transcende a mera manipulação de conjuntos, tornando-se uma ferramenta fundamental para expressar alternativas em especificações de linguagens.\nUnião: A união de duas linguagens \\(L_1\\) e \\(L_2\\) é:\n\\[L_1 \\cup L_2 = \\{w \\mid w \\in L_1 \\text{ ou } w \\in L_2\\}\\]\nA palavra ou aqui é inclusiva: uma string pertence à união se está em \\(L_1\\), ou em \\(L_2\\), ou em ambas.\n\n3.3.3.1.1 Exemplos de União\nExemplo 1: Linguagens de tokens Simples\n\n\\(L_{\\text{num}} = \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\\}\\) (dígitos);\n\\(L_{\\text{op}} = \\{+, -, *, /\\}\\) (operadores);\n\\(L_{\\text{token}} = L_{\\text{num}} \\cup L_{\\text{op}} = \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, +, -, *, /\\}\\).\n\nEsta união modela os tokens básicos de uma calculadora simples.\nExemplo 2: Linguagens de Comprimentos Específicos\n\n\\(L_{\\text{par}} = \\{w \\in \\{a, b\\}^* \\mid |w| \\text{ é par}\\}\\);\n\\(L_{\\text{ímpar}} = \\{w \\in \\{a, b\\}^* \\mid |w| \\text{ é ímpar}\\}\\);\n\n\\(L_{\\text{par}} \\cup L_{\\text{ímpar}} = \\{a, b\\}^*\\) (todas as strings sobre \\(\\{a, b\\}\\)).\n\nExemplo 3: Linguagens Sobrepostas\n\n\\(L_1 = \\{a, ab, abc\\}\\);\n\\(L_2 = \\{ab, bc, c\\}\\);\n\\(L_1 \\cup L_2 = \\{a, ab, abc, bc, c\\}\\).\n\nNote que \\(ab\\) aparece em ambas as linguagens, mas na união aparece apenas uma vez, seguindo a definição de conjunto.\n\n\n3.3.3.1.2 Propriedades Algébricas da União\nA união satisfaz propriedades fundamentais que a tornam uma operação bem-comportada:\n1. Comutatividade: \\[L_1 \\cup L_2 = L_2 \\cup L_1\\]\nA ordem das linguagens na união não importa.\n2. Associatividade: \\[(L_1 \\cup L_2) \\cup L_3 = L_1 \\cup (L_2 \\cup L_3)\\]\nPodemos agrupar uniões de qualquer forma, permitindo escrever \\(L_1 \\cup L_2 \\cup L_3\\) sem ambiguidade.\n3. Elemento Neutro: \\[L \\cup \\emptyset = \\emptyset \\cup L = L\\]\nA linguagem vazia não adiciona elementos a qualquer união.\n4. Idempotência: \\[L \\cup L = L\\]\nUnir uma linguagem consigo mesma não a modifica.\n5. Absorção: \\[L \\cup \\Sigma^* = \\Sigma^*\\]\nUnir qualquer linguagem com a linguagem universal resulta na linguagem universal.\n\n\n3.3.3.1.3 Aplicações Práticas da União\n1. Definição de Alfabetos Estendidos: em linguagens de programação, frequentemente definimos categorias de caracteres como:\n\nLetras: \\(L_{\\text{letras}} = \\{a, b, \\ldots, z, A, B, \\ldots, Z\\}\\);\nDígitos: \\(L_{\\text{dígitos}} = \\{0, 1, \\ldots, 9\\}\\);\n\nAlfanuméricos: \\(L_{\\text{alfanum}} = L_{\\text{letras}} \\cup L_{\\text{dígitos}}\\).\n\n2. Validação de Formatos Alternativos: para aceitar diferentes formatos de data:\n\n\\(L_{\\text{br}} = \\{dd/mm/aaaa\\}\\) (formato brasileiro);\n\\(L_{\\text{us}} = \\{mm/dd/aaaa\\}\\) (formato americano);\n\\(L_{\\text{iso}} = \\{aaaa-mm-dd\\}\\) (formato ISO);\n\\(L_{\\text{data}} = L_{\\text{br}} \\cup L_{\\text{us}} \\cup L_{\\text{iso}}\\).\n\n3. Tratamento de Variações Lexicais: em processamento de linguagem natural:\n\n\\(L_{\\text{sim}} = \\{\\text{sim}, \\text{yes}, \\text{oui}, \\text{sí}\\}\\);\n\\(L_{\\text{não}} = \\{\\text{não}, \\text{no}, \\text{non}\\}\\);\n\\(L_{\\text{resposta}} = L_{\\text{sim}} \\cup L_{\\text{não}}\\).\n\n\n\n\n3.3.3.2 Concatenação de Linguagens\nA concatenação captura a essência da composição sequencial: formar novas strings justapondo elementos de duas linguagens em uma ordem específica. É como estabelecer uma regra de construção do tipo primeiro algo de \\(L_1\\), depois algo de \\(L_2\\).\nEsta operação estende naturalmente a concatenação de strings individuais para conjuntos inteiros de strings, criando um mecanismo fundamental para a construção de linguagens complexas a partir de componentes mais simples.\nConcatenação: A concatenação de duas linguagens \\(L_1\\) e \\(L_2\\) é:\n\\[L_1 \\cdot L_2 = \\{xy \\mid x \\in L_1 \\text{ e } y \\in L_2\\}\\]\nO resultado é o conjunto de todas as strings possíveis formadas escolhendo-se uma string de \\(L_1\\) seguida por uma string de \\(L_2\\).\n\n3.3.3.2.1 Exemplos de Concatenação\nExemplo 1: construção de Identificadores\n\n\\(L_{\\text{letra}} = \\{a, b, c, \\ldots, z\\}\\);\n\\(L_{\\text{dígito}} = \\{0, 1, 2, \\ldots, 9\\}\\);\n\\(L_{\\text{letra}} \\cdot L_{\\text{dígito}} = \\{a0, a1, \\ldots, a9, b0, b1, \\ldots, z9\\}\\).\n\nEsta concatenação gera todos os identificadores de exatamente dois caracteres que começam com uma letra seguida de um dígito.\nExemplo 2: Construção Passo a Passo\n\n\\(L_1 = \\{a, bb\\}\\);\n\\(L_2 = \\{c, dd\\}\\);\n\\(L_1 \\cdot L_2 = \\{ac, add, bbc, bbdd\\}\\).\n\nCálculo detalhado:\n\n\\(a \\in L_1, c \\in L_2 \\Rightarrow ac \\in L_1 \\cdot L_2\\)\n\\(a \\in L_1, dd \\in L_2 \\Rightarrow add \\in L_1 \\cdot L_2\\)\n\n\\(bb \\in L_1, c \\in L_2 \\Rightarrow bbc \\in L_1 \\cdot L_2\\)\n\\(bb \\in L_1, dd \\in L_2 \\Rightarrow bbdd \\in L_1 \\cdot L_2\\)\n\nExemplo 3: Concatenação com Linguagem Unitária\n\n\\(L = \\{hello, hi\\}\\);\n\\(\\{!\\} = \\{!\\}\\);\n\\(L \\cdot \\{!\\} = \\{hello!, hi!\\}\\).\n\n\n\n3.3.3.2.2 Propriedades Algébricas da Concatenação\nA concatenação possui um conjunto distinto de propriedades que a diferenciam significativamente da união:\n1. Associatividade: \\[(L_1 \\cdot L_2) \\cdot L_3 = L_1 \\cdot (L_2 \\cdot L_3)\\]\nPodemos agrupar concatenações de qualquer forma, permitindo escrever \\(L_1 \\cdot L_2 \\cdot L_3\\) sem ambiguidade.\n2. Elemento Neutro: \\[L \\cdot \\{\\epsilon\\} = \\{\\epsilon\\} \\cdot L = L\\]\nA linguagem contendo apenas a string vazia atua como elemento neutro para a concatenação.\n3. Elemento Anulador: \\[L \\cdot \\emptyset = \\emptyset \\cdot L = \\emptyset\\]\nA linguagem vazia anula qualquer concatenação.\n4. Não-Comutatividade (propriedade crucial): \\[L_1 \\cdot L_2 \\neq L_2 \\cdot L_1 \\text{ (em geral)}\\]\nA ordem na concatenação importa fundamentalmente. Do exemplo anterior:\n\n\\(L_1 \\cdot L_2 = \\{ac, add, bbc, bbdd\\}\\);\n\\(L_2 \\cdot L_1 = \\{ca, cbb, dda, ddbb\\}\\);\n\n5. Distributividade sobre União: \\[L_1 \\cdot (L_2 \\cup L_3) = L_1 \\cdot L_2 \\cup L_1 \\cdot L_3\\] \\[(L_1 \\cup L_2) \\cdot L_3 = L_1 \\cdot L_3 \\cup L_2 \\cdot L_3\\]\nEsta propriedade é fundamental para simplificação de expressões regulares.\n\n\n3.3.3.2.3 Casos Especiais da Concatenação\n1. Concatenação com a Linguagem Universal: \\[L \\cdot \\Sigma^* = \\{xy \\mid x \\in L, y \\in \\Sigma^*\\}\\]\nResulta em todas as strings que começam com algum elemento de \\(L\\).\n2. Autoconcatenação: \\[L \\cdot L = L^2 = \\{xy \\mid x, y \\in L\\}\\]\nBase para a definição de potências de linguagens.\n3. Concatenação de Linguagens Infinitas:\nSe \\(L_1\\) tem \\(m\\) elementos e \\(L_2\\) tem \\(n\\) elementos, então \\(|L_1 \\cdot L_2| \\leq mn\\), com igualdade quando todas as concatenações resultam em strings distintas.\n\n\n3.3.3.2.4 Aplicações Práticas da Concatenação\n1. Construção de Padrões Estruturados: Para validar endereços de email (versão muito simplificada):\n\n\\(L_{\\text{usuário}} = \\{[a-zA-Z0-9]+\\}\\);\n\\(L_{\\text{domínio}} = \\{[a-zA-Z0-9.-]+\\}\\);\n\\(L_{\\text{ext}} = \\{[a-zA-Z]{2,}\\}\\);\n\\(L_{\\text{email}} = L_{\\text{usuário}} \\cdot \\{@\\} \\cdot L_{\\text{domínio}} \\cdot \\{.\\} \\cdot L_{\\text{ext}}\\).\n\n2. Análise Sintática: Em linguagens de programação:\n\n\\(L_{\\text{tipo}} = \\{int, float, string\\}\\);\n\\(L_{\\text{id}} = \\{[a-zA-Z][a-zA-Z0-9]*\\}\\);\n\\(L_{\\text{declaração}} = L_{\\text{tipo}} \\cdot \\{\\) \\(\\} \\cdot L_{\\text{id}}\\).\n\n3. Protocolos de Comunicação:\n\n\\(L_{\\text{header}} = \\{HTTP/1.1, HTTP/2.0\\}\\);\n\\(L_{\\text{método}} = \\{GET, POST, PUT, DELETE\\}\\);\n\\(L_{\\text{requisição}} = L_{\\text{método}} \\cdot \\{\\) \\(\\} \\cdot L_{\\text{URL}} \\cdot \\{\\) \\(\\} \\cdot L_{\\text{header}}\\).\n\n\n\n3.3.3.2.5 Autômatos e Complexidade\nA concatenação de linguagens finitas tem complexidade \\(O(|L_1| \\times |L_2|)\\) no caso geral, mas pode ser otimizada quando as linguagens têm estruturas específicas. Esta consideração é crucial em implementações práticas de analisadores léxicos e sintáticos.\nNa teoria de autômatos, a concatenação corresponde à construção sequencial: aceitar uma string em \\(L_1 \\cdot L_2\\) significa processar uma parte inicial com um autômato para \\(L_1\\) e a parte restante com um autômato para \\(L_2\\). Esta intuição será fundamental para construções como a de Thompson para expressões regulares.\nA concatenação, portanto, não é apenas uma operação matemática, mas a operação que captura a essência da sequencialidade na computação e na especificação de linguagens.\n\n\n\n3.3.3.3 Interseção e Complemento\nAlém das operações que derivam diretamente da teoria dos conjuntos (união) e das que estendem conceitos de strings (concatenação), duas operações adicionais desempenham papéis fundamentais na teoria de linguagens formais: a interseção e o complemento. Estas operações, embora não sejam diretamente expressáveis por expressões regulares simples, são cruciais para compreender o poder e as limitações das Linguagens Regulares.\nInterseção: a interseção de duas linguagens \\(L_1\\) e \\(L_2\\) será dada por:\n\\[L_1 \\cap L_2 = \\{w \\mid w \\in L_1 \\text{ e } w \\in L_2\\}\\]\nA interseção captura as strings que pertencem simultaneamente a ambas as linguagens. Esta operação é particularmente útil para construir linguagens que devem satisfazer múltiplas condições.\nExemplo prático: Considere uma linguagem de programação onde queremos definir identificadores que:\n\nComecem com letra: \\(L_1 = \\{w \\in \\{a, \\ldots, z, 0, \\ldots, 9\\}^* \\mid w \\text{ inicia com letra}\\}\\)\nTenham comprimento par: \\(L_2 = \\{w \\in \\{a, \\ldots, z, 0, \\ldots, 9\\}^* \\mid |w| \\text{ é par}\\}\\)\n\nA interseção \\(L_1 \\cap L_2\\) contém exatamente os identificadores que começam com letra e têm comprimento par.\nComplemento: O complemento de uma linguagem \\(L\\) sobre um alfabeto \\(\\Sigma\\) é:\n\\[\\overline{L} = \\Sigma^* - L = \\{w \\in \\Sigma^* \\mid w \\notin L\\}\\]\nO complemento de \\(L\\) contém todas as strings possíveis sobre \\(\\Sigma\\) exceto aquelas que pertencem a \\(L\\). Esta operação é fundamental para expressar condições negativas.\nExemplo prático: Se \\(L\\) é a linguagem de todas as strings binárias que contêm a substring \\(01\\), então \\(\\overline{L}\\) é a linguagem de todas as strings binárias que não contêm \\(01\\) como substring — exatamente o padrão que vimos nos exemplos anteriores ser descrito pela expressão \\(1^*0^*\\).\nNa prática, interseção e complemento aparecem frequentemente em:\n\nValidação de dados: Verificar que uma entrada satisfaz múltiplas condições simultaneamente\nAnálise de segurança: Definir padrões de tráfego permitido como complemento de padrões maliciosos\nProcessamento de linguagens naturais: Filtrar textos que pertencem a uma categoria mas não a outra\nOtimização de compiladores: Análise de fluxo de dados onde certas condições devem ser simultaneamente satisfeitas\n\nEstas operações, embora mais abstratas que concatenação e união, formam uma base teórica sólida que será fundamental quando explorarmos autômatos finitos e técnicas mais avançadas de análise de linguagens.\n\n\n3.3.3.4 Potências de Linguagens\nPara uma linguagem \\(L\\) e um inteiro não-negativo \\(n\\):\n\\[L^0 = \\{\\epsilon\\}\\] \\[L^{n+1} = L^n \\cdot L \\text{ para } n \\geq 0\\]\n\n\n3.3.3.5 Fechamento de Kleene e Fechamento Positivo\nEntre todas as operações com linguagens, o fechamento de Kleene destaca-se como uma das mais poderosas e fundamentais. Nomeada em homenagem ao matemático Stephen Kleene, esta operação captura a essência da repetição arbitrária — um conceito central tanto na teoria formal quanto nas aplicações práticas da computação.\nIntuitivamente, o fechamento de Kleene de uma linguagem \\(L\\) representa todas as possíveis concatenações de zero ou mais elementos de \\(L\\). É como se perguntássemos: Que strings podemos formar se nos for permitido escolher elementos de \\(L\\) quantas vezes quisermos, incluindo a possibilidade de não escolher nenhum?\nFechamento de Kleene: O fechamento de Kleene de uma linguagem \\(L\\) é:\n\\[L^* = L^0 \\cup L^1 \\cup L^2 \\cup L^3 \\cup \\ldots = \\bigcup_{i=0}^{\\infty} L^i\\]\nEsta definição revela uma estrutura elegante: \\(L^*\\) é construído pela união de todas as potências possíveis de \\(L\\), começando de \\(L^0 = \\{\\epsilon\\}\\) (a linguagem contendo apenas a string vazia) e estendendo-se infinitamente.\nFechamento Positivo: O fechamento positivo de \\(L\\) é uma variação que exclui a possibilidade de não escolher nada:\n\\[L^+ = L^1 \\cup L^2 \\cup L^3 \\cup \\ldots = \\bigcup_{i=1}^{\\infty} L^i = L \\cdot L^*\\]\nA diferença fundamental é que \\(L^+\\) exige pelo menos uma escolha de \\(L\\), enquanto \\(L^*\\) permite zero escolhas.\nA relação entre fechamento de Kleene e o fechamento positivo depende crucialmente de um fator: se a string vazia \\(\\epsilon\\) pertence ou não à linguagem original \\(L\\).\nCaso 1: Se \\(\\epsilon \\notin L\\), então: \\[L^+ = L^* - \\{\\epsilon\\}\\]\nNeste caso, como \\(\\epsilon \\notin L\\), ela só pode aparecer em \\(L^*\\) por meio de \\(L^0 = \\{\\epsilon\\}\\). Todas as outras potências \\(L^i\\) para \\(i \\geq 1\\) não contêm \\(\\epsilon\\). Estas potências que não contêm \\(\\epsilon\\) são formadas por concatenações de elementos de \\(L\\) que não incluem \\(\\epsilon\\).\nCaso 2: Se \\(\\epsilon \\in L\\), então: \\[L^+ = L^*\\]\nNeste caso, se \\(\\epsilon \\in L = L^1\\), então \\(\\epsilon \\in L^+\\). Como \\(\\epsilon\\) também está em \\(L^*\\) (via \\(L^0\\)), e ambos contêm todas as outras potências, os conjuntos são idênticos.\n\n3.3.3.5.1 Exemplos\nExemplo 1: \\(L = \\{a\\}\\)\n\n\\(L^* = \\{\\epsilon, a, aa, aaa, aaaa, \\ldots\\} = \\{a^n \\mid n \\geq 0\\}\\)\n\\(L^+ = \\{a, aa, aaa, aaaa, \\ldots\\} = \\{a^n \\mid n \\geq 1\\}\\)\nComo \\(\\epsilon \\notin L\\), temos \\(L^+ = L^* - \\{\\epsilon\\}\\)\n\nExemplo 2: \\(L = \\{ab, cd\\}\\)\n\n\\(L^* = \\{\\epsilon, ab, cd, abab, abcd, cdab, cdcd, ababab, \\ldots\\}\\)\n\\(L^+ = \\{ab, cd, abab, abcd, cdab, cdcd, ababab, \\ldots\\}\\)\nNovamente, \\(L^+ = L^* - \\{\\epsilon\\}\\)\n\nExemplo 3: \\(L = \\{\\epsilon, a\\}\\)\n\n\\(L^* = L^+ = \\{\\epsilon, a, aa, aaa, \\ldots\\} = \\{a^n \\mid n \\geq 0\\}\\)\nComo \\(\\epsilon \\in L\\), ambos os fechamentos são idênticos\n\n\n\n3.3.3.5.2 Propriedades Matemáticas Fundamentais\nOs fechamentos de Kleene satisfazem várias propriedades algébricas importantes:\n1. Idempotência do Fechamento de Kleene: \\[(L^*)^* = L^*\\]\nIntuição: Aplicar o fechamento de Kleene duas vezes não adiciona novos elementos. Se já temos todas as concatenações possíveis de \\(L\\), concatenar essas concatenações não produz nada novo.\n2. Inclusão da Linguagem Original: \\[L \\subseteq L^+ \\subseteq L^*\\]\n3. Elemento Neutro: \\[L^* = \\{\\epsilon\\} \\cup L^+\\]\n4. Relação com Concatenação: \\[L^* = \\{\\epsilon\\} \\cup L \\cdot L^*\\]\nEsta última propriedade fornece uma definição recursiva alternativa para o fechamento de Kleene, extremamente útil em demonstrações formais e construções de autômatos.\n\n\n3.3.3.5.3 Casos Especiais Importantes\nLinguagem Vazia: Para \\(L = \\emptyset\\): \\[\\emptyset^* = \\{\\epsilon\\}\\] \\[\\emptyset^+ = \\emptyset\\]\nJustificativa: Como não podemos escolher elementos de \\(\\emptyset\\), a única concatenação possível é a de zero elementos, resultando em \\(\\epsilon\\).\nLinguagem com string Vazia: Para \\(L = \\{\\epsilon\\}\\): \\[\\{\\epsilon\\}^* = \\{\\epsilon\\}\\] \\[\\{\\epsilon\\}^+ = \\{\\epsilon\\}\\]\nJustificativa: Concatenar \\(\\epsilon\\) consigo mesmo qualquer número de vezes sempre resulta em \\(\\epsilon\\).\n\n\n3.3.3.5.4 Aplicações Práticas em Ciência da Computação\n1. Análise Léxica: o padrão [a-zA-Z][a-zA-Z0-9]* para identificadores utiliza o fechamento de Kleene implicitamente, onde [a-zA-Z0-9]* representa \\(\\Sigma^*\\) para o alfabeto alfanumérico.\n2. Expressões Regulares em Editores de Texto: o operador * em expressões regulares corresponde diretamente ao fechamento de Kleene, permitindo buscar padrões com repetições arbitrárias.\n3. Protocolos de Comunicação: muitos protocolos definem mensagens como sequências de unidades básicas, onde o fechamento de Kleene modela a repetição arbitrária dessas unidades.\n4. Análise de Algoritmos: estruturas como loops infinitos ou processos iterativos são naturalmente modelados usando fechamentos de Kleene de operações básicas.\n\n\n3.3.3.5.5 Conexão com Autômatos Finitos\nO fechamento de Kleene possui uma interpretação natural em termos de autômatos finitos: \\(L^*\\) corresponde à linguagem aceita por um autômato que pode reiniciar infinitas vezes após aceitar uma string de \\(L\\). Esta conexão será fundamental quando explorarmos a construção de Thompson para converter expressões regulares em autômatos.\n\n\n3.3.3.5.6 Fechamento como Operação Universal\nUma perspectiva interessante é que o fechamento de Kleene pode ser visto como a menor solução para a equação: \\[X = \\{\\epsilon\\} \\cup L \\cdot X\\]\nEsta caracterização como ponto fixo mínimo conecta os fechamentos de Kleene com áreas avançadas da ciência da computação, incluindo semântica denotacional e análise de programas.\nO fechamento de Kleene, portanto, não é apenas uma operação matemática abstrata, mas uma ferramenta conceitual poderosa que captura a essência da computação iterativa e da geração de linguagens por meio de repetição controlada.\n\n\n\n3.3.3.6 Propriedades e Fechamento\nUma propriedade notável das Linguagens Regulares é que elas são fechadas sob ambas as operações:\nTeorema (Fechamento das Linguagens Regulares):\nSe \\(L_1\\) e \\(L_2\\) são Linguagens Regulares sobre um alfabeto \\(\\Sigma\\), então:\n\n\\(L_1 \\cap L_2\\) é uma Linguagem Regular\n\\(\\overline{L_1}\\) é uma Linguagem Regular\n\nEsta propriedade de fechamento é uma das características que tornam as Linguagens Regulares tão robustas e úteis na prática. Ela garante que operações complexas entre Linguagens Regulares sempre produzem linguagens que permanecem dentro da mesma classe de complexidade.\n\n\n\n3.3.4 As Leis de De Morgan em Linguagens Regulares\nAs Leis de De Morgan são um par de regras fundamentais da teoria dos conjuntos que se aplicam diretamente às linguagens formais, uma vez que linguagens são conjuntos de strings. Elas estabelecem uma relação poderosa entre as operações de união, interseção e complemento.\nAs leis são definidas da seguinte forma:\n\nO complemento da união é a interseção dos complementos: \\[ \\overline{L_1 \\cup L_2} = \\overline{L_1} \\cap \\overline{L_2} \\]\nO complemento da interseção é a união dos complementos: \\[ \\overline{L_1 \\cap L_2} = \\overline{L_1} \\cup \\overline{L_2} \\]\n\nA principal importância dessas leis reside na sua capacidade de simplificar e transformar especificações de linguagens, especialmente aquelas que envolvem condições negativas (negações). As Leis de De Morgan nos permitem reescrever uma condição complexa de uma forma diferente e, muitas vezes, mais fácil de entender ou construir. A utilidade dessas leis se manifesta em:\n\nExpressar Negações Complexas: as Leis de De Morgan permitem converter uma negação sobre uma operação complexa (como não ser (A ou B)) em operações mais simples sobre negações individuais (não ser A E não ser B).\nManipulação Algébrica: Assim como na álgebra tradicional, as leis permitem manipular formalmente as descrições de linguagens para otimizá-las ou provar equivalências.\n\nComo exemplo, se quisermos uma linguagem que descreva strings que não terminam em $01$ nem em $10$, estamos descrevendo uma condição $ \\neg (A \\lor B) $. Usando a primeira lei de De Morgan, podemos transformar isso em $ (\\neg A) \\land (\\neg B) $:\n\\[ \\overline{L_{01} \\cup L_{10}} = \\overline{L_{01}} \\cap \\overline{L_{10}} \\]\nIsso significa que podemos construir a linguagem procurando por strings que satisfaçam duas condições simultaneamente: não terminar em $01$ E não terminar em $10$.\nNo contexto das Linguagens Regulares, as Leis de De Morgan são mais uma ferramenta teórica do que uma sintaxe prática dentro das expressões regulares básicas. As operações de interseção (\\(`\\cap`\\)) e complemento (\\(`\\overline{L}`\\)) não possuem operadores diretos nas expressões regulares tradicionais (como $*$ para Kleene ou $ \\cup $ para união).\nO seu uso prático e teórico ocorre principalmente na teoria dos autômatos finitos:\n\nConstrução de Autômatos: Sabemos que as Linguagens Regulares são fechadas sob interseção e complemento. Isso significa que se temos autômatos para $L_1$ e $L_2$, podemos construir autômatos para $L_1 \\cap L_2$ e $ \\overline{L_1} $. As Leis de De Morgan garantem que podemos, por exemplo, construir um autômato para $ \\overline{L_1 \\cup L_2} $ construindo autômatos para $ \\overline{L_1} $ e $ \\overline{L_2} $ e depois aplicando o algoritmo de interseção sobre eles.\nProvas de Equivalência: As leis são usadas para provar que duas descrições de linguagens diferentes são, na verdade, equivalentes, o que é fundamental para a otimização de analisadores léxicos.\n\nEm resumo, embora você não escreva $ \\overline{L_1} \\cap \\overline{L_2} $ diretamente em uma expressão regular comum, as Leis de De Morgan são o fundamento matemático que garante que podemos construir uma máquina (um autômato finito) para reconhecer essa linguagem complexa, validando o poder e a robustez da classe das Linguagens Regulares.\n\n3.3.4.1 Limitações das Expressões Regulares Básicas\nÉ importante observar que, embora as Linguagens Regulares sejam fechadas sob interseção e complemento, essas operações não são diretamente expressáveis usando apenas as três operações fundamentais das expressões regulares (união \\(\\cup\\), concatenação \\(\\cdot\\), e fechamento de Kleene \\(*\\)).\nPara expressar interseções e complementos, frequentemente precisamos:\n\nConstruir autômatos finitos correspondentes às expressões regulares;\nAplicar algoritmos específicos para interseção e complemento de autômatos;\nConverter o resultado de volta para uma expressão regular sempre que possível.\n\nEste é um exemplo de como a teoria matemática subjacente (autômatos finitos) pode ser mais expressiva que a notação conveniente (expressões regulares) para certas operações, embora ambas representem exatamente a mesma classe de linguagens.\n\n\n\n3.3.5 Exercícios 3\n\nSejam \\(L_1 = \\{a, ab, b\\}\\) e \\(L_2 = \\{b, ba, \\epsilon\\}\\). Calcule:\n\n\\(L_1 \\cup L_2\\);\n\\(L_1 \\cap L_2\\);\n\\(L_1 - L_2\\) (diferença);\n\\(|L_1 \\cup L_2|\\) e \\(|L_1 \\cap L_2|\\).\n\nPara as linguagens \\(L_1 = \\{a, bb\\}\\) e \\(L_2 = \\{c, dd\\}\\):\n\nCalcule \\(L_1 \\cdot L_2\\) e \\(L_2 \\cdot L_1\\);\nDetermine \\(|L_1 \\cdot L_2|\\);\nVerifique se \\(L_1 \\cdot L_2 = L_2 \\cdot L_1\\).\n\nSeja \\(L = \\{a, b\\}\\). Determine:\n\n\\(L^0\\), \\(L^1\\), \\(L^2\\);\n\\(|L^n|\\) em função de \\(n\\);\nAs três primeiras strings em ordem lexicográfica de \\(L^3\\).\n\nPara \\(L = \\{ab\\}\\):\n\nListe os elementos de \\(L^*\\) até strings de comprimento 6;\nDetermine \\(L^+\\);\nVerifique se \\(\\epsilon \\in L^*\\) e se \\(\\epsilon \\in L^+\\).\n\nSeja \\(L = \\{a\\}\\). Prove ou refute:\n\n\\(L^* = L^+\\);\n\\(L^* \\cup L^+ = L^*\\);\n\\((L^*)^* = L^*\\);\nSe \\(\\epsilon \\in L\\), então \\(L^+ = L^*\\).",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Alfabetos, Linguagens e Strings: Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "01a-lexico.html#expressões-regulares-uma-notação-concisa",
    "href": "01a-lexico.html#expressões-regulares-uma-notação-concisa",
    "title": "3  Alfabetos, Linguagens e Strings: Fundamentos Matemáticos",
    "section": "3.4 Expressões Regulares: Uma Notação Concisa",
    "text": "3.4 Expressões Regulares: Uma Notação Concisa\nAs expressões regulares fornecem uma notação algébrica concisa e poderosa para descrever linguagens. Esta notação, introduzida por Stephen Kleene, como vimos na Section 2.1.2, permite especificar conjuntos potencialmente infinitos de strings por meio de padrões finitos.\n\n3.4.1 Definição Indutiva\nA definição de uma expressão regular sobre um alfabeto \\(\\Sigma\\) possui uma natureza indutiva, o que significa que ela é construída progressivamente, de elementos simples para estruturas complexas. Primeiramente, definimos as expressões regulares mais elementares, que servem como nossos casos base. Em seguida, estabelecemos as regras de construção, ou casos indutivos, que nos permitem combinar expressões existentes para formar novas:\nCasos base:\n\n\\(\\emptyset\\) é uma expressão regular que denota a linguagem vazia;\n\\(\\epsilon\\) é uma expressão regular que denota a linguagem \\(\\{\\epsilon\\}\\);\nPara cada \\(a \\in \\Sigma\\), \\(a\\) é uma expressão regular que denota a linguagem \\(\\{a\\}\\).\n\nCasos indutivos: Se \\(r\\) e \\(s\\) são expressões regulares, então:\n\n\\((r \\cup s)\\) é uma expressão regular (união);\n\\((r \\cdot s)\\) ou simplesmente \\((rs)\\) é uma expressão regular (concatenação);\n\\((r^*)\\) é uma expressão regular (fechamento de Kleene).\n\n\n\n3.4.2 Precedência dos Operadores\nDe forma análoga à precedência de operadores na aritmética, onde a multiplicação (como em \\(5 \\cdot 2\\)) é executada antes da adição (como em \\(3 + 5\\)), as operações em expressões regulares também seguem uma hierarquia de prioridade. Esta convenção é adotada para simplificar a escrita e reduzir o uso excessivo de parênteses, tornando as expressões mais limpas e legíveis. A ordem de precedência, da maior para a menor, é a seguinte:\n\nFechamento de Kleene (\\(*\\)) - maior precedência\nConcatenação (\\(\\cdot\\)) - precedência média\nUnião (\\(\\cup\\)) - menor precedência\n\nTomemos como exemplo a expressão \\(ab^* \\cup c\\). Para interpretá-la, aplicamos a hierarquia de precedência em etapas:\n\nPrimeiro, o operador de maior prioridade, o Fechamento de Kleene (\\(*\\)), é aplicado ao seu argumento imediato à esquerda, \\(b\\), resultando em (b*).\nEm seguida, a concatenação (que tem precedência sobre a união) liga o a ao resultado anterior, formando (a(b*)).\nPor fim, o operador de menor prioridade, a união (\\(\\cup\\)), combina os termos adjacentes, levando à interpretação final e inequívoca: \\(((a(b^*)) \\cup c)\\).\n\n\n\n3.4.3 Linguagem Denotada por uma Expressão Regular\nAté agora, definimos uma expressão regular \\(r\\) como uma sequência de símbolos, ou seja, uma construção puramente sintática. Agora, precisamos lhe atribuir um significado (semântica), definindo precisamente qual linguagem ela descreve. Para isso, introduzimos a notação \\(L(r)\\) para representar a linguagem denotada por \\(r\\). A definição de \\(L(r)\\) é indutiva, espelhando perfeitamente a estrutura usada para definir a sintaxe de uma expressão regular:\nCasos base:\n\n\\(L(\\emptyset) = \\emptyset\\)\n\\(L(\\epsilon) = \\{\\epsilon\\}\\)\n\\(L(a) = \\{a\\}\\) para \\(a \\in \\Sigma\\)\n\nCasos indutivos:\n\n\\(L(r \\cup s) = L(r) \\cup L(s)\\)\n\\(L(rs) = L(r) \\cdot L(s)\\)\n\\(L(r^*) = (L(r))^*\\)\n\nEste processo de definição nos permite computar a linguagem de qualquer expressão. Por exemplo, para decodificar \\(r = (a \\cup b)c\\), aplicamos as regras indutivamente: partindo dos casos base \\(L(a)=\\{a\\}\\), \\(L(b)=\\{b\\}\\) e \\(L(c)=\\{c\\}\\), usamos a regra da união para obter \\(L(a \\cup b) = \\{a, b\\}\\). Por fim, a regra da concatenação nos dá o resultado final: \\(L(r) = L(a \\cup b) \\cdot L(c) = \\{a, b\\} \\cdot \\{c\\} = \\{ac, bc\\}\\).\n\n3.4.3.1 Exercícios 4\n\nDetermine a linguagem denotada pelas seguintes expressões regulares:\n\n\\(r_1 = a \\cup b\\);\n\\(r_2 = (a \\cup b)(a \\cup b)\\);\n\\(r_3 = a^*b\\);\n\\(r_4 = (ab)^*\\).\n\nReescreva as seguintes expressões com parênteses explícitos, respeitando a precedência:\n\n\\(ab^* \\cup c\\);\n\\(a \\cup bc^*\\);\n\\(ab \\cup cd^*e\\);\n\\(a^*b^* \\cup c^*\\).\n\nPara cada expressão regular, determine se as strings dadas pertencem à linguagem:\n\n\\(r = a^*ba^*\\): strings \\(\\{ab, ba, aba, baa, bb\\}\\);\n\\(r = (a \\cup b)^*b\\): strings \\(\\{b, ab, ba, abb, bbb\\}\\).\n\nUse a definição indutiva de \\(L(r)\\) para calcular \\(L((a \\cup b)c)\\):\n\nIdentifique os casos base aplicáveis;\nAplique as regras indutivas passo a passo;\nApresente o resultado final.\n\nConstrua expressões regulares que denotem as seguintes linguagens:\n\n\\(L_1 = \\{a, b, aa, bb\\}\\);\n\\(L_2 = \\{\\epsilon, a, aa, aaa\\}\\);\n\\(L_3 = \\{w \\in \\{a,b\\}^* \\mid w \\text{ termina com } a\\}\\).\n\n\n\n\n\n3.4.4 Exemplos de Expressões Regulares\nCom os fundamentos teóricos e a semântica de \\(L(r)\\) estabelecidos, a melhor maneira de permitir que a esforçada leitora crie uma intuição sobre o poder expressivo das expressões regulares é por meio da análise de exemplos práticos. Os casos a seguir ilustram como padrões textuais concisos podem descrever com precisão linguagens complexas e, frequentemente, infinitas. Cada exemplo serve para solidificar a conexão entre a notação abstrata e os conjuntos de strings concretos que ela representa.\n\nExemplo 1: strings binárias terminando em \\(01\\) \\[r_1 = (0 \\cup 1)^*01\\] \\[L(r_1) = \\{01, 001, 101, 0001, 0101, 1001, 1101, \\ldots\\}\\]\nAnálise: Esta expressão é dividida em duas partes. A primeira, $(0 \\cup 1)^*$, gera qualquer sequência possível de ’0’s e ’1’s de qualquer comprimento, incluindo a string vazia \\(\\epsilon\\). A segunda parte, 01, é uma string literal. Ao concatenar as duas, forçamos que qualquer string gerada pela primeira parte seja seguida por 01, garantindo assim a terminação desejada.\n\\(L(r_1)\\) contém: \\(\\{01, 101, 001, 11101, \\ldots\\}\\). \\(L(r_1)\\) não contém: \\(\\{\\epsilon, 0, 1, 10, 010, \\ldots\\}\\).\nExemplo 2: strings sobre \\(\\{a, b\\}\\) com número par de \\(a\\)’s \\[r_2 = b^*(ab^*ab^*)^*\\]\nAnálise: esta é uma construção elegante. A parte central é $(ab^*ab^*)$. Observe que dentro dos parênteses existem exatamente dois \\(a\\)’s. Os \\(b^*\\)’s permitem que qualquer número de \\(b\\)’s apareçam antes, entre e depois desses \\(a\\)’s. O Fechamento de Kleene externo, $(...)^*, permite que este bloco contendo dois \\(a\\)’s se repita zero ou mais vezes. Se o bloco se repete \\(k\\) vezes, o número total de \\(a\\)’s será \\(2k\\), que é sempre um número par (0, 2, 4, …). O \\(b^*\\) no início permite que a string comece com \\(b\\)’s ou, caso o bloco não se repita nenhuma vez, gere strings compostas apenas por \\(b\\)’s (que possuem zero \\(a\\)’s, e zero é par).\n\\(L(r_2)\\) contém: \\(\\{b, bb, abab, aab, baab, bbaabb, \\ldots\\}\\). \\(L(r_2)\\) não contém: \\(\\{a, bbbab, aaabb, \\ldots\\}\\).\nExemplo 3: Identificadores que começam com letra \\[r_3 = (a \\cup b \\cup \\ldots \\cup z)(a \\cup b \\cup \\ldots \\cup z \\cup 0 \\cup 1 \\cup \\ldots \\cup 9)^*\\]\nAnálise: Esta expressão regular define uma regra clássica para identificadores (nomes de variáveis, funções, etc.) em muitas linguagens de programação. Ela é composta por duas partes concatenadas que impõem uma estrutura rígida:\n\nA primeira parte, $(a \\cup b \\cup \\ldots \\cup z)$, estabelece a condição para o caractere inicial. Ela exige que a string comece com exatamente uma letra minúscula.\nA segunda parte, $(a \\cup b \\cup \\ldots \\cup z \\cup 0 \\cup 1 \\cup \\ldots \\cup 9)^*$, define os caracteres subsequentes. O Fechamento de Kleene ($*$) permite que o caractere inicial seja seguido por uma sequência de zero ou mais caracteres, que podem ser letras minúsculas ou dígitos.\n\n\\(L(r_3)\\) contém: {x, nome, var1, contador, a1b2c3, ...}. \\(L(r_3)\\) não contém: {1var, _nome, temp-1, $var, ...}.\nExemplo 4: Números inteiros com sinal opcional \\[r_4 = (\\epsilon \\cup + \\cup -)(0 \\cup 1 \\cup \\ldots \\cup 9)(0 \\cup 1 \\cup \\ldots \\cup 9)^*\\]\nAnálise: Esta expressão é construída em três partes lógicas para capturar a estrutura de um número inteiro:\n\n$(\\epsilon \\cup + \\cup -)$: A primeira parte define o prefixo do número. A união com a string vazia ($\\epsilon$) torna o caractere de sinal (+ ou -) opcional. Isso permite que o número comece diretamente com um dígito.\n$(0 \\cup 1 \\cup \\ldots \\cup 9)$: Esta parte garante que, após o sinal (ou a ausência dele), exista pelo menos um dígito. Isso é fundamental para invalidar strings que contenham apenas um sinal, como + ou -.\n$(0 \\cup 1 \\cup \\ldots \\cup 9)^*: O Fechamento de Kleene na parte final permite que este primeiro dígito seja seguido por uma sequência de zero ou mais dígitos adicionais, formando números de qualquer comprimento.\n\n\\(L(r_4)\\) contém: {42, -199, +7, 0, 9, 007, ...}. \\(L(r_4)\\) não contém: {\\epsilon, +, -, --5, 1+1, 9A, ...}.\nNota sobre a limitação: é importante que a atenta leitora note que esta expressão regular, embora funcional, aceita números com zeros à esquerda (como 007), o que pode não ser desejável em todos os contextos de programação ou validação. Expressões mais complexas podem ser criadas para proibir essa característica, por exemplo, tratando o 0 como um caso especial separado de números que começam com $[1-9]$.\n\nEsses exemplos demonstram a notável versatilidade das expressões regulares. Com apenas três operações fundamentais — união, concatenação e Fechamento de Kleene — somos capazes de descrever uma vasta gama de padrões, desde sequências simples e finitas até conjuntos infinitos com regras estruturais complexas. Fica evidente como essa ferramenta se torna indispensável na análise léxica, validação de dados e em inúmeras outras tarefas da computação. Contudo, apesar de seu poder, veremos adiante que existem linguagens, até mesmo algumas com descrições aparentemente simples, que transcendem a capacidade expressiva das expressões regulares.\n\n3.4.4.1 Exercícios 5\n\nBaseando-se no exemplo de strings terminando em \\(01\\):\n\nConstrua uma expressão para strings terminando em \\(10\\);\nConstrua uma expressão para strings começando com \\(01\\);\nConstrua uma expressão para strings que contêm \\(01\\) como substring;\nConstrua uma expressão para strings que não contêm \\(01\\).\n\nInspirando-se no exemplo de número par de \\(a\\)’s:\n\nConstrua uma expressão para strings com número ímpar de \\(a\\)’s sobre \\(\\{a,b\\}\\);\nConstrua uma expressão para strings com número múltiplo de 3 de \\(a\\)’s;\nConstrua uma expressão para strings com pelo menos dois \\(a\\)’s.\n\nBaseando-se no padrão de identificadores:\n\nModifique para permitir underscores em qualquer posição;\nModifique para proibir dígitos na primeira e última posições;\nCrie um padrão para identificadores que devem ter entre 3 e 8 caracteres.\n\nEstendendo o exemplo de números inteiros:\n\nConstrua uma expressão para números decimais (com ponto decimal);\nConstrua uma expressão para números em notação científica simples (\\(1e5\\), \\(2e-3\\));\nConstrua uma expressão para números hexadecimais com prefixo \\(0x\\).\n\nPara as expressões construídas nos exercícios anteriores:\n\nVerifique se as strings \\(\\{101, 1010, 0101\\}\\) pertencem ao padrão termina em 10;\nVerifique se \\(\\{aab, baba, ababa\\}\\) têm número ímpar de \\(a\\)’s;\nTeste se \\(\\{var_1, _temp, item2_\\}\\) são identificadores válidos com underscores.\n\n\n\n\n\n3.4.5 Equivalência de Expressões Regulares e suas Leis Algébricas\nAo explorarmos o poder das expressões regulares, a atenta leitora descobrirá que diferentes expressões podem, na verdade, descrever exatamente a mesma linguagem. De forma análoga à álgebra tradicional, onde \\(x(y+z)\\) e \\(xy + xz\\) são formulações distintas para o mesmo resultado, na teoria das linguagens podemos ter duas expressões sintaticamente diferentes que são semanticamente idênticas. Essa noção de equivalência não é apenas uma curiosidade teórica; ela é fundamental para a otimização e simplificação de padrões, permitindo-nos encontrar a representação mais concisa ou computacionalmente mais eficiente para uma dada linguagem.\nA perspicaz leitora perceberá que a capacidade de manipular expressões e transformá-las em formas equivalentes, porém mais simples, é uma habilidade poderosa, especialmente na construção de analisadores léxicos e ferramentas de processamento de texto.\nFormalmente, duas expressões regulares \\(r\\) e \\(s\\) são ditas equivalentes, o que denotamos por \\(r \\equiv s\\), se, e somente se, elas denotam a mesma linguagem. Matematicamente, isso é expresso como:\n\\[r \\equiv s \\iff L(r) = L(s)\\]\nPara manipular e simplificar expressões regulares, contamos com um conjunto de leis algébricas que governam as operações de união, concatenação e fechamento de Kleene. Estas leis são a base para a otimização de padrões.\n\n3.4.5.1 Principais Leis Algébricas\nA seguir, apresentamos as identidades mais importantes que as expressões regulares satisfazem. Para expressões regulares \\(r\\), \\(s\\) e \\(t\\):\n\n3.4.5.1.1 Leis Associativas e Comutativas\nEstas leis nos permitem reagrupar e reordenar os termos em operações de união e concatenação.\n\nComutatividade da União: a ordem na união não importa. \\[r \\cup s \\equiv s \\cup r\\]\nAssociatividade da União: podemos agrupar uniões de qualquer forma. \\[(r \\cup s) \\cup t \\equiv r \\cup (s \\cup t)\\]\nAssociatividade da Concatenação: o agrupamento na concatenação também é flexível. Lembre-se, no entanto, que a concatenação não é comutativa (\\(rs \\not\\equiv sr\\) em geral). \\[(rs)t \\equiv r(st)\\]\n\n\n\n3.4.5.1.2 Leis de Identidade e Anulação\nEstas leis definem o papel dos elementos especiais \\(\\epsilon\\) (a string vazia) e \\(\\emptyset\\) (a linguagem vazia).\n\nElemento Neutro da Concatenação: A string vazia é o elemento neutro da concatenação. \\[r\\epsilon \\equiv \\epsilon r \\equiv r\\]\nElemento Anulador da Concatenação: Concatenar com a linguagem vazia resulta na linguagem vazia. \\[r\\emptyset \\equiv \\emptyset r \\equiv \\emptyset\\]\nElemento Neutro da União: A linguagem vazia é o elemento neutro da união. \\[r \\cup \\emptyset \\equiv \\emptyset \\cup r \\equiv r\\]\n\n\n\n3.4.5.1.3 Lei Distributiva\nEsta lei conecta as operações de concatenação e união, de forma muito semelhante à álgebra numérica.\n\nDistributividade da Concatenação sobre a União: \\[r(s \\cup t) \\equiv rs \\cup rt\\] \\[(s \\cup t)r \\equiv sr \\cup tr\\]\n\n\n\n3.4.5.1.4 Lei da Idempotência\nEsta lei estabelece que a união de uma expressão com ela mesma não adiciona nada novo.\n\nIdempotência da União: \\[r \\cup r \\equiv r\\]\n\n\n\n3.4.5.1.5 Leis do Fechamento de Kleene\nEstas propriedades definem a natureza do operador de fechamento.\n\nDefinição Recursiva: o fechamento de \\(r\\) é a string vazia ou um \\(r\\) seguido por mais r’s. \\[r^* \\equiv \\epsilon \\cup rr^*\\]\n\n2.Fechamento do Fechamento: aplicar o fechamento duas vezes é redundante.\n$$(r^*)^* \\equiv r^*$$\n\nFechamento da Linguagem Vazia: A única string que pode ser formada por zero ou mais escolhas da linguagem vazia é a string vazia.\n\\[\\emptyset^* \\equiv \\epsilon\\]\nFechamento da string Vazia: o mesmo se aplica à linguagem contendo apenas a string vazia.\n\\[\\epsilon^* \\equiv \\epsilon\\]\n\n\n\n\n3.4.5.2 Exemplo Prático de Simplificação\nPara que a atenta leitora possa ver a utilidade dessas leis em ação, vamos simplificar a expressão regular \\(r = a(b \\cup c) \\cup ab\\). Nosso objetivo é encontrar uma expressão equivalente que seja mais curta.\n\nExpressão Inicial: \\(r = a(b \\cup c) \\cup ab\\)\nAplicar a Distributividade: usamos a lei distributiva à esquerda no termo \\(a(b \\cup c)\\).\n\\[r \\equiv (ab \\cup ac) \\cup ab\\]\nAplicar a Comutatividade: reordenamos os termos da união para agrupar os termos idênticos.\n\\[r \\equiv ab \\cup ab \\cup ac\\]\nAplicar a Idempotência: a união de \\(ab\\) com \\(ab\\) é simplesmente \\(ab\\).\n\\[r \\equiv ab \\cup ac\\]\nChegamos à expressão: \\(s = ab \\cup ac\\). Como \\(L(r) = L(s)\\).\n\nAs expressões são equivalentes (\\(a(b \\cup c) \\cup ab \\equiv ab \\cup ac\\)), mas a segunda é visivelmente mais simples. Essa capacidade de simplificação tem valor prático no projeto de compiladores e em sistemas de busca de texto.\n\n\n3.4.5.3 Exercícios 6\n\nUse as leis algébricas para simplificar:\n\n\\((a \\cup \\emptyset)b\\);\n\\(a(\\epsilon \\cup b)\\);\n\\((a \\cup a)^*\\);\n\\(a \\cup ab^*a\\).\n\nSimplifique a expressão \\(((a \\cup b)a) \\cup (aa)\\) usando as leis passo a passo:\n\nIdentifique que leis podem ser aplicadas;\nMostre cada passo da simplificação;\nVerifique o resultado testando strings específicas.\n\nProve que as seguintes expressões são equivalentes:\n\n\\(a^*a\\) e \\(aa^*\\);\n\\((a \\cup b)^*\\) e \\(\\epsilon \\cup (a \\cup b)(a \\cup b)^*\\);\n\\(a^*b^*\\) e \\((a \\cup b)^*\\) (esta é falsa - encontre um contraexemplo).\n\nSimplifique usando as leis do fechamento:\n\n\\((a^*)^*\\);\n\\(\\epsilon^* \\cup a^*\\);\n\\(\\emptyset^* \\cup a\\);\n\\((a \\cup \\epsilon)^*\\).\n\nPara a expressão \\(ab^* \\cup abb^* \\cup abbb^*\\):\n\nIdentifique o padrão comum;\nUse a distributividade para fatorar;\nSimplifique usando propriedades do fechamento de Kleene;\nVerifique que as linguagens são idênticas.\n\n\n\n\n\n3.4.6 Notações Convencionais Adicionais\nEmbora as três operações fundamentais, união (\\(\\cup\\)), concatenação (\\(\\cdot\\)) e fechamento de Kleene (\\(*\\)), sejam teoricamente suficientes para descrever qualquer Linguagem Regular, na prática, elas podem gerar expressões longas, repetitivas e de difícil leitura. Para contornar essa complexidade, foram introduzidas diversas notações adicionais que funcionam como abreviações ou macros.\nA criativa leitora pode enxergar estas notações como funções ou módulos predefinidos: elas não adicionam um novo poder teórico ao formalismo, mas aumentam a expressividade e a conveniência da escrita, permitindo construir padrões complexos de forma mais limpa e intuitiva. Dominar estas abreviações é um passo importante para escrever expressões regulares eficazes no mundo real.\nA seguir, detalhamos as notações mais comuns.\n\nFechamento Positivo (\\(r^+\\))\n\nDefinição: É uma abreviação para uma ou mais ocorrências de \\(r\\). Formalmente, \\(r^+ \\equiv rr^*\\).\nAnálise: Enquanto \\(r^*\\) corresponde a zero ou mais repetições, \\(r^+\\) exige que o padrão ocorra pelo menos uma vez. É uma das abreviações mais utilizadas.\n\n\nExemplo: Para descrever números inteiros positivos, podemos usar a expressão \\([1-9][0-9]^*\\). Usando o fechamento positivo, a expressão para um ou mais dígitos, \\([0-9]^+\\), pode ser mais intuitiva em certos contextos, embora a primeira seja mais precisa para evitar zeros à esquerda. Um exemplo mais direto é \\(a^+\\), que denota a linguagem \\(\\{a, aa, aaa, \\ldots\\}\\), sendo mais concisa que \\(aa^*\\).\n\nOpcionalidade (\\(r?\\))\nDefinição: Indica que a expressão \\(r\\) é opcional, podendo aparecer uma ou nenhuma vez. É um atalho para \\((\\epsilon \\cup r)\\). Análise: Esta notação é perfeita para partes de um padrão que podem ou não estar presentes.\nExemplo: Para validar URLs que podem ser http ou https (com ‘s’ opcional), usamos a expressão https?. Ela corresponde a http ou https. Outro exemplo seria modelar um número com sinal opcional: \\((+ \\cup -)? [0-9]^+\\).\nClasses de Caracteres (\\([\\ldots]\\))\nDefinição: Funcionam como uma abreviação para uma união de múltiplos caracteres. Por exemplo, \\([abc] \\equiv (a \\cup b \\cup c)\\). Análise: Tornam a expressão muito mais compacta quando precisamos permitir um de vários caracteres possíveis em uma determinada posição.\nExemplo: Para encontrar qualquer vogal minúscula, em vez de escrever \\((a \\cup e \\cup i \\cup o \\cup u)\\), podemos simplesmente usar \\([aeiou]\\).\nIntervalos em Classes de Caracteres (\\([a-z]\\))\nDefinição: Dentro de uma classe de caracteres, o hífen - pode ser usado para denotar um intervalo de símbolos com base em uma ordem convencional (como a da tabela ASCII). Análise: Esta é uma generalização poderosa das classes de caracteres, evitando a necessidade de listar todos os símbolos individualmente.\nExemplo: Para descrever uma letra minúscula qualquer, usamos \\([a-z]\\). Para um dígito hexadecimal, podemos combinar intervalos: \\([0-9a-fA-F]\\).\nNegação de Classes de Caracteres (\\([^\\ldots]\\))\nDefinição: O acento circunflexo ^, quando é o primeiro símbolo dentro de uma classe, nega o conjunto. A classe passa a corresponder a qualquer caractere do alfabeto \\(\\Sigma\\) exceto os que estão listados. Análise: É útil para especificar proibições, ou seja, tudo, exceto um pequeno conjunto de caracteres.\nExemplo: Uma expressão para encontrar uma string que não contenha vogais poderia usar \\([^aeiou]\\). Para encontrar um caractere que não é um dígito, usamos \\([^0-9]\\).\nQuantificadores de Repetição (\\(\\{n, m\\}\\))\nDefinição: Oferecem um controle preciso sobre o número de repetições de uma expressão \\(r\\). Análise: Generalizam as operações \\(?\\), \\(*\\) e \\(+\\), permitindo especificar limites exatos, mínimos ou intervalos de ocorrências.\n\n\\(r\\{n\\}\\): \\(r\\) repetido exatamente \\(n\\) vezes.\n\\(r\\{n,m\\}\\): \\(r\\) repetido no mínimo \\(n\\) e no máximo \\(m\\) vezes.\n\\(r\\{n,\\}\\): \\(r\\) repetido pelo menos \\(n\\) vezes.\n\nExemplos:\n\nCEP Brasileiro: Um CEP no formato XXXXX-XXX pode ser descrito por \\([0-9]\\{5\\}-[0-9]\\{3\\}\\).\nValidade de Senha: Uma regra de senha que exige de \\(8\\) a \\(16\\) caracteres alfanuméricos pode ser modelada por \\([a-zA-Z0-9]\\{8,16\\}\\).\nIdentificador Mínimo: Um nome de variável que precisa ter pelo menos \\(3\\) caracteres, começando com uma letra e seguido por letras ou números, pode ser escrito como \\([a-zA-Z][a-zA-Z0-9]\\{2,\\}\\).\n\n\n\n3.4.6.1 Exercícios 7\n\nReescreva usando apenas união (\\(\\cup\\)), concatenação e fechamento de Kleene (\\(*\\)):\n\n\\(a^+\\);\n\\(b?\\);\n\\([abc]\\);\n\\(a\\{3\\}\\);\n\\(b\\{2,4\\}\\).\n\nConstrua expressões usando classes de caracteres para:\n\nQualquer dígito: \\([0-9]\\);\nQualquer letra minúscula: \\([a-z]\\);\nQualquer caractere que não seja espaço: \\([^ ]\\);\nQualquer caractere alfanumérico: \\([a-zA-Z0-9]\\).\n\nUse quantificadores para construir padrões para:\n\nCEP brasileiro no formato \\(99999-999\\);\nPlaca de carro brasileira antiga \\(AAA-9999\\);\nSenha com exatamente 8 caracteres alfanuméricos;\nCódigo de área de telefone com 2 ou 3 dígitos.\n\nConstrua expressões regulares para validar:\n\nURL simples começando com \\(http\\) ou \\(https\\);\nData no formato \\(dd/mm/aaaa\\) (versão simples);\nHorário no formato \\(hh:mm\\) (24 horas);\nNúmero de CPF no formato \\(999.999.999-99\\).\n\nReescreva as seguintes expressões de forma mais concisa:\n\n\\((a \\cup b \\cup c \\cup d)(a \\cup b \\cup c \\cup d)^*\\);\n\\(a(\\epsilon \\cup b)\\);\n\\((0 \\cup 1 \\cup 2 \\cup 3 \\cup 4 \\cup 5 \\cup 6 \\cup 7 \\cup 8 \\cup 9)(0 \\cup 1 \\cup 2 \\cup 3 \\cup 4 \\cup 5 \\cup 6 \\cup 7 \\cup 8 \\cup 9)^*\\);\n\n\n\n\n\n3.4.7 Aplicações Práticas\nApós a jornada pelos fundamentos matemáticos, a curiosa leitora pode estar se perguntando: onde essa teoria se manifesta no dia a dia da computação? A resposta curta é: em praticamente todos os lugares. As expressões regulares representam um dos casos mais bem-sucedidos de uma teoria matemática que transcendeu a academia e se tornou uma ferramenta indispensável, utilizada diariamente por desenvolvedores de software, administradores de sistemas e cientistas de dados.\nPara a resposta longa precisamos abandonar o formalismo puro para ilustrar como as expressões regulares, com suas notações concisas, resolvem problemas concretos.\n\n\n\n\n\n\nDo Formalismo à Prática: Expressões Regulares no Mundo Real\n\n\n\nA leitora atenta deve notar que as expressões regulares definidas formalmente neste capítulo, baseadas apenas em união, concatenação e Fechamento de Kleene, são o alicerce matemático da teoria. No entanto, as ferramentas que usamos no dia a dia, conhecidas como regex engines, são equipadas com funções que vão além dessa definição.\nEsses engines, como o PCRE (Perl Compatible Regular Expressions), que influencia as implementações em linguagens como Python, PHP e JavaScript, estendem a notação formal com recursos para maior conveniência e poder expressivo. Os mais notáveis são:\n\nLookarounds (lookaheads e lookbehinds): Permitem verificar a existência de um padrão antes (lookbehind) ou depois (lookahead) da posição atual, sem consumir os caracteres verificados (ou seja, sem incluí-los no resultado da captura). Um exemplo clássico é a expressão q(?=u), que encontra a letra q apenas se ela for imediatamente seguida pela letra u, mas sem incluir o u no resultado.\nBackreferences (Retrovisores): Permitem referenciar um grupo que foi previamente capturado dentro da mesma expressão. Por exemplo, a expressão (\\w)\\1 encontra qualquer caractere alfanumérico (\\w) que é seguido imediatamente por ele mesmo. É com esta capacidade que um motor de regex pode reconhecer a linguagem não-regular \\(L = \\{ww \\mid w \\in \\{a,b\\}^*\\}\\).\n\nA consequência mais importante é que esses sistemas de regex modernos podem reconhecer linguagens que não são regulares no sentido estrito da teoria. Enquanto uma expressão regular formal jamais poderia reconhecer a linguagem \\(L = \\{a^n b^n \\mid n \\geq 1\\}\\), as extensões práticas oferecem um poder que transcende os limites dos Autômatos Finitos.\nPortanto, é valioso distinguir entre a classe teórica das Linguagens Regulares e o conjunto de padrões que as ferramentas de software podem processar. O formalismo discutido neste livro é a base para entender o poder, as limitações e a complexidade computacional por trás dessas operações, enquanto as extensões práticas oferecem conveniência e expressividade para resolver problemas do mundo real.\n\n\n\n3.4.7.1 Análise Léxica em Compiladores\nA análise léxica é a primeira fase da compilação de um programa. O analisador, chamado de scanner ou lexer, lê o código-fonte como uma sequência de caracteres e a converte em uma sequência de tokens, unidades lexicais como identificadores, palavras-chave, números e operadores. Cada tipo de token é definido precisamente por um padrão, que é, em sua essência, uma expressão regular.\nExemplo 1: identificadores: nomes de variáveis, funções, etc..\nPadrão: [a-zA-Z_][a-zA-Z0-9_]*. Análise: Este padrão decreta que um identificador deve começar com uma letra (maiúscula ou minúscula) ou um underscore ([a-zA-Z_]), seguido por zero ou mais caracteres que podem ser letras, números ou underscores ([a-zA-Z0-9_]*).\nExemplo 2: números inteiros:\nPadrão: -?[1-9][0-9]*|0. Análise: Este padrão elegante lida com vários casos. O -? torna o sinal de negativo opcional. O trecho [1-9][0-9]* garante que números com múltiplos dígitos não comecem com zero (como 042). Por fim, o |0 trata o número zero como um caso especial.\n\n\n3.4.7.2 Validação de Dados de Entrada\nEm qualquer aplicação que receba dados de um usuário, é vital garantir que esses dados estejam no formato correto antes de serem processados. As expressões regulares são a ferramenta padrão para essa tarefa de validação.\nExemplo 1: endereço de e-mail (Muito Simplificado):\nPadrão: [a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,} Análise: Embora um padrão de e-mail 100% compatível com as RFCs seja extremamente complexo, esta versão simplificada cobre a maioria dos casos. Ela busca: uma sequência de caracteres de usuário ([...]+), o símbolo @, uma sequência de caracteres de domínio ([...]+), um ponto literal (\\.) e, por fim, o domínio de topo (TLD) com pelo menos duas letras ([a-zA-Z]{2,}).\n\n\n3.4.7.3 Telefone (Formato Brasileiro)\nPadrão: \\([0-9]{2}\\) ?[0-9]{4,5}-[0-9]{4} Análise: Este padrão modela um número de telefone comum no Brasil, incluindo o DDD entre parênteses e um espaço opcional e pode ser dividido nas seguintes partes:\n\n\\( e \\): Correspondem aos parênteses literais. Como ( e ) são metacaracteres em expressões regulares, eles precisam ser escapados com uma barra invertida para serem tratados como caracteres literais;\n[0-9]{2}: Exige exatamente dois dígitos numéricos para o código de área (DDD);\n?: Permite a existência opcional de um único caractere de espaço após os parênteses do DDD;\n[0-9]{4,5}: Captura a primeira parte do número, que pode ter 4 ou 5 dígitos, acomodando tanto números fixos quanto números móveis que já adotaram o nono dígito;\n-[0-9]{4}: Corresponde ao hífen literal e aos 4 dígitos finais do número.\n\n\n\n3.4.7.4 Busca, Extração e Substituição de Texto\nEsta é talvez a aplicação mais visível das expressões regulares, formando o coração de ferramentas como grep e sed em sistemas Unix, e as funcionalidades de Localizar e Substituir em editores de texto e ambientes integrados de edição (IDEs).\nExemplo 1: busca por Palavras Exatas:\nPadrão: \\b[Pp]alavra\\b Análise: O \\b é uma âncora que corresponde a uma fronteira de palavra (word boundary). Isso garante que a busca encontre palavra como uma palavra inteira, e não como parte de subpalavra. O [Pp] torna a busca insensível a maiúsculas para a primeira letra.\n\n\n3.4.7.5 Hora (Formato 24h: HH:MM ou HH:MM:SS)\nPadrão: ([01][0-9]|2[0-3]):[0-5][0-9](:[0-5][0-9])? Análise: Este é um excelente exemplo de como expressões regulares podem validar formatos com regras numéricas. Ele valida horas no formato 24h, com segundos opcionais e pode ser dividido em partes:\n\n([01][0-9]|2[0-3]): Esta parte valida as horas. A união | cria duas possibilidades: ou um dígito de 0 ou 1 seguido por qualquer dígito (00-19), ou o dígito 2 seguido por um dígito de 0 a 3 (20-23).\n:: Corresponde ao separador literal.\n[0-5][0-9]: Valida os minutos, garantindo que estejam no intervalo de 00 a 59.\n(:[0-5][0-9])?: Esta parte torna os segundos opcionais. Os parênteses agrupam o separador : e os dígitos dos segundos. O quantificador ? aplicado a este grupo faz com que toda a seção de segundos (:SS) possa aparecer uma ou nenhuma vez.\n\n\\(L(r)\\) contém: {14:30, 09:15, 23:59:59, 00:00}. \\(L(r)\\) não contém: {25:00, 12:61, 9:30, 14:30:, 15:10:99}.\nEsses exemplos arranham apenas a superfície, mas demonstram a imensa versatilidade das expressões regulares. Elas formam uma ponte poderosa entre a teoria formal das linguagens e a resolução de problemas práticos e onipresentes no desenvolvimento de software.\n\n\n\n3.4.8 Exercícios 8\n\nProjete expressões regulares para tokens de uma linguagem de programação simples:\n\nPalavras-chave: \\(\\{\\text{if}, \\text{then}, \\text{else}, \\text{while}, \\text{do}\\}\\);\nNúmeros inteiros (incluindo negativos);\nComentários de linha iniciados por \\(//\\);\nOperadores relacionais: \\(\\{&lt;, &gt;, &lt;=, &gt;=, ==, !=\\}\\).\n\nConstrua expressões regulares para validar:\n\nTelefone celular: \\((11) 99999-9999\\);\nRG: \\(99.999.999-9\\);\nCNPJ: \\(99.999.999/9999-99\\);\nCEP: \\(99999-999\\) ou \\(99.999-999\\).\n\nProjete expressões para encontrar:\n\nEndereços de email em um texto;\nValores monetários no formato \\(R\\$ 99,99\\);\nDatas em formatos variados: \\(dd/mm/aaaa\\), \\(dd-mm-aaaa\\), \\(dd.mm.aaaa\\);\nNúmeros de cartão de crédito (formato \\(9999-9999-9999-9999\\)).\n\nIdentifique e corrija os erros nas seguintes expressões:\n\nPara validar email: \\([a-z]+@[a-z]+.[a-z]+\\) (problema: ponto literal);\nPara números decimais: \\([0-9]*.[0-9]*\\) (problema: pontos opcionais);\nPara identificadores: \\([a-zA-Z][a-zA-Z0-9]?\\) (problema: comprimento mínimo).\n\nPara cada expressão, proponha uma versão otimizada:\n\n\\((abc|abd|abe)\\) → \\(ab(c|d|e)\\);\n\\([0-9][0-9][0-9][0-9]\\) → \\([0-9]\\{4\\}\\);\n\\((a^*b^*|b^*a^*)\\) → \\((a|b)^*\\) (verifique se são realmente equivalentes);\nAnalise qual versão seria mais eficiente em uma implementação real.\n\nDefina o alfabeto, e as expressões regulares que sejam capazes de validar expressões aritméticas entre números inteiros considerando apenas as operações de soma, subtração, divisão e multiplicação.\n\n\n\n3.4.9 Limitações das Expressões Regulares\n\nParênteses aninhados arbitrariamente não são reconhecíveis\nRequer gramáticas livres de contexto para estruturas recursivas\nExemplo problemático: ((3 + 5) * (7 - 2))",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Alfabetos, Linguagens e Strings: Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "01a-lexico.html#o-lugar-das-linguagens-regulares-poder-e-limitações",
    "href": "01a-lexico.html#o-lugar-das-linguagens-regulares-poder-e-limitações",
    "title": "3  Alfabetos, Linguagens e Strings: Fundamentos Matemáticos",
    "section": "3.5 O Lugar das Linguagens Regulares: Poder e Limitações",
    "text": "3.5 O Lugar das Linguagens Regulares: Poder e Limitações\nApós testemunharmos a versatilidade das expressões regulares em aplicações práticas, uma questão natural emerge: será que elas são capazes de descrever qualquer padrão ou linguagem que possamos imaginar? A resposta, embora surpreendente para alguns, é não. As expressões regulares ocupam um lugar específico e bem definido no universo das linguagens formais.\nPara compreender essa especificidade, recorremos à Hierarquia de Chomsky, um sistema de classificação proposto pelo linguista Noam Chomsky que organiza as linguagens formais em níveis de complexidade crescente. Neste framework, as expressões regulares definem o primeiro e mais fundamental degrau: a classe das Linguagens Regulares.\nA aventureira leitora depois de entender as expressões regulares pode explorar as propriedades que definem o poder desta classe de linguagens e, igualmente importante, as fronteiras que revelam suas limitações.\n\n3.5.1 A Essência do Poder: Simplicidade e Previsibilidade\nO poder das Linguagens Regulares reside em sua simplicidade e previsibilidade. Esse poder é o resultado de um conjunto de propriedades matemáticas e uma correspondência fundamental com um tipo específico de máquina teórica: o Autômato Finito. Vamos explorar essas propriedades que definem o que torna as Linguagens Regulares tão úteis e amplamente aplicáveis.\nA curiosa leitora deve saber que existe uma associação clássica entre as Linguagens Regulares e os Autômatos Finitos. De fato, a propriedade mais importante de uma Linguagem Regular é que ela pode ser reconhecida por um Autômato Finito. Um Autômato Finito, como vimos antes no Capítulo Chapter 2, é uma máquina teórica com uma quantidade finita de memória, representada por seus estados. A máquina lê uma string de entrada, um símbolo por vez, e sem poder voltar atrás, decide se a string pertence ou não à linguagem. A incapacidade de armazenar uma quantidade ilimitada de informações é a característica que define, limita e, ao mesmo tempo, determina o poder das Linguagens Regulares. Veremos Autômatos Finitos mais detalhadamente no Capítulo Chapter 4.\n\n\n3.5.2 Propriedades Fundamentais das Linguagens Regulares\nA utilidade prática das linguagens regulares não se limita à sua capacidade de descrever padrões, mas também se baseia em um conjunto de garantias matemáticas. Duas propriedades centrais neste contexto são:\n\nFechamento sob Operações: a classe das Linguagens Regulares é fechada sob todas as operações que vimos: união, concatenação, fechamento de Kleene, e também sob interseção e complemento. Isso significa que se combinarmos Linguagens Regulares usando essas operações, o resultado será sempre outra Linguagem Regular. Essa propriedade é de grande valor prático, ela garante que podemos construir sistemas de reconhecimento complexos a partir de componentes simples com a certeza de que o todo permanecerá computacionalmente tratável.\nDecidibilidade: Para as Linguagens Regulares, questões fundamentais são sempre decidíveis, ou seja, existe um algoritmo que pode respondê-las em um tempo finito. Podemos sempre construir um programa para determinar, por exemplo:\n\nSe uma string \\(w\\) pertence à linguagem \\(L(r)\\);\nSe a linguagem \\(L(r)\\) é vazia;\nSe duas expressões, \\(r\\) e \\(s\\), são equivalentes (\\(L(r) = L(s)\\)).\n\n\nA previsibilidade e a eficiência dos algoritmos associados às Linguagens Regulares são características que as destacam em comparação com outras classes de linguagens mais complexas, como as Linguagens Livres de Contexto ou as Linguagens Sensíveis ao Contexto.\n\n\n3.5.3 As Fronteiras do Mundo Regular: O Limite da Memória\nA principal limitação das Linguagens Regulares está diretamente ligada à memória finita dos autômatos. Tarefas que exigem contar ou lembrar uma quantidade arbitrária e ilimitada de informações estão além de seu alcance. Para reconhecer linguagens que envolvem contagem ou correspondência simétrica, a máquina precisaria de uma memória que pudesse crescer conforme o tamanho da entrada. Como um autômato finito tem um número fixo de estados, ele não consegue realizar essa tarefa.\nExemplos Clássicos de Linguagens Não-Regulares:\n\n\\(L = \\{a^n b^n \\mid n \\geq 0\\}\\): Para verificar se uma string pertence a esta linguagem (por exemplo, aaabbb), uma máquina precisaria contar todos os \\(a\\)’s e depois garantir que o número de \\(b\\)’s é exatamente o mesmo. Como \\(n\\) pode ser qualquer número, a contagem exige uma memória potencialmente infinita.\nPalíndromos (\\(L = \\{w \\mid w = w^R\\}\\)): Para reconhecer um palíndromo como abccba, a máquina teria que memorizar a primeira metade da string (abc) para compará-la com a segunda metade (cba). Novamente, o comprimento da string é ilimitado, exigindo memória ilimitada.\nParênteses Balanceados: A linguagem das expressões com parênteses corretamente aninhados, como ((())), exige uma estrutura de pilha para lembrar quantos parênteses foram abertos e ainda não foram fechados. Esta é uma forma de memória mais poderosa do que a disponível para um autômato finito.\n\n\n\n3.5.4 O Lema do Bombeamento: Uma Ferramenta de Prova\nPara provar formalmente que uma linguagem não é regular, utilizamos uma ferramenta poderosa chamada Lema do Bombeamento (Pumping Lemma). Em vez de ser uma fórmula, é melhor compreendido como uma propriedade que toda Linguagem Regular deve satisfazer. Se conseguirmos mostrar que uma linguagem viola essa propriedade, então provamos que ela não pode ser regular.\nA sua intuição é a seguinte:\nSe uma linguagem \\(L\\) é regular, então ela é aceita por um autômato finito com um número fixo de estados, digamos, \\(p\\). Agora, considere uma string \\(w\\) em \\(L\\) que seja suficientemente longa (especificamente, com comprimento \\(|w| \\geq p\\)). Como a string tem mais símbolos do que o autômato tem estados, o Princípio da Casa dos Pombos nos garante que o autômato necessariamente revisitará pelo menos um de seus estados enquanto lê a string. Isso significa que o caminho do autômato ao ler \\(w\\) contém um ciclo. Podemos, então, dividir a string \\(w\\) em três partes, \\(w = xyz\\):\n\n$x$: a parte da string lida antes do ciclo começar;\n$y$: a parte da string lida durante o ciclo. É uma condição fundamental do lema que esta parte não pode ser vazia (\\(|y| &gt; 0\\));\n$z$: o restante da string, lida após o ciclo.\n\nO lema afirma que, como \\(y\\) corresponde a um ciclo, essa porção pode ser bombeada: podemos percorrê-la zero vezes (removendo \\(y\\)), uma vez (a string original), ou múltiplas vezes. Todas as strings resultantes, da forma \\(xy^iz\\) para qualquer \\(i \\geq 0\\), ainda devem ser aceitas pela máquina e, portanto, devem pertencer à linguagem \\(L\\).\nPara provar que uma linguagem não é regular, usamos esta propriedade para criar uma contradição: encontramos pelo menos uma string longa na linguagem onde o bombeamento de sua parte cíclica a quebra, gerando uma string que não pertence à linguagem. Se isso for possível, a linguagem viola o lema e, portanto, não é regular.\n\n\n3.5.5 Exemplo do Lema do Bombeamento\nVamos usar o Lema do Bombeamento para provar formalmente que a linguagem \\(L = \\{a^n b^n \\mid n \\geq 0\\}\\), uma sequência de \\(a\\)’s seguida pelo mesmo número de \\(b\\)’s, não é regular. A prova segue um roteiro de contradição cujo passo a passo a atenta leitora pode ver a seguir:\n\nAssunção Inicial (para Contradição): assumimos que \\(L\\) é uma linguagem regular.\nAplicação do Lema: se \\(L\\) é regular, então o Lema do Bombeamento deve se aplicar. Isso significa que existe uma constante, o comprimento de bombeamento \\(p\\), para a linguagem \\(L\\).\nEscolha da String: escolhemos uma string \\(w\\) que pertence a \\(L\\) e que seja longa o suficiente, ou seja, \\(|w| \\geq p\\). A escolha mais estratégica é \\(w = a^p b^p\\). Esta string claramente pertence a \\(L\\) (o número de \\(a\\)’s é igual ao de \\(b\\)’s) e seu comprimento é \\(2p\\), que é maior ou igual a \\(p\\).\nDivisão da String: de acordo com o lema, \\(w\\) pode ser dividida em três partes, \\(w=xyz\\), que devem satisfazer as seguintes condições:\n\n\\(|y| &gt; 0\\);\n\\(|xy| \\leq p\\);\n\\(xy^iz \\in L\\) para todo \\(i \\geq 0\\).\n\nAnálise da Parte Bombeável (\\(y\\)): a condição \\(|xy| \\leq p\\) é a chave. Como nossa string é \\(w = \\underbrace{a a \\ldots a}_{p \\text{ vezes}} \\underbrace{b b \\ldots b}_{p \\text{ vezes}}\\), a parte \\(xy\\) (com no máximo \\(p\\) caracteres) deve estar inteiramente contida no bloco inicial de \\(a\\)’s. Além disso, como \\(|y| &gt; 0\\), a string \\(y\\) deve conter pelo menos um \\(a\\). Portanto, \\(y\\) é da forma \\(y=a^k\\) para algum \\(k\\) onde \\(1 \\leq k \\leq p\\).\nO Bombeamento (A Contradição): o lema afirma que qualquer string bombeada \\(xy^iz\\) também deve pertencer a \\(L\\). Vamos testar para \\(i=2\\):\n\nA nova string é \\(w' = xy^2z\\).\nComo \\(x\\), \\(y\\) e \\(z\\) juntos formavam \\(a^p b^p\\), e \\(y\\) era \\(a^k\\), a nova string \\(w'\\) terá \\(k\\) \\(a\\)’s a mais que a original.\nPortanto, \\(w' = a^{p+k} b^p\\).\nComo sabemos que \\(k \\geq 1\\), o número de \\(a\\)’s (\\(p+k\\)) não é mais igual ao número de \\(b\\)’s (\\(p\\)). Logo, a string \\(w' = a^{p+k} b^p\\) não pertence a \\(L\\).\n\nConclusão: chegamos a uma contradição. Nossa suposição inicial de que \\(L\\) era regular nos levou, por meio do Lema do Bombeamento, à conclusão de que uma string gerada pelo bombeamento (\\(w'\\)) deveria estar em \\(L\\), mas mostramos que ela viola a definição de \\(L\\). Portanto, a suposição inicial estava errada. A linguagem \\(L = \\{a^n b^n \\mid n \\geq 0\\}\\) não é regular.\n\n\n3.5.5.1 Por que o Lema do Bombeamento é Importante?\nO Lema do Bombeamento é uma ferramenta diagnóstica fundamental na teoria da computação. Sua importância não está em descrever o que as linguagens regulares são, mas sim em fornecer um método rigoroso para provar o que elas não são. [cite_start]Ele formaliza a noção de que autômatos finitos possuem memória finita. A existência de um ciclo bombeável é a consequência direta dessa memória finita; qualquer tarefa que exija contagem ou memória ilimitada, como garantir que o número de \\(a\\)’s é igual ao de \\(b\\)’s, irá quebrar sob o bombeamento. Na prática, isso permite aos cientistas da computação e engenheiros de software classificar problemas, entendendo quando uma ferramenta simples (como uma expressão regular) é insuficiente e quando uma abordagem mais poderosa (como um analisador sintático para gramáticas livres de contexto) é necessária.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Alfabetos, Linguagens e Strings: Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "01a-lexico.html#exercícios-desafiadores",
    "href": "01a-lexico.html#exercícios-desafiadores",
    "title": "3  Alfabetos, Linguagens e Strings: Fundamentos Matemáticos",
    "section": "3.6 Exercícios Desafiadores",
    "text": "3.6 Exercícios Desafiadores\n\n3.6.1 Exercício 1:\nSeja \\(\\Sigma = \\{a, b, c\\}\\) e considere a linguagem \\(L = \\{w \\in \\Sigma^* \\mid |w|_a + 2|w|_b = |w|_c\\}\\), onde \\(|w|_x\\) denota o número de ocorrências do símbolo \\(x\\) na string \\(w\\).\n\nDetermine se \\(L\\) é uma linguagem regular. Justifique sua resposta.\nSe \\(L\\) não for regular, use o Lema do Bombeamento para prová-lo formalmente.\nConstrua uma expressão regular para a linguagem \\(L' = \\{w \\in \\{a,b\\}^* \\mid |w|_a \\leq 3\\}\\).\n\nSolução:\n\nA linguagem \\(L\\) não é regular. A condição \\(|w|_a + 2|w|_b = |w|_c\\) estabelece uma relação aritmética que requer contagem precisa dos símbolos, algo que excede a capacidade de memória finita dos autômatos finitos.\nProva usando o Lema do Bombeamento:\n\nAssumimos por contradição que \\(L\\) é regular. Então existe uma constante \\(p\\) tal que qualquer string \\(w \\in L\\) com \\(|w| \\geq p\\) pode ser dividida como \\(w = xyz\\) satisfazendo: a. \\(|y| &gt; 0\\); b. \\(|xy| \\leq p\\); c. \\(xy^iz \\in L\\) para todo \\(i \\geq 0\\).\nEscolhemos \\(w = a^p c^p \\in L\\) (neste caso, \\(p + 2 \\cdot 0 = p\\)). Como \\(|xy| \\leq p\\) e \\(w\\) começa com \\(p\\) símbolos \\(a\\), temos que \\(y = a^k\\) para algum \\(1 \\leq k \\leq p\\).\nPara \\(i = 2\\): \\(w' = xy^2z = a^{p+k}c^p\\)\nNa string \\(w'\\): \\(|w'|_a = p + k\\), \\(|w'|_b = 0\\), \\(|w'|_c = p\\)\nA condição requer: \\((p + k) + 2 \\cdot 0 = p\\), ou seja, \\(p + k = p\\), logo \\(k = 0\\).\nMas isso contradiz \\(|y| &gt; 0\\), que implica \\(k \\geq 1\\).\nPortanto, \\(L\\) não é regular.\n\nPara \\(L' = \\{w \\in \\{a,b\\}^* \\mid |w|_a \\leq 3\\}\\):\n\n\\[L' = b^* \\cup b^*ab^* \\cup b^*ab^*ab^* \\cup b^*ab^*ab^*ab^*\\]\nForma mais concisa: \\(L' = b^*(\\epsilon \\cup ab^* \\cup ab^*ab^* \\cup ab^*ab^*ab^*)\\)",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Alfabetos, Linguagens e Strings: Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "01a-lexico.html#exercício-2",
    "href": "01a-lexico.html#exercício-2",
    "title": "3  Alfabetos, Linguagens e Strings: Fundamentos Matemáticos",
    "section": "3.7 Exercício 2:",
    "text": "3.7 Exercício 2:\nConsidere as expressões regulares sobre \\(\\Sigma = \\{0, 1\\}\\):\n\n\\(r_1 = (01)^*0(10)^*\\)\n\\(r_2 = 0(10)^* \\cup (01)^*0\\)\n\n\nDetermine se \\(r_1 \\equiv r_2\\) construindo strings específicas que testem a equivalência.\nConstrua uma expressão regular equivalente mais simples.\nDetermine \\(|L(r_1) \\cap \\{w \\in \\{0,1\\}^* \\mid |w| = 5\\}|\\).\n\nSolução Letra a:\n\nTeste de equivalência por exemplos:\n\\(L(r_1) = L((01)^*0(10)^*)\\) gera strings que:\n\nComeçam com zero ou mais pares \\(01\\);\nTêm um \\(0\\) central obrigatório;\nTerminam com zero ou mais pares \\(10\\).\n\n\\(L(r_2) = L(0(10)^* \\cup (01)^*0)\\) gera strings que:\n\nOu começam com \\(0\\) seguido de pares \\(10\\);\nOu começam com pares \\(01\\) e terminam com \\(0\\).\n\nTeste com strings específicas:\n\nstring \\(010\\): Em \\(r_1\\): \\((01)^1 \\cdot 0 \\cdot (10)^0 = 010\\). Ok!\nstring \\(010\\): Em \\(r_2\\): \\((01)^1 \\cdot 0 = 010\\). Ok!\nstring \\(0\\): Em ambas. Ok!\nstring \\(01010\\): Em \\(r_1\\): \\((01)^1 \\cdot 0 \\cdot (10)^1 = 01010\\). Ok!\nstring \\(01010\\): Em \\(r_2\\): \\((01)^2 \\cdot 0 = 01010\\). Ok!\n\nPor análise estrutural, \\(r_1 \\equiv r_2\\).\n\nSolução Letra b:\nA expressão \\(r_2 = 0(10)^* \\cup (01)^*0\\) é, de fato, a forma simplificada de \\(r_1\\). A afirmação pode ser mais detalhada da seguinte forma:\n\nSimplificação Estrutural:\n\nA expressão original, \\(r_1 = (01)^*0(10)^*\\), descreve a linguagem por meio de uma concatenação de três partes. Isso exige que qualquer string da linguagem seja mentalmente dividida em um prefixo de \\((01)^*\\), um \\(0\\) central e um sufixo de \\((10)^*\\).\nA expressão \\(r_2\\), já comprovada como equivalente na letra (a), descreve a mesma linguagem como a união de dois padrões mais diretos. Essa forma é frequentemente considerada mais simples por representar a linguagem como strings que pertencem ao padrão A OU ao padrão B, o que pode ser mais fácil de analisar e implementar.\n\nAnálise de Minimalidade:\n\nA expressão \\(r_2\\) é considerada minimal na prática. A linguagem consiste em strings que começam e terminam com \\(0\\) e têm \\(0\\)s e \\(1\\)s alternados no interior. Existem duas maneiras naturais de gerar tais strings:\n\nComeçando com um \\(0\\) e adicionando pares de \\(10\\) à direita (padrão \\(0(10)^*\\)).\nTerminando com um \\(0\\) e adicionando pares de \\(01\\) à esquerda (padrão \\((01)^*0\\)).\n\nA união em \\(r_2\\) captura perfeitamente essas duas perspectivas geradoras. Tentar unir os dois padrões em uma única expressão sem o operador de união (\\(\\cup\\)) muito provavelmente resultaria em uma expressão mais longa, complexa e menos intuitiva.\n\nConclusão:pPortanto, embora a prova formal de minimalidade de uma expressão regular seja um problema complexo (geralmente envolvendo autômatos finitos), a expressão \\(r_2 = 0(10)^* \\cup (01)^*0\\) é a representação equivalente mais simples e canônica de \\(r_1\\).\n\nSolução Letra c:\nContagem para \\(|w| = 5\\):\nStrings de \\(L(r_1)\\) têm comprimento ímpar (todas começam e terminam com \\(0\\), com pares no meio).\nPara comprimento 5: a. Caso \\(0(10)^*\\): \\(0\\) seguido de \\((10)^k\\) onde \\(1 + 2k = 5\\), logo \\(k = 2\\): \\(01010\\). Ok! b. Caso \\((01)^*0\\): \\((01)^k\\) seguido de \\(0\\) onde \\(2k + 1 = 5\\), logo \\(k = 2\\): \\(01010\\). Ok!\nPortanto: \\(|L(r_1) \\cap \\{w \\in \\{0,1\\}^* \\mid |w| = 5\\}| = 1\\)",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Alfabetos, Linguagens e Strings: Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "01a-lexico.html#exercício-3",
    "href": "01a-lexico.html#exercício-3",
    "title": "3  Alfabetos, Linguagens e Strings: Fundamentos Matemáticos",
    "section": "3.8 Exercício 3:",
    "text": "3.8 Exercício 3:\nSeja \\(\\Sigma\\) o alfabeto contendo dígitos de 0 a 9 e os símbolos ‘(’, ‘)’, ‘-’, e o espaço ’ ’. Defina as seguintes expressões para conjuntos de dígitos: 1. \\(D = (0 \\cup 1 \\cup 2 \\cup 3 \\cup 4 \\cup 5 \\cup 6 \\cup 7 \\cup 8 \\cup 9)\\); 2. \\(D_{nz} = (1 \\cup 2 \\cup 3 \\cup 4 \\cup 5 \\cup 6 \\cup 7 \\cup 8 \\cup 9)\\); 3. \\(D_{fixo} = (2 \\cup 3 \\cup 4 \\cup 5 \\cup 6 \\cup 7 \\cup 8)\\).\nCom base nessas definições, considere a linguagem \\(L_{tel}\\) de números de telefone brasileiros.\n\nConstrua expressões regulares separadas para os componentes da linguagem: o DDD e o Número do telefone.\nCombine as expressões da letra (a) para construir a expressão regular final para a linguagem \\(L_{tel}\\) completa.\nDemonstre que a string \\((11) 99999-9999\\) pertence a \\(L_{tel}\\) mostrando sua derivação a partir da expressão final.\n\nSolução Letra a:\n\nComponente DDD (\\(r_{ddd}\\)):\n\nO DDD consiste em dois dígitos, o primeiro não podendo ser zero. A base é \\(D_{nz}D\\);\nEle pode ter dois formatos: com parênteses ou sem, ambos seguidos por um espaço opcional;\nExpressão para o formato com parênteses: \\(\\( D_{nz} D \\) (\\epsilon \\cup \\text{ })\\);\nExpressão para o formato sem parênteses: \\(D_{nz} D (\\epsilon \\cup \\text{ })\\);\nA expressão completa para o DDD é a união dos dois formatos:\n\n\\[r_{ddd} = (\\( D_{nz} D \\) (\\epsilon \\cup \\text{ })) \\cup (D_{nz} D (\\epsilon \\cup \\text{ }))\\]\nComponente Número (\\(r_{num}\\)):\n\nO número pode ser de celular (9 dígitos começando com 9) ou fixo (8 dígitos começando com 2-8). Ambos podem ter um hífen opcional;\nExpressão para celular: \\(9 D^4 (\\epsilon \\cup -) D^4\\);\nExpressão para fixo: \\(D_{fixo} D^3 (\\epsilon \\cup -) D^4\\);\nA expressão completa para o número é a união dos dois tipos.\n\n\\[r_{num} = (9 D^4 (\\epsilon \\cup -) D^4) \\cup (D_{fixo} D^3 (\\epsilon \\cup -) D^4)\\]\n\nSolução Letra b:\nA expressão regular final \\(r_{tel}\\) é a concatenação do componente DDD com o componente Número.\n\nExpressão Final: \\[r_{tel} = r_{ddd} \\cdot r_{num}\\]\nSubstituindo as definições da letra (a):\n\\[r_{tel} = [(\\( D_{nz} D \\) (\\epsilon \\cup \\text{ })) \\cup (D_{nz} D (\\epsilon \\cup \\text{ }))] \\cdot [(9 D^4 (\\epsilon \\cup -) D^4) \\cup (D_{fixo} D^3 (\\epsilon \\cup -) D^4)]\\]\n\nSolução Letra c:\nVamos demonstrar que a string \\(w = (11) 99999-9999\\) pertence a \\(L(r_{tel})\\).\n\nDivisão da String: Podemos dividir \\(w\\) em duas partes, \\(w_{ddd}\\) e \\(w_{num}\\), onde:\n\n\\(w_{ddd} = (11)\\text{ }\\) (inclui o espaço);\n\\(w_{num} = 99999-9999\\);\n\nVerificação do Componente DDD: A substring \\(w_{ddd}\\) deve pertencer a \\(L(r_{ddd})\\). Ela corresponde ao primeiro termo da união em \\(r_{ddd}\\):\n\n\\(\\( D_{nz} D \\) (\\epsilon \\cup \\text{ })\\);\n\\(\\(\\) corresponde ao símbolo literal \\((\\);\n\\(1\\) pertence a \\(L(D_{nz})\\) neste caso, \\(1 \\in \\{1,2,3,4,5,6,7,8,9\\}\\);\n\\(1\\) pertence a \\(L(D)\\) neste caso, \\(1 \\in \\{0,1,2,3,4,5,6,7,8,9\\}\\);\n\\(\\)\\) corresponde ao símbolo literal \\()\\);\nO espaço pertence a \\(L(\\epsilon \\cup \\text{ })\\);\nPortanto, \\(w_{ddd} \\in L(r_{ddd})\\). Ok!\n\nVerificação do Componente Número: A substring \\(w_{num}\\) deve pertencer a \\(L(r_{num})\\). Ela corresponde ao primeiro termo da união (celular) em \\(r_{num}\\):\n\n\\(9 D^4 (\\epsilon \\cup -) D^4\\);\nO primeiro \\(9\\) corresponde ao símbolo literal \\(9\\);\n\\(9999\\) pertence a \\(L(D^4) = L(D \\cdot D \\cdot D \\cdot D)\\);\n\\(-\\) pertence a \\(L(\\epsilon \\cup -)\\);\n\\(9999\\) pertence a \\(L(D^4)\\);\nPortanto, \\(w_{num} \\in L(r_{num})\\). Ok!\n\nConclusão: como \\(w = w_{ddd} \\cdot w_{num}\\) e ambas as partes são geradas pelas respectivas subexpressões de \\(r_{tel}\\), pela definição de concatenação de linguagens, temos que \\(w \\in L(r_{tel})\\).\n\n\n3.8.1 Exercício 4:\nSeja \\(\\Sigma = \\{a, b\\}\\) e considere as seguintes linguagens: - \\(L_1 = \\{a^nb^m \\mid n \\geq m \\geq 0\\}\\) - \\(L_2 = \\{a^mb^n \\mid n \\geq m \\geq 0\\}\\)\n\nAnalise se \\(L_1\\) e \\(L_2\\) são linguagens regulares usando o Lema do Bombeamento.\nDetermine as linguagens resultantes das operações \\(L_1 \\cap L_2\\), \\(L_1 \\cup L_2\\), e \\(L_1 \\cdot L_2\\), e classifique cada uma quanto à regularidade.\nConstrua uma expressão regular para \\(L_3 = \\{a^nb^n \\mid n \\geq 0\\} \\cup \\{a^mb^m \\mid m \\geq 0\\}\\) ou prove que não existe.\n\nSolução Letra a:\n\nAnálise de \\(L_1 = \\{a^nb^m \\mid n \\geq m \\geq 0\\}\\)\nCaracterização da linguagem: \\(L_1\\) contém strings onde o número de \\(a\\)’s é maior ou igual ao número de \\(b\\)’s. Exemplos: \\(\\{\\epsilon, a, aa, ab, aaa, aab, aaab, \\ldots\\}\\).\nAplicação do Lema do Bombeamento:\nPasso 1 - Hipótese: Assumimos por contradição que \\(L_1\\) é regular.\nPasso 2 - Constante de bombeamento: Se \\(L_1\\) é regular, então existe uma constante \\(p &gt; 0\\) tal que qualquer string \\(w \\in L_1\\) com \\(|w| \\geq p\\) pode ser dividida como \\(w = xyz\\) satisfazendo:\n\n\\(|y| &gt; 0\\);\n\\(|xy| \\leq p\\);\n\\(xy^iz \\in L_1\\) para todo \\(i \\geq 0\\).\n\nPasso 3 - Escolha da string: Escolhemos \\(w = a^pb^p \\in L_1\\) (válida porque \\(p \\geq p \\geq 0\\)).\nPasso 4 - Análise da divisão: Como \\(|xy| \\leq p\\) e \\(w\\) inicia com \\(p\\) símbolos \\(a\\) seguidos de \\(p\\) símbolos \\(b\\), a substring \\(xy\\) está inteiramente contida no bloco inicial de \\(a\\)’s. Portanto, \\(y = a^k\\) para algum \\(k\\) com \\(1 \\leq k \\leq p\\).\nPasso 5 - Teste do bombeamento: Consideramos \\(i = 2\\): \\[xy^2z = a^{p+k}b^p\\]\nPasso 6 - Verificação da condição: Para que \\(xy^2z \\in L_1\\), devemos ter: \\[\\text{número de } a\\text{'s} \\geq \\text{número de } b\\text{'s}\\] \\[p + k \\geq p\\] \\[k \\geq 0\\]\nComo \\(k \\geq 1\\) (porque \\(|y| &gt; 0\\)), a condição \\(k \\geq 0\\) é sempre satisfeita.\nPasso 7 - Teste com \\(i = 0\\): Consideramos \\(i = 0\\): \\[xy^0z = xz = a^{p-k}b^p\\]\nPara que \\(xz \\in L_1\\), devemos ter: \\[p - k \\geq p\\] \\[-k \\geq 0\\] \\[k \\leq 0\\]\nPasso 8 - Contradição: Temos \\(k \\geq 1\\) (de \\(|y| &gt; 0\\)) e \\(k \\leq 0\\) (da condição de bombeamento), o que é uma contradição.\nConclusão: \\(L_1\\) não é regular.\nAnálise de \\(L_2 = \\{a^mb^n \\mid n \\geq m \\geq 0\\}\\)\nCaracterização da linguagem: \\(L_2\\) contém strings onde o número de \\(b\\)’s é maior ou igual ao número de \\(a\\)’s. Exemplos: \\(\\{\\epsilon, b, a, bb, ab, bbb, abb, abbb, \\ldots\\}\\).\nAplicação do Lema do Bombeamento:\nPassos 1-4: Idênticos à análise de \\(L_1\\), escolhendo \\(w = a^pb^p \\in L_2\\).\nPasso 5 - Teste do bombeamento com \\(i = 2\\): \\[xy^2z = a^{p+k}b^p\\]\nPasso 6 - Verificação da condição: Para que \\(xy^2z \\in L_2\\), devemos ter: \\[\\text{número de } b\\text{'s} \\geq \\text{número de } a\\text{'s}\\] \\[p \\geq p + k\\] \\[0 \\geq k\\] \\[k \\leq 0\\]\nPasso 7 - Contradição: Temos \\(k \\geq 1\\) (de \\(|y| &gt; 0\\)) e \\(k \\leq 0\\) (da condição), o que é uma contradição.\nConclusão: \\(L_2\\) não é regular.\n\nSolução Letra b:\n\nAnálise de \\(L_1 \\cap L_2\\)\nDeterminação da interseção: \\[L_1 \\cap L_2 = \\{a^nb^m \\mid n \\geq m \\geq 0\\} \\cap \\{a^mb^n \\mid n \\geq m \\geq 0\\}\\]\nPara uma string \\(a^rb^s\\) pertencer à interseção, deve satisfazer simultaneamente:\n\n\\(r \\geq s\\) (condição de \\(L_1\\))\n\\(s \\geq r\\) (condição de \\(L_2\\))\n\nIsso implica \\(r = s\\), portanto: \\[L_1 \\cap L_2 = \\{a^nb^n \\mid n \\geq 0\\}\\]\nAnálise de regularidade: Esta é a linguagem clássica de balanceamento, que não é regular. Pode ser provada usando o Lema do Bombeamento com a string \\(a^pb^p\\).\nAnálise de \\(L_1 \\cup L_2\\)\nDeterminação da união: \\[L_1 \\cup L_2 = \\{a^nb^m \\mid n \\geq m \\geq 0\\} \\cup \\{a^mb^n \\mid n \\geq m \\geq 0\\}\\]\nAnálise de regularidade: Como \\(L_1 \\subseteq L_1 \\cup L_2\\) e \\(L_1\\) não é regular, se \\(L_1 \\cup L_2\\) fosse regular, então pela propriedade de fechamento das linguagens regulares sob interseção com linguagens regulares, poderíamos construir \\(L_1\\) como uma interseção envolvendo \\(L_1 \\cup L_2\\). Isso criaria uma contradição.\nConclusão: \\(L_1 \\cup L_2\\) não é regular.\nAnálise de \\(L_1 \\cdot L_2\\)\nCaracterização da concatenação: \\[L_1 \\cdot L_2 = \\{uv \\mid u \\in L_1 \\text{ e } v \\in L_2\\}\\]\nAnálise de regularidade: A concatenação de duas linguagens não-regulares não é necessariamente não-regular. No entanto, neste caso específico, podemos mostrar que \\(L_1 \\cdot L_2\\) contém como subset a linguagem \\(\\{a^na^nb^nb^n \\mid n \\geq 0\\} = \\{a^{2n}b^{2n} \\mid n \\geq 0\\}\\), que é uma variação da linguagem de balanceamento e não é regular.\nConclusão: \\(L_1 \\cdot L_2\\) não é regular.\n\nSolução Letra c:\n\nAnálise de \\(L_3 = \\{a^nb^n \\mid n \\geq 0\\} \\cup \\{a^mb^m \\mid m \\geq 0\\}\\)\nSimplificação da linguagem: \\[L_3 = \\{a^nb^n \\mid n \\geq 0\\} \\cup \\{a^mb^m \\mid m \\geq 0\\} = \\{a^kb^k \\mid k \\geq 0\\}\\]\n\nAmbos os conjuntos na união representam a mesma linguagem (apenas com variáveis diferentes).\nAnálise de regularidade: A linguagem \\(L_3 = \\{a^kb^k \\mid k \\geq 0\\}\\) é a linguagem clássica de strings balanceadas, que sabemos não ser regular.\nAplicação do Lema do Bombeamento para \\(L_3\\):\nAssumindo que \\(L_3\\) é regular, seja \\(p\\) a constante de bombeamento. Escolhemos \\(w = a^pb^p \\in L_3\\).\nPara qualquer divisão \\(w = xyz\\) com \\(|xy| \\leq p\\) e \\(|y| &gt; 0\\), temos \\(y = a^k\\) com \\(k \\geq 1\\).\nPara \\(i = 2\\): \\(xy^2z = a^{p+k}b^p\\). Como \\(p + k \\neq p\\), esta string não pertence a \\(L_3\\).\nIsso contradiz o Lema do Bombeamento.\nConclusão: \\(L_3\\) não é regular, portanto não existe expressão regular para \\(L_3\\).\n\n\n3.8.2 Exercício 5\nSeja \\(\\Sigma\\) o alfabeto ASCII estendido. Defina os seguintes conjuntos de símbolos: a. \\(L = (a \\cup b \\cup c \\cup \\ldots \\cup z \\cup A \\cup B \\cup C \\cup \\ldots \\cup Z)\\) (letras); b. \\(D = (0 \\cup 1 \\cup 2 \\cup 3 \\cup 4 \\cup 5 \\cup 6 \\cup 7 \\cup 8 \\cup 9)\\) (dígitos); c. \\(D_{nz} = (1 \\cup 2 \\cup 3 \\cup 4 \\cup 5 \\cup 6 \\cup 7 \\cup 8 \\cup 9)\\) (dígitos não-zero); d. \\(S = (+ \\cup - \\cup * \\cup / \\cup = \\cup &lt; \\cup &gt; \\cup !)\\) (símbolos de operadores).\nProjete um analisador léxico para uma linguagem que suporte identificadores, números (inteiros, decimais, científicos), operadores aritméticos e relacionais, e comentários.\n\nConstrua expressões regulares sistemáticas para cada categoria de token.\nAnalise os conflitos de ambiguidade entre as categorias.\nEstabeleça uma hierarquia de precedência para resolver os conflitos.\n\nSolução Letra a:\n\n3.8.2.1 1. Categoria: Identificadores (\\(r_{id}\\))\nEspecificação: Identificadores começam com letra ou underscore, seguidos de letras, dígitos ou underscores.\nConstrução sistemática: 1. Símbolos iniciais válidos: \\(S_{ini} = L \\cup \\{\\_\\}\\); 2. Símbolos de continuação: \\(S_{cont} = L \\cup D \\cup \\{\\_\\}\\); 3. Expressão para identificadores:\n\\[r_{id} = S_{ini} \\cdot S_{cont}^*\\]\nExpandindo as definições: \\[r_{id} = (L \\cup \\{\\_\\}) \\cdot (L \\cup D \\cup \\{\\_\\})^*\\]\n\n\n3.8.2.2 2. Categoria: Números (\\(r_{num}\\))\nSubcategoria 2.1 - Números Inteiros (\\(r_{int}\\)): 1. Zero isolado: \\(0\\); 2. Números positivos: \\(D_{nz} \\cdot D^*\\); 3. Números com sinal: \\((+ \\cup - \\cup \\epsilon) \\cdot (0 \\cup (D_{nz} \\cdot D^*))\\).\n\\[r_{int} = (+ \\cup - \\cup \\epsilon) \\cdot (0 \\cup (D_{nz} \\cdot D^*))\\]\nSubcategoria 2.2 - Números Decimais (\\(r_{dec}\\)): - Parte inteira obrigatória, ponto, parte fracionária obrigatória:\n\\[r_{dec} = r_{int} \\cdot \\{.\\} \\cdot D \\cdot D^*\\]\nSubcategoria 2.3 - Notação Científica (\\(r_{sci}\\)): - Base (inteiro ou decimal), indicador científico, expoente:\n\\[r_{sci} = (r_{int} \\cup r_{dec}) \\cdot (e \\cup E) \\cdot (+ \\cup - \\cup \\epsilon) \\cdot D \\cdot D^*\\]\nExpressão completa para números: \\[r_{num} = r_{sci} \\cup r_{dec} \\cup r_{int}\\]\n\n\n3.8.2.3 3. Categoria: Operadores (\\(r_{op}\\))\nSubcategoria 3.1 - Operadores Simples (\\(r_{op\\_simples}\\)): \\[r_{op\\_simples} = + \\cup - \\cup * \\cup / \\cup = \\cup &lt; \\cup &gt;\\]\nSubcategoria 3.2 - Operadores Compostos (\\(r_{op\\_comp}\\)):\n\nIncremento/Decremento: \\((+ \\cdot +) \\cup (- \\cdot -)\\);\nIgualdade/Desigualdade: \\((= \\cdot =) \\cup (! \\cdot =)\\);\nRelacionais compostos: \\((&lt; \\cdot =) \\cup (&gt; \\cdot =)\\).\n\n\\[r_{op\\_comp} = (+ \\cdot +) \\cup (- \\cdot -) \\cup (= \\cdot =) \\cup (! \\cdot =) \\cup (&lt; \\cdot =) \\cup (&gt; \\cdot =)\\]\nExpressão completa para operadores: \\[r_{op} = r_{op\\_comp} \\cup r_{op\\_simples}\\]\n\n\n3.8.2.4 4. Categoria: Comentários (\\(r_{com}\\))\nSubcategoria 4.1 - Comentários de Linha (\\(r_{com\\_linha}\\)): 1. Sequência // seguida de qualquer caractere até quebra de linha:\n\\[r_{com\\_linha} = / \\cdot / \\cdot \\Sigma_{texto}^* \\cdot \\text{EOL}\\]\nonde \\(\\Sigma_{texto}\\) representa todos os caracteres exceto quebra de linha.\nSubcategoria 4.2 - Comentários de Bloco (\\(r_{com\\_bloco}\\)): 1. Sequência /* seguida de qualquer texto até */:\n\\[r_{com\\_bloco} = / \\cdot * \\cdot \\Sigma_{qualquer}^* \\cdot * \\cdot /\\]\nonde \\(\\Sigma_{qualquer}\\) representa qualquer caractere, mas com restrição de não formar */ prematuramente.\nExpressão completa para comentários: \\[r_{com} = r_{com\\_linha} \\cup r_{com\\_bloco}\\]\nSolução Letra b:\n\n\n3.8.2.5 Análise de Conflitos de Ambiguidade\n\nConflito 1 - Identificadores vs Palavras-chave: palavras reservadas como if, while, for são sintaticamente válidas como identificadores.\n\nExemplo: A string if satisfaz \\(r_{id} = (L \\cup \\{\\_\\}) \\cdot (L \\cup D \\cup \\{\\_\\})^*\\)\nImpacto: O analisador não pode distinguir entre palavra-chave e identificador apenas pela forma\n\nConflito 2 - Operadores Compostos vs Sequência de Operadores Simples: sequências como ++ podem ser interpretadas como operador composto ou dois operadores +.\nExemplo: A string ++ satisfaz tanto \\(r_{op\\_comp}\\) quanto \\(r_{op\\_simples} \\cdot r_{op\\_simples}\\) Implicação: Ambiguidade na tokenização que afeta a análise sintática\nConflito 3 - Números Decimais vs Operador Ponto: se existir operador ponto (.), pode conflitar com números decimais.\n\nExemplo: 1.5 pode ser interpretado como número decimal ou 1 seguido de . seguido de 5 Contexto: Especialmente problemático em linguagens com notação de acesso a membros\n\nConflito 4 - Comentários vs Operadores de Divisão: o símbolo / inicia tanto comentários quanto operação de divisão\n\nExemplo: A sequência / pode ser início de //, /* ou operador de divisão Complexidade: Requer lookahead para decidir a interpretação\n\nConflito 5 - Sinais vs Operadores vs Números com Sinal: os símbolos + e - podem ser operadores binários, unários, ou parte de números.\n\nExemplo: -5 pode ser número negativo ou operador - seguido de número 5 Dependência: Resolução depende do contexto sintático\nSolução Letra c:\n\n\n3.8.2.6 Hierarquia de Precedência para Resolução de Conflitos\n\nNível 1 (Maior Precedência): Comentários\n\n\nRegra: Reconhecer // e /* antes de qualquer outra análise\nJustificativa: Comentários alteram fundamentalmente o processamento (podem “comentar” outros tokens)\nImplementação: Scanner deve verificar comentários primeiro em cada posição\n\n\nNível 2: Palavras-chave Reservadas\n\n\nRegra: Lista finita de palavras reservadas tem precedência sobre \\(r_{id}\\)\nMétodo: Lookup table de palavras-chave após reconhecer padrão de identificador\nAlgoritmo:\n\nAplicar \\(r_{id}\\) para reconhecer padrão\nConsultar tabela de palavras reservadas\nSe encontrada, classificar como palavra-chave; senão, como identificador\n\n\n\nNível 3: Números (Regra do Maior Match)\n\n\nRegra: Sempre consumir a maior sequência válida como número\nOrdem de tentativa: \\(r_{sci} \\succ r_{dec} \\succ r_{int}\\)\nExemplo: 1.5e-3 deve ser reconhecido como um token científico, não como três tokens separados\n\n\nNível 4: Operadores Compostos (Regra do Maior Match)\n\n\nRegra: Operadores compostos têm precedência sobre sequências de operadores simples\nExemplo: ++ é reconhecido como incremento, não como dois operadores +\nImplementação: Verificar padrões compostos antes dos simples\n\n\nNível 5: Operadores Simples\n\n\nRegra: Reconhecer operadores individuais\nResolução de ambiguidade: Dependente do contexto sintático (análise posterior)\n\nAlgoritmo de Tokenização:\nA seguir a esforçada leitora pode ver um pseudocódigo de um dos possíveis algoritmos de tokenização que implementa a hierarquia de precedência conforme definido neste exercício:\nPara cada posição no texto fonte:\n   1. Ignorar espaços em branco\n   2. Se match(r_com): retornar TOKEN_COMENTARIO\n   3. Se match(r_num): retornar TOKEN_NUMERO\n   4. Se match(r_id):\n      4.1. Se é palavra_reservada: retornar TOKEN_PALAVRA_CHAVE\n      4.2. Senão: retornar TOKEN_IDENTIFICADOR\n   5. Se match(r_op_comp): retornar TOKEN_OPERADOR_COMPOSTO\n   6. Se match(r_op_simples): retornar TOKEN_OPERADOR_SIMPLES\n   7. Senão: erro léxico\nConsiderações Especiais:\n\nLookahead Necessário:\n\nComentários: Verificar / seguido de / ou *;\nOperadores: Verificar + seguido de +, = seguido de =, etc.;\nNúmeros científicos: Verificar e/E em contexto numérico.\n\nTratamento de Contexto:\n\nSinais unários vs binários: Resolver na fase de análise sintática;\nNúmeros negativos: Tratar como operador unário aplicado a número positivo.\n\n\n\n\n3.8.2.7 Em C++ 23\nO código a seguir contém uma implementação do analisador léxico definido neste problema em C++23, usando as características desta linguagem para simplificar a implementação:\n#include &lt;iostream&gt;\n#include &lt;string&gt;\n#include &lt;string_view&gt;\n#include &lt;vector&gt;\n#include &lt;unordered_set&gt;\n#include &lt;expected&gt;\n#include &lt;print&gt;\n#include &lt;ranges&gt;\n#include &lt;algorithm&gt;\n#include &lt;concepts&gt;\n#include &lt;optional&gt;\n\n// Definição dos tipos de token seguindo nossa hierarquia\nenum class TokenType {\n    COMENTARIO,\n    PALAVRA_CHAVE, \n    NUMERO,\n    IDENTIFICADOR,\n    OPERADOR_COMPOSTO,\n    OPERADOR_SIMPLES,\n    UNKNOWN,\n    END_OF_INPUT\n};\n\n// Estrutura do token\nstruct Token {\n    TokenType type;\n    std::string value;\n    \n    void print() const {\n        std::println(\"Token: {} | Valor: '{}'\", to_string(type), value);\n    }\n};\n\n// Conversão de TokenType para string\nstd::string to_string(TokenType type) {\n    switch (type) {\n        case TokenType::COMENTARIO: return \"COMENTARIO\";\n        case TokenType::PALAVRA_CHAVE: return \"PALAVRA_CHAVE\";\n        case TokenType::NUMERO: return \"NUMERO\";\n        case TokenType::IDENTIFICADOR: return \"IDENTIFICADOR\";\n        case TokenType::OPERADOR_COMPOSTO: return \"OPERADOR_COMPOSTO\";\n        case TokenType::OPERADOR_SIMPLES: return \"OPERADOR_SIMPLES\";\n        case TokenType::UNKNOWN: return \"UNKNOWN\";\n        case TokenType::END_OF_INPUT: return \"END_OF_INPUT\";\n    }\n    return \"INVALID\";\n}\n\n// Conceitos para classificação de caracteres (conjuntos L, D, etc.)\ntemplate&lt;typename T&gt;\nconcept Character = std::same_as&lt;T, char&gt;;\n\nclass CharacterClassifier {\npublic:\n    // Conjunto L = (a ∪ b ∪ ... ∪ z ∪ A ∪ B ∪ ... ∪ Z)\n    static constexpr bool isLetter(char c) {\n        return (c &gt;= 'a' && c &lt;= 'z') || (c &gt;= 'A' && c &lt;= 'Z');\n    }\n    \n    // Conjunto D = (0 ∪ 1 ∪ 2 ∪ ... ∪ 9)\n    static constexpr bool isDigit(char c) {\n        return c &gt;= '0' && c &lt;= '9';\n    }\n    \n    // Conjunto D_nz = (1 ∪ 2 ∪ ... ∪ 9)\n    static constexpr bool isNonZeroDigit(char c) {\n        return c &gt;= '1' && c &lt;= '9';\n    }\n    \n    // Conjunto S_ini = L ∪ {_}\n    static constexpr bool isIdentifierStart(char c) {\n        return isLetter(c) || c == '_';\n    }\n    \n    // Conjunto S_cont = L ∪ D ∪ {_}\n    static constexpr bool isIdentifierContinuation(char c) {\n        return isLetter(c) || isDigit(c) || c == '_';\n    }\n    \n    static constexpr bool isWhitespace(char c) {\n        return c == ' ' || c == '\\t' || c == '\\n' || c == '\\r';\n    }\n    \n    static constexpr bool isOperatorSymbol(char c) {\n        return c == '+' || c == '-' || c == '*' || c == '/' || \n               c == '=' || c == '&lt;' || c == '&gt;' || c == '!';\n    }\n};\n\nclass LexicalAnalyzer {\nprivate:\n    std::string_view input;\n    size_t position = 0;\n    \n    // Palavras-chave reservadas (Nível 2 da hierarquia)\n    std::unordered_set&lt;std::string&gt; keywords = {\n        \"if\", \"else\", \"while\", \"for\", \"int\", \"float\", \"double\",\n        \"char\", \"void\", \"return\", \"break\", \"continue\", \"true\", \"false\"\n    };\n    \npublic:\n    explicit LexicalAnalyzer(std::string_view text) : input(text) {}\n    \n    // Método principal de tokenização seguindo nossa hierarquia\n    std::expected&lt;Token, std::string&gt; nextToken() {\n        skipWhitespace();\n        \n        if (position &gt;= input.length()) {\n            return Token{TokenType::END_OF_INPUT, \"\"};\n        }\n        \n        // Nível 1: Comentários (maior precedência)\n        if (auto comment = tryParseComment()) {\n            return *comment;\n        }\n        \n        // Nível 3: Números (regra do maior match)\n        if (auto number = tryParseNumber()) {\n            return *number;\n        }\n        \n        // Nível 2: Identificadores (depois verificamos se é palavra-chave)\n        if (auto identifier = tryParseIdentifier()) {\n            return *identifier;\n        }\n        \n        // Nível 4: Operadores compostos (regra do maior match)\n        if (auto compoundOp = tryParseCompoundOperator()) {\n            return *compoundOp;\n        }\n        \n        // Nível 5: Operadores simples\n        if (auto simpleOp = tryParseSimpleOperator()) {\n            return *simpleOp;\n        }\n        \n        // Erro léxico\n        return std::unexpected(std::format(\"Caractere inválido '{}' na posição {}\", \n                                         input[position], position));\n    }\n    \nprivate:\n    void skipWhitespace() {\n        while (position &lt; input.length() && CharacterClassifier::isWhitespace(input[position])) {\n            position++;\n        }\n    }\n    \n    char peek(size_t offset = 0) const {\n        size_t pos = position + offset;\n        return pos &lt; input.length() ? input[pos] : '\\0';\n    }\n    \n    // Nível 1: r_com = r_com_linha ∪ r_com_bloco\n    std::optional&lt;Token&gt; tryParseComment() {\n        if (peek() != '/') return std::nullopt;\n        \n        // r_com_linha = / · / · Σ_texto* · EOL\n        if (peek(1) == '/') {\n            size_t start = position;\n            position += 2; // Consome \"//\"\n            \n            // Consome até o final da linha\n            while (position &lt; input.length() && input[position] != '\\n') {\n                position++;\n            }\n            if (position &lt; input.length()) position++; // Consome '\\n'\n            \n            return Token{TokenType::COMENTARIO, \n                        std::string(input.substr(start, position - start))};\n        }\n        \n        // r_com_bloco = / · * · Σ_qualquer* · * · /\n        if (peek(1) == '*') {\n            size_t start = position;\n            position += 2; // Consome \"/*\"\n            \n            // Procura por \"*/\"\n            while (position &lt; input.length() - 1) {\n                if (input[position] == '*' && input[position + 1] == '/') {\n                    position += 2; // Consome \"*/\"\n                    return Token{TokenType::COMENTARIO,\n                                std::string(input.substr(start, position - start))};\n                }\n                position++;\n            }\n            \n            // Comentário não fechado - erro\n            return std::nullopt;\n        }\n        \n        return std::nullopt;\n    }\n    \n    // Nível 3: r_num = r_sci ∪ r_dec ∪ r_int\n    std::optional&lt;Token&gt; tryParseNumber() {\n        if (!CharacterClassifier::isDigit(peek()) && \n            peek() != '+' && peek() != '-') {\n            return std::nullopt;\n        }\n        \n        size_t start = position;\n        \n        // Consome sinal opcional\n        if (peek() == '+' || peek() == '-') {\n            position++;\n        }\n        \n        // r_int = (+ ∪ - ∪ ε) · (0 ∪ (D_nz · D*))\n        if (!parseIntegerPart()) {\n            position = start;\n            return std::nullopt;\n        }\n        \n        // Tenta parte decimal: r_dec = r_int · {.} · D · D*\n        bool hasDecimal = false;\n        if (peek() == '.') {\n            size_t dotPos = position;\n            position++; // Consome '.'\n            \n            if (CharacterClassifier::isDigit(peek())) {\n                hasDecimal = true;\n                // Consome dígitos após o ponto\n                while (CharacterClassifier::isDigit(peek())) {\n                    position++;\n                }\n            } else {\n                // Não é número decimal, volta atrás\n                position = dotPos;\n            }\n        }\n        \n        // Tenta notação científica: r_sci = (r_int ∪ r_dec) · (e ∪ E) · (+ ∪ - ∪ ε) · D · D*\n        if (peek() == 'e' || peek() == 'E') {\n            size_t expPos = position;\n            position++; // Consome 'e' ou 'E'\n            \n            // Sinal opcional no expoente\n            if (peek() == '+' || peek() == '-') {\n                position++;\n            }\n            \n            // Deve ter pelo menos um dígito no expoente\n            if (CharacterClassifier::isDigit(peek())) {\n                while (CharacterClassifier::isDigit(peek())) {\n                    position++;\n                }\n            } else {\n                // Notação científica inválida\n                position = expPos;\n            }\n        }\n        \n        return Token{TokenType::NUMERO, \n                    std::string(input.substr(start, position - start))};\n    }\n    \n    bool parseIntegerPart() {\n        if (peek() == '0') {\n            position++;\n            return true;\n        }\n        \n        if (CharacterClassifier::isNonZeroDigit(peek())) {\n            position++;\n            while (CharacterClassifier::isDigit(peek())) {\n                position++;\n            }\n            return true;\n        }\n        \n        return false;\n    }\n    \n    // Nível 2: r_id = (L ∪ {_}) · (L ∪ D ∪ {_})*\n    std::optional&lt;Token&gt; tryParseIdentifier() {\n        if (!CharacterClassifier::isIdentifierStart(peek())) {\n            return std::nullopt;\n        }\n        \n        size_t start = position;\n        position++; // Consome primeiro caractere\n        \n        // Consome caracteres de continuação\n        while (CharacterClassifier::isIdentifierContinuation(peek())) {\n            position++;\n        }\n        \n        std::string value(input.substr(start, position - start));\n        \n        // Verifica se é palavra-chave (Nível 2 da hierarquia)\n        TokenType type = keywords.contains(value) ? \n                        TokenType::PALAVRA_CHAVE : TokenType::IDENTIFICADOR;\n        \n        return Token{type, value};\n    }\n    \n    // Nível 4: r_op_comp = (+ · +) ∪ (- · -) ∪ (= · =) ∪ (! · =) ∪ (&lt; · =) ∪ (&gt; · =)\n    std::optional&lt;Token&gt; tryParseCompoundOperator() {\n        char first = peek();\n        char second = peek(1);\n        \n        if ((first == '+' && second == '+') ||\n            (first == '-' && second == '-') ||\n            (first == '=' && second == '=') ||\n            (first == '!' && second == '=') ||\n            (first == '&lt;' && second == '=') ||\n            (first == '&gt;' && second == '=')) {\n            \n            std::string value{first, second};\n            position += 2;\n            return Token{TokenType::OPERADOR_COMPOSTO, value};\n        }\n        \n        return std::nullopt;\n    }\n    \n    // Nível 5: r_op_simples = + ∪ - ∪ * ∪ / ∪ = ∪ &lt; ∪ &gt;\n    std::optional&lt;Token&gt; tryParseSimpleOperator() {\n        char c = peek();\n        \n        if (CharacterClassifier::isOperatorSymbol(c)) {\n            position++;\n            return Token{TokenType::OPERADOR_SIMPLES, std::string(1, c)};\n        }\n        \n        return std::nullopt;\n    }\n};\n\n// Função para processar uma linha de entrada\nvoid processLine(std::string_view line) {\n    std::println(\"\\n=== Analisando: '{}' ===\", line);\n    \n    LexicalAnalyzer analyzer(line);\n    \n    while (true) {\n        auto result = analyzer.nextToken();\n        \n        if (!result) {\n            std::println(\"ERRO: {}\", result.error());\n            break;\n        }\n        \n        Token token = *result;\n        \n        if (token.type == TokenType::END_OF_INPUT) {\n            std::println(\"=== Fim da análise ===\");\n            break;\n        }\n        \n        token.print();\n    }\n}\n\nint main() {\n    std::println(\"=== Analisador Léxico C++23 ===\");\n    std::println(\"Digite expressões linha por linha (Ctrl+C para sair):\");\n    std::println();\n    \n    std::string line;\n    while (std::getline(std::cin, line)) {\n        if (!line.empty()) {\n            processLine(line);\n        }\n        std::println(\"\\nDigite a próxima expressão:\");\n    }\n    \n    return 0;\n}\nPara usar:\nDigite: int x = 42;\nNeste caso a saída deve ser:\nToken: PALAVRA_CHAVE | Valor: 'int'\nToken: IDENTIFICADOR | Valor: 'x'\nToken: OPERADOR_SIMPLES | Valor: '='\nToken: NUMERO | Valor: '42'",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Alfabetos, Linguagens e Strings: Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "01a-lexico.html#exercício-6-classes-de-equivalência",
    "href": "01a-lexico.html#exercício-6-classes-de-equivalência",
    "title": "3  Alfabetos, Linguagens e Strings: Fundamentos Matemáticos",
    "section": "3.9 Exercício 6: Classes de Equivalência",
    "text": "3.9 Exercício 6: Classes de Equivalência\nConsidere a linguagem \\(L = \\{w \\in \\{0,1\\}^* \\mid w \\text{ contém um número par de } 0\\text{'s e um número ímpar de } 1\\text{'s}\\}\\).\n\nDetermine quantas classes de equivalência existem na relação de Myhill-Nerode para \\(L\\) (ver Chapter 10).\nConstrua o autômato finito correspondente.\n\nSolução Letra a\n\nClasses de equivalência: precisamos rastrear paridades de 0’s e 1’s:\n\n\n\\(q_{00}\\): par de 0’s, par de 1’s;\n\\(q_{01}\\): par de 0’s, ímpar de 1’s ← estado de aceitação;\n\\(q_{10}\\): ímpar de 0’s, par de 1’s;\n\\(q_{11}\\): ímpar de 0’s, ímpar de 1’s.\n\nTotal: 4 classes de equivalência\nSolução Letra b: Autômato Finito:\nEstados: {q00, q01, q10, q11} Estado inicial: q00 Estado final: {q01}\n\n\n\nEstado\n0\n1\n\n\n\n\n→q00\nq10\nq01\n\n\n*q01\nq11\nq00\n\n\nq10\nq00\nq11\n\n\nq11\nq01\nq10\n\n\n\nLegenda: → Estado inicial; * Estado final.\n\n3.9.1 Exercício 7: Lema do Bombeamento\nConsidere a linguagem \\(L = \\{0^i1^j0^k \\mid i, j, k \\geq 1 \\text{ e } i + k = j\\}\\).\n\nProve que \\(L\\) não é regular usando o Lema do Bombeamento.\nIdentifique qual propriedade específica de \\(L\\) viola a capacidade dos autômatos finitos.\nConstrua uma linguagem regular \\(L'\\) que seja o “mais próxima possível” de \\(L\\).\n\nSolução letra a:\n\nProva usando Lema do Bombeamento:\n\nAssumimos que \\(L\\) é regular. Seja \\(p\\) a constante do bombeamento.\nEscolhemos \\(w = 0^p1^{2p}0^p \\in L\\) (porque \\(p + p = 2p\\)).\nComo \\(|w| = 4p \\geq p\\), podemos dividir \\(w = xyz\\) onde: - \\(|y| &gt; 0\\) - \\(|xy| \\leq p\\)\n- \\(xy^iz \\in L\\) para todo \\(i \\geq 0\\)\nComo \\(|xy| \\leq p\\) e \\(w\\) começa com \\(p\\) zeros, temos \\(y = 0^m\\) para algum \\(1 \\leq m \\leq p\\).\nPara \\(i = 2\\): \\(w' = xy^2z = 0^{p+m}1^{2p}0^p\\)\nNa string \\(w'\\): - Número de 0’s iniciais: \\(p + m\\) - Número de 1’s: \\(2p\\)\n- Número de 0’s finais: \\(p\\)\nA condição \\(i + k = j\\) requer: \\((p + m) + p = 2p\\), ou seja, \\(2p + m = 2p\\), logo \\(m = 0\\).\nMas isso contradiz \\(|y| &gt; 0\\), que implica \\(m \\geq 1\\).\nPortanto, \\(L\\) não é regular.\nSolução Letra b:\nA linguagem \\(L\\) requer uma relação aritmética entre partes não-adjacentes da string (\\(i + k = j\\)). Um autômato finito não pode “lembrar” o valor de \\(i\\) para comparar com \\(k\\) após processar toda a sequência de 1’s.\nSolução Letra c:\n\\[L' = 0^+1^+0^+\\]\nEsta linguagem captura a estrutura (0’s, depois 1’s, depois 0’s) sem a restrição aritmética.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Alfabetos, Linguagens e Strings: Fundamentos Matemáticos</span>"
    ]
  },
  {
    "objectID": "02-lexico.html",
    "href": "02-lexico.html",
    "title": "4  Autômatos Finitos Determinísticos",
    "section": "",
    "text": "4.1 Definição Formal e Componentes\nUm Autômato Finito Determinístico é formalmente definido como uma 5-tupla matemática:\n\\[M = (Q, \\Sigma, \\delta, q_0, F)\\]\nna qual cada componente possui um papel específico e bem definido:\nÉ importante que a atenta leitora observe que esta definição impõe restrições matemáticas precisas:\n\\[q_0 \\in Q \\quad \\text{e} \\quad F \\subseteq Q\\]\nEstas restrições garantem a consistência do modelo: o estado inicial deve necessariamente pertencer ao conjunto de estados, e todos os estados de aceitação devem ser estados válidos da máquina. Mais formalmente, um Autômato Finito Determinístico representa um sistema que, em qualquer momento, encontra-se em exatamente um estado do conjunto \\(Q\\), e que, ao receber um símbolo de entrada do alfabeto \\(\\Sigma\\), transita deterministicamente para um novo estado, que pode, inclusive, ser o mesmo estado atual.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autômatos Finitos Determinísticos</span>"
    ]
  },
  {
    "objectID": "02-lexico.html#sec-definicao-formal",
    "href": "02-lexico.html#sec-definicao-formal",
    "title": "4  Autômatos Finitos Determinísticos",
    "section": "",
    "text": "\\(Q\\) é o conjunto finito de estados da máquina;\n\\(\\Sigma\\) é o alfabeto de entrada, um conjunto finito de símbolos;\n\\(\\delta\\) é a função de transição determinística;\n\\(q_0\\) é o estado inicial único;\n\\(F\\) é o conjunto de estados de aceitação (ou estados finais).\n\n\n\n\n\n4.1.1 Exercícios 1\n1. Dado o Autômato Finito Determinístico \\(M =(\\{s_0,s_1,s_2,s_3\\},\\{a,b\\},\\delta,s_0,\\{s_0,s_3\\})\\), identifique cada um dos cinco componentes da tupla com sua respectiva descrição.\n2. Um modelo foi definido como \\(M =(Q,\\Sigma,\\delta,q_0,F)\\) onde \\(Q=\\{q_0,q_1\\}\\), \\(\\Sigma=\\{0,1\\}\\), \\(q_0=q_0\\), e \\(F=\\{q_2\\}\\). Este modelo representa um Autômato Finito Determinístico válido? Justifique sua resposta com base nas restrições formais.\n3. Se um Autômato Finito Determinístico possui \\(\\mid Q \\mid=3\\) estados e um alfabeto com \\(\\mid \\Sigma \\mid=4\\) símbolos, qual é o número exato de pares no domínio da função de transição \\(\\delta\\)?\n4. É possível que o conjunto de estados de aceitação \\(F\\) seja igual ao conjunto de todos os estados \\(Q\\)? Se sim, o que isso significaria sobre a linguagem reconhecida pelo autômato?\n5. É possível que o estado inicial \\(q_0\\) também seja um estado de aceitação, ou seja, q_0inF? Se sim, o que isso implica sobre a aceitação da string vazia, epsilon?\n\n\n4.1.2 A Função de Transição Determinística\nO coração de um Autômato Finito Determinístico reside na sua função de transição \\(\\delta\\), que captura completamente o comportamento da máquina. Esta função é definida matematicamente como:\n\\[\\delta : Q \\times \\Sigma \\rightarrow Q\\]\nA função de transição \\(\\delta\\) possui uma propriedade fundamental que a distingue de modelos não-determinísticos: para cada par \\((q, a)\\) no qual \\(q \\in Q\\) e \\(a \\in \\Sigma\\), existe exatamente um estado de destino. Esta unicidade elimina qualquer ambiguidade no processo de computação e permite que a máquina seja simulada de forma eficiente em tempo linear.\nA definição matemática da função de transição \\(\\delta : Q \\times \\Sigma \\rightarrow Q\\) implica que ela deve ser uma função total, ou seja, deve haver exatamente uma transição definida para cada par de estado e símbolo de entrada. Um Autômato Finito Determinístico que cumpre essa exigência é chamado de completo.\nNa prática, muitos autômatos possuem combinações de estado e símbolo que quebram a lógica do padrão que está sendo reconhecido. Por exemplo, em um autômato que reconhece a palavra “treco”, o que acontece se ele estiver no estado inicial e ler a letra ‘z’?\nPara tratar esses casos e manter a função de transição total, introduz-se um estado de erro, também conhecido como estado poço ou sumidouro. Este é um estado especial não-final do qual não há escapatória: toda transição a partir do estado de erro aponta para ele mesmo.\n\\[\n\\forall a \\in \\Sigma, \\quad \\delta(q_{\\text{erro}}, a) = q_{\\text{erro}}\n\\]\nDessa forma, qualquer sequência de entrada que desvie do padrão desejado é permanentemente capturada pelo estado de erro, garantindo que a string seja rejeitada. Por uma questão de clareza visual, muitos diagramas de transição omitem o estado de erro e as setas que levam a ele, mas é importante saber que, para um autômato ser formalmente completo, essas transições implícitas devem existir.\nA característica determinística da função de transição tem implicações profundas para a implementação prática. Em um Autômato Finito Determinístico, não há escolhas a serem feitas durante a execução: dado o estado atual e o símbolo de entrada, o próximo estado é inequivocamente determinado. Esta propriedade permite implementações extremamente eficientes, nas quais cada símbolo de entrada requer apenas uma consulta à tabela de transições, tipicamente implementada como um array bidimensional, ou uma operação equivalente.\nO domínio da função \\(\\delta\\) é o produto cartesiano \\(Q \\times \\Sigma\\), que representa o conjunto de todas as combinações possíveis de estado atual e símbolo de entrada. Para uma máquina com cardinalidade \\(\\mid Q \\mid = n\\) estados e um alfabeto com cardinalidade \\(\\mid \\Sigma \\mid = k\\) símbolos, existem exatamente \\(n \\times k\\) transições possíveis, e cada uma deve estar definida para que o autômato seja completo e funcional.\n\n4.1.2.1 Exercícios 2\n1. Considere um Autômato Finito Determinístico que reconhece identificadores que começam com ‘l’ (letra) e são seguidos por ‘d’ (dígito). O alfabeto é \\(\\Sigma=l,d\\). As transições definidas são \\(\\delta(q_0,l)=q_1\\) e \\(\\delta(q_1,d)=q_1\\). O estado de aceitação é \\(F=q_1\\). Este autômato é completo? Se não, adicione um estado de erro \\(q_e\\) e liste todas as transições que faltam para completá-lo.\n2. Explique com suas próprias palavras por que a propriedade determinística (para cada par \\((q,a)\\), existe exatamente um estado de destino) é importante para a eficiência da implementação de um analisador léxico.\n3. Um estado de erro (ou poço) pode ser um estado de aceitação? Justifique sua resposta com base na função de um estado de erro.\n4. Se um Autômato Finito Determinístico completo não possui um estado de erro explicitamente desenhado em seu diagrama, o que se assume sobre as transições não mostradas?\n5. Dada a definição \\(\\delta:Q \\times \\Sigma \\rightarrow Q\\), se a máquina está no estado \\(q_i\\) e lê um símbolo \\(a \\in \\Sigma\\), é possível que ela permaneça no mesmo estado, ou seja, \\(\\delta(q_i,a)=q_i\\)? Dê um exemplo prático.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autômatos Finitos Determinísticos</span>"
    ]
  },
  {
    "objectID": "02-lexico.html#representações-de-autômatos-finitos-determinísticos",
    "href": "02-lexico.html#representações-de-autômatos-finitos-determinísticos",
    "title": "4  Autômatos Finitos Determinísticos",
    "section": "4.2 Representações de Autômatos Finitos Determinísticos",
    "text": "4.2 Representações de Autômatos Finitos Determinísticos\nOs Autômatos Finitos Determinísticos podem ser representados de diversas formas, cada uma adequada a diferentes contextos e propósitos. As três representações mais comuns são a algébrica, a tabular e a gráfica. Cada representação oferece vantagens específicas: a forma algébrica é precisa e compacta para definições formais, a tabela de transições é eficiente para implementação computacional, e o diagrama de transições oferece intuição visual sobre o comportamento da máquina.\n\n4.2.1 Representação Tabular\nA tabela de transições constitui uma das formas mais práticas e amplamente utilizadas para representar Autômatos Finitos Determinísticos, especialmente em implementações computacionais. Esta representação organiza a função de transição \\(\\delta\\) em uma matriz bidimensional, na qual as linhas correspondem aos estados e as colunas aos símbolos do alfabeto.\nA Table 4.1 ilustra um exemplo de tabela de transições para um Autômato Finito Determinístico:\n\n\n\nTable 4.1: Exemplo de tabela de transições para um Autômato Finito Determinístico que aceita strings que contenham pelo menos um ‘0’ seguido, em algum momento, por pelo menos um ‘1’.\n\n\n\n\n\nEstado            \n0    \n1    \n\n\n\n\n\\(\\rightarrow q_0\\)\n\\(q_2\\)\n\\(q_0\\)\n\n\n\\(*q_1\\)            \n\\(q_1\\)\n\\(q_1\\)\n\n\n\\(q_2\\)            \n\\(q_2\\)\n\\(q_1\\)\n\n\n\n\n\n\nNa Table 4.1, as convenções notacionais seguem o padrão estabelecido na literatura de teoria da computação:\n\nOs estados estão listados na primeira coluna, cada um representando um elemento do conjunto \\(Q\\);\nAs colunas subsequentes representam os símbolos do alfabeto \\(\\Sigma = \\{0, 1\\}\\);\nAs células da tabela contêm os estados de destino, definindo completamente a função \\(\\delta\\);\nA seta \\(\\rightarrow\\) identifica o estado inicial \\(q_0\\);\nO asterisco \\(*\\) marca os estados de aceitação, neste caso, o conjunto \\(F = \\{q_1\\}\\).\n\nEsta representação tabular possui vantagens computacionais significativas. A implementação de um Autômato Finito Determinístico baseada em tabela permite acesso em tempo constante \\(O(1)\\) para cada transição, resultando em uma complexidade total de \\(O(n)\\) para processar uma string de entrada de comprimento \\(n\\). Além disso, a estrutura tabular mapeia-se naturalmente para arrays bidimensionais na maioria das linguagens de programação.\n\n\n4.2.2 Representação Gráfica\nA representação gráfica, conhecida como diagrama de transições, oferece uma visualização intuitiva do comportamento dinâmico do Autômato Finito Determinístico. Nesta representação, a máquina é modelada como um grafo direcionado, no qual os vértices representam estados e as arestas rotuladas representam transições.\nA Figure 4.1 apresenta o diagrama de transições correspondente à tabela Table 4.1:\n\n\n\n\n\n\nFigure 4.1: Diagrama de transições de um Autômato Finito Determinístico que aceita strings que contenham pelo menos um ‘0’ seguido, em algum momento, por pelo menos um ‘1’.\n\n\n\nNo diagrama da Figure 4.1, as convenções visuais estabelecem uma linguagem gráfica precisa:\n\nOs círculos representam os estados do conjunto \\(Q\\);\nAs setas direcionadas representam as transições, codificando a função \\(\\delta\\);\nOs rótulos nas setas indicam os símbolos de entrada que causam as transições;\nO estado inicial é identificado por uma seta sem origem, vinda do nada;\nOs estados de aceitação são representados por círculos duplos, destacando visualmente sua função especial.\n\nA representação gráfica é particularmente valiosa para compreender o comportamento global da máquina e para visualizar caminhos de computação. Para a atenta leitora, o diagrama torna evidente que qualquer quantidade de ‘1’s iniciais será ignorada no estado \\(q_0\\). A máquina só avança ao ler um ’0’, transicionando para o estado \\(q_2\\). A partir daí, qualquer ‘1’ lido leva ao estado de aceitação \\(q_1\\), enquanto mais ’0’s mantêm a máquina em \\(q_2\\). Esta visualização facilita a compreensão do fluxo de estados e das condições de aceitação.\n\n\n4.2.3 Especificação Formal do Exemplo\nPara o Autômato Finito Determinístico ilustrado na Table 4.1 e na Figure 4.1, um autômato que aceita strings contendo um ‘0’ seguido em algum momento por um ‘1’ será dada pela 5-tupla \\(M = (Q, \\Sigma, \\delta, q_0, F)\\) definida como:\n\nConjunto de Estados (\\(Q\\)): \\(Q = \\{q_0, q_1, q_2\\}\\);\nAlfabeto (\\(\\Sigma\\)): \\(\\Sigma = \\{0, 1\\}\\);\nEstado Inicial (\\(q_0\\)): o estado inicial é \\(q_0\\);\nConjunto de Estados de Aceitação (\\(F\\)): \\(F = \\{q_1\\}\\);\nFunção de Transição (\\(\\delta\\)):\n\n\\(\\delta(q_0, 0) = q_2\\)\n\\(\\delta(q_0, 1) = q_0\\)\n\\(\\delta(q_1, 0) = q_1\\)\n\\(\\delta(q_1, 1) = q_1\\)\n\\(\\delta(q_2, 0) = q_2\\)\n\\(\\delta(q_2, 1) = q_1\\)\n\n\nEsta especificação formal define completamente o comportamento do autômato, incluindo seus estados, alfabeto, estado inicial, estados de aceitação e a função de transição. A atenta leitora notará que esta definição é suficiente para implementar ou simular o autômato em qualquer sistema computacional.\n\n\n4.2.4 Exercícios 3\n1. Desenhe o diagrama de transições para o Autômato Finito Determinístico descrito pela seguinte tabela. Use as convenções gráficas para o estado inicial e os estados de aceitação.\n\n\n\nEstado\n0\n1\n\n\n\n\nq_A\nq_B\nq_A\n\n\nq_B\nq_B\nq_C\n\n\n*q_C\nq_B\nq_A\n\n\n\n2. Para o diagrama de transições abaixo, que reconhece strings em a,b com um número par de ’a’s e um número par de ’b’s, forneça a especificação formal completa (a 5-tupla).\n3. Converta a especificação formal do Exemplo 2 da seção “Exemplos Práticos” (reconhecimento da senha abre) em uma tabela de transições completa, incluindo o estado de erro e todas as transições para ele. Considere o alfabeto =a,b,r,e,x, onde ‘x’ representa qualquer outro caractere.\n4. Qual representação (tabular, gráfica ou formal) você considera mais útil para depurar o comportamento de um autômato? E para implementá-lo em um programa? Justifique.\n5. Considere a tabela de transições da questão 3.1. Modifique-a para que o autômato passe a aceitar strings que terminam com “010” em vez de “01”.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autômatos Finitos Determinísticos</span>"
    ]
  },
  {
    "objectID": "02-lexico.html#computação-e-aceitação-de-strings",
    "href": "02-lexico.html#computação-e-aceitação-de-strings",
    "title": "4  Autômatos Finitos Determinísticos",
    "section": "4.3 Computação e Aceitação de strings",
    "text": "4.3 Computação e Aceitação de strings\nO processo de computação em um Autômato Finito Determinístico segue um algoritmo simples e elegante. Dada uma string de entrada \\(w = a_1a_2...a_k\\) onde cada \\(a_i \\in \\Sigma\\), a máquina executa os seguintes passos:\n\nInicialização: a máquina posiciona-se no estado inicial \\(q_0\\);\nProcessamento sequencial: para cada símbolo \\(a_i\\) da string de entrada, a máquina transita do estado atual \\(q\\) para o estado \\(\\delta(q, a_i)\\);\nDecisão de aceitação: após processar todos os símbolos, a string é aceita se, e somente se, o estado final atingido pertence ao conjunto \\(F\\) de estados de aceitação.\n\nFormalmente, uma string \\(w\\) é aceita pelo Autômato Finito Determinístico \\(M\\) se existe uma sequência de estados \\(r_0, r_1, ..., r_k\\) tal que:\n\n\\(r_0 = q_0\\) (inicia no estado inicial);\n\\(\\delta(r_i, a_{i+1}) = r_{i+1}\\) para \\(i = 0, 1, ..., k-1\\) (cada transição é válida);\n\\(r_k \\in F\\) (termina em um estado de aceitação).\n\nEsta formalização matemática da computação em Autômatos Finitos Determinísticos fundamenta toda a teoria de Linguagens Regulares e serve como base para os algoritmos de análise léxica utilizados em compiladores modernos.\n\n4.3.1 Exercícios 4\n1. Usando o autômato da questão 3.1, trace a computação para a string 01101 e determine se ela é aceita ou rejeitada.\n2. Para o autômato do Exemplo 3 (operadores relacionais), trace a computação para a string &lt;= e determine o resultado.\n3. Para o mesmo autômato de operadores relacionais, o que acontece ao processar a string !=? E a string =!?\n4. Uma string é aceita se a computação termina em um estado de aceitação. Se uma string passa por um estado de aceitação no meio de sua computação, mas não termina em um, ela é aceita? Justifique com base na definição formal de aceitação.\n5. Descreva uma string de comprimento 5 que é aceita pelo autômato da questão 3.2 e uma que é rejeitada. Mostre o caminho para ambas.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autômatos Finitos Determinísticos</span>"
    ]
  },
  {
    "objectID": "02-lexico.html#exemplos-práticos-de-autômatos-finitos-determinísticos",
    "href": "02-lexico.html#exemplos-práticos-de-autômatos-finitos-determinísticos",
    "title": "4  Autômatos Finitos Determinísticos",
    "section": "4.4 Exemplos Práticos de Autômatos Finitos Determinísticos",
    "text": "4.4 Exemplos Práticos de Autômatos Finitos Determinísticos\nPara consolidar a compreensão dos conceitos apresentados, a dedicada leitora encontrará a seguir três exemplos práticos que demonstram a versatilidade e aplicabilidade dos Autômatos Finitos Determinísticos em diferentes contextos. Estes exemplos ilustram desde problemas matemáticos básicos, como verificação de paridade, até aplicações práticas em análise léxica, como reconhecimento de senhas e operadores relacionais.\n\n4.4.1 Exemplo 1: Reconhecimento de Números Binários com Paridade Par\nO primeiro exemplo aborda um problema clássico na teoria da computação: o reconhecimento de números binários que possuem uma quantidade par de bits ‘1’. Este problema ilustra elegantemente como um Autômato Finito Determinístico pode manter informações de estado sobre propriedades matemáticas da entrada processada.\nDefinição do Problema: construir um Autômato Finito Determinístico que aceite todas as strings binárias (sobre o alfabeto \\(\\Sigma = \\{0, 1\\}\\)) que contenham um número par de símbolos ‘1’, incluindo zero ocorrências.\nAnálise: O autômato deve contar o número de bits ‘1’ processados, distinguindo entre quantidades pares e ímpares. Como só precisamos da paridade (par ou ímpar), dois estados são suficientes para capturar toda a informação necessária.\nUma forma de construir este autômato é definir dois estados: um para quando o número de ‘1’s é par e outro para quando é ímpar. A transição entre esses estados ocorre sempre que um símbolo ’1’ é lido, enquanto os símbolos ‘0’ não afetam a paridade. Em geral, tentar desenhar o diagrama de transições melhora a compreensão do comportamento do autômato. Neste caso, o Figure 4.2 ilustra o diagrama de transições correspondente:\n\n\n\n\n\n\nFigure 4.2: Diagrama de transições de um Autômato Finito Determinístico que aceita strings binárias com número par de símbolos ‘1’.\n\n\n\nEspecificação Formal:\n\nEstados: \\(Q = \\{q_{\\text{par}}, q_{\\text{ímpar}}\\}\\);\nAlfabeto: \\(\\Sigma = \\{0, 1\\}\\);\nEstado inicial: \\(q_0 = q_{\\text{par}}\\), zero, nenhum ‘1’, é par;\nEstados de aceitação: \\(F = \\{q_{\\text{par}}\\}\\);\nFunção de transição: \\(\\delta\\) definida por:\n\n\\(\\delta(q_{\\text{par}}, 0) = q_{\\text{par}}\\), zeros não alteram a paridade;\n\\(\\delta(q_{\\text{par}}, 1) = q_{\\text{ímpar}}\\), primeiro ‘1’ torna ímpar;\n\\(\\delta(q_{\\text{ímpar}}, 0) = q_{\\text{ímpar}}\\), zeros não alteram a paridade;\n\\(\\delta(q_{\\text{ímpar}}, 1) = q_{\\text{par}}\\), segundo ‘1’ retorna ao par.\n\n\nA Table 4.2 apresenta a tabela de transições correspondente:\n\n\n\nTable 4.2: Tabela de transições para reconhecimento de números binários com paridade par.\n\n\n\n\n\n\n\n\n\n\nEstado\n0\n1\n\n\n\n\n\\(\\rightarrow *q_{\\text{par}}\\)\n\\(q_{\\text{par}}\\)\n\\(q_{\\text{ímpar}}\\)\n\n\n\\(q_{\\text{ímpar}}\\)\n\\(q_{\\text{ímpar}}\\)\n\\(q_{\\text{par}}\\)\n\n\n\n\n\n\nEste autômato funciona como um contador módulo 2. O estado \\(q_{\\text{par}}\\) representa que um número par de ‘1’s foi processado, enquanto \\(q_{\\text{ímpar}}\\) indica um número ímpar. Os bits ’0’ são irrelevantes para a paridade e não causam mudanças de estado. Este é um exemplo perfeito de como um Autômato Finito Determinístico pode manter informações agregadas sobre a entrada sem necessidade de memória ilimitada. A definição zero é par pode ser vista como uma convenção prática. Quando temos apenas um zero não há ’1’s para contar. Por isso, o estado inicial (que representa a leitura de zero ’1’s) é também um estado de aceitação.\n\n\n4.4.2 Exemplo 2: Reconhecimento da Senha abre\nO segundo exemplo demonstra como Autômatos Finitos Determinísticos podem ser utilizados para reconhecer sequências específicas de caracteres, como senhas ou palavras-chave em linguagens de programação. Este tipo de reconhecimento é fundamental em analisadores léxicos.\nDefinição do Problema: construir um Autômato Finito Determinístico que aceite exatamente a string abre sobre o alfabeto das letras minúsculas.\nAnálise: o autômato deve reconhecer a sequência exata de caracteres ‘a’, ‘b’, ‘r’, ‘e’. Qualquer desvio desta sequência deve levar a um estado de rejeição. Como a entrada deve ser exatamente abre, necessitamos de cinco estados: um inicial, três intermediários correspondentes aos prefixos a, ab, abr, e um final de aceitação que corresponda a abre.\nNovamente, o diagrama de transições ajuda a visualizar o comportamento do autômato. A Figure 4.3 ilustra o diagrama de transições correspondente:\n\n\n\n\n\n\nFigure 4.3: Diagrama de transições de um Autômato Finito Determinístico que aceita a string abre.\n\n\n\nNo diagrama da Figure 4.3, cada estado representa um prefixo da string abre. A transição entre os estados ocorre conforme os caracteres são lidos, e o autômato rejeita qualquer entrada que não siga a sequência exata representada no diagrama por \\(x\\). O estado final \\(q_4\\) é o único estado de aceitação, indicando que a string completa foi reconhecida com sucesso.\nEspecificação Formal:\n\nEstados: \\(Q = \\{q_0, q_1, q_2, q_3, q_4, q_{\\text{erro}}\\}\\).\nAlfabeto: \\(\\Sigma = \\{a, b, c, ..., z\\}\\) (letras minúsculas).\nEstado inicial: \\(q_0\\).\nEstados de aceitação: \\(F = \\{q_4\\}\\).\nFunção de transição: \\(\\delta\\) definida por:\n\n\\(\\delta(q_0, a) = q_1\\), \\(\\delta(q_0, x) = q_{\\text{erro}}\\) para \\(x \\neq a\\);\n\\(\\delta(q_1, b) = q_2\\), \\(\\delta(q_1, x) = q_{\\text{erro}}\\) para \\(x \\neq b\\);\n\\(\\delta(q_2, r) = q_3\\), \\(\\delta(q_2, x) = q_{\\text{erro}}\\) para \\(x \\neq r\\);\n\\(\\delta(q_3, e) = q_4\\), \\(\\delta(q_3, x) = q_{\\text{erro}}\\) para \\(x \\neq e\\);\n\\(\\delta(q_4, x) = q_{\\text{erro}}\\) para qualquer \\(x \\in \\Sigma\\);\n\\(\\delta(q_{\\text{erro}}, x) = q_{\\text{erro}}\\) para qualquer \\(x \\in \\Sigma\\).\n\n\nA Table 4.3 apresenta uma versão simplificada da tabela de transições:\n\n\n\nTable 4.3: Tabela de transições para reconhecimento da senha abre.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstado\na\nb\nr\ne\noutros\n\n\n\n\n\\(\\rightarrow q_0\\)\n\\(q_1\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(q_1\\)\n\\(q_{\\text{erro}}\\)\n\\(q_2\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(q_2\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_3\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(q_3\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_4\\)\n\\(q_{\\text{erro}}\\)\n\n\n*\\(q_4\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\n\n\n\n\n\n4.4.3 Exemplo 3: Reconhecimento de Operadores Relacionais\nO terceiro exemplo aborda um problema típico da análise léxica: o reconhecimento de operadores relacionais compostos. Este exemplo ilustra como tratar ambiguidades que surgem quando alguns tokens são prefixos de outros (1).\nDefinição do Problema: Construir um Autômato Finito Determinístico que reconheça os operadores relacionais: &gt;, &lt;, &gt;=, &lt;=, ==, e !=.\nAnálise: Este problema apresenta desafios interessantes. Os operadores &gt; e &lt; são prefixos dos operadores &gt;= e &lt;=, respectivamente. Similarmente, = seria prefixo de ==. O autômato deve implementar a regra da correspondência mais longa (maximal munch), continuando a ler enquanto uma correspondência mais longa for possível.\nNós discutimos a regra da correspondência mais longa na seção Section 2.2 do capítulo anterior. Esta regra é fundamental para resolver ambiguidades de prefixos. O autômato deve ser capaz de distinguir entre os operadores simples e compostos, aceitando os mais longos quando possível.\nVamos ver como construir o autômato passo a passo, definindo estados que correspondem a cada prefixo dos operadores. O estado inicial \\(q_0\\) inicia o reconhecimento, e os estados subsequentes são alcançados conforme os símbolos de entrada são lidos. A Figure 4.4 ilustra o diagrama de transições correspondente:\n\n\n\n\n\n\nFigure 4.4: Diagrama de transições de um Autômato Finito Determinístico capaz de identificar os operadores relacionais ‘&lt;’, ‘&gt;’, ‘&lt;=’, ‘&gt;=’, ‘==’, ‘!=’.\n\n\n\nEspecificação Formal:\n\nEstados: \\(Q = \\{q_0, q_&gt;, q_&lt;, q_=, q_!, q_{\\geq}, q_{\\leq}, q_{==}, q_{\\neq}, q_{\\text{erro}}\\}\\) ;\nAlfabeto: \\(\\Sigma = \\{&gt;, &lt;, =, !, \\text{outros}\\}\\) ;\nEstado inicial: \\(q_0\\) ;\nEstados de aceitação: \\(F = \\{q_&gt;, q_&lt;, q_{\\geq}, q_{\\leq}, q_{==}, q_{\\neq}\\}\\).\n\nA Table 4.4 apresenta a tabela de transições:\n\n\n\nTable 4.4: Tabela de transições para reconhecimento de operadores relacionais.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstado\n&gt;\n&lt;\n=\n!\noutros\n\n\n\n\n\\(\\rightarrow q_0\\)\n\\(*q_&gt;\\)\n\\(*q_&lt;\\)\n\\(q_=\\)\n\\(q_!\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(*q_&gt;\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\geq}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(*q_&lt;\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\leq}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(*q_=\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{==}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(*q_!\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\neq}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(*q_{\\geq}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(*q_{\\leq}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(*q_{==}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(*q_{\\neq}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\\(q_{\\text{erro}}\\)\n\n\n\n\n\n\nNote que os estados \\(q_&gt;\\) e \\(q_&lt;\\) são marcados como estados de aceitação, permitindo que os operadores simples &gt; e &lt; sejam reconhecidos. Contudo, se a entrada continuar com =, o autômato transita para os estados de aceitação \\(q_{\\geq}\\) ou \\(q_{\\leq}\\), implementando assim a regra da correspondência mais longa. Os estados \\(q_=\\) e \\(q_!\\) não são de aceitação porque = e ! isolados não são operadores relacionais válidos neste contexto - apenas == e != são aceitos.\n\n\n4.4.4 Exercícios 5\n1. Projete o diagrama de transições de um Autômato Finito Determinístico sobre \\(\\Sigma=\\{a,b\\}\\) que aceite todas e somente as strings que contêm a substring aba.\n2. Projete o diagrama de transições de um Autômato Finito Determinístico sobre \\(\\Sigma=\\{0,1\\}\\) que aceite strings que representam números binários cujo valor é múltiplo de \\(3\\). (Dica: Mantenha o resto da divisão por \\(3\\) como estado. \\(v(w0)=2 \\cdot v(w)\\) e \\(v(w1)=2 \\cdot v(w)+1\\)).\n3. Projete a tabela de transições de um Autômato Finito Determinístico que reconhece comentários de uma linha em uma linguagem de programação hipotética. Um comentário começa com // e vai até o final da linha (não nos preocuparemos com o final da linha). O alfabeto é \\(\\Sigma=\\{/,c\\}\\), onde ‘c’ representa qualquer outro caractere.\n4. Desenhe o diagrama de um Autômato Finito Determinístico que aceite strings em \\(\\Sigma=\\{a,b\\}\\) que tenham comprimento ímpar e terminem com ‘a’.\n5. Projete um Autômato Finito Determinístico que aceite apenas as strings cat e car sobre o alfabeto \\(\\Sigma=\\{c,a,t,r\\}\\).",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autômatos Finitos Determinísticos</span>"
    ]
  },
  {
    "objectID": "02-lexico.html#considerações-sobre-implementação-prática",
    "href": "02-lexico.html#considerações-sobre-implementação-prática",
    "title": "4  Autômatos Finitos Determinísticos",
    "section": "4.5 Considerações sobre Implementação Prática",
    "text": "4.5 Considerações sobre Implementação Prática\nEstes três exemplos demonstram alguns aspectos importantes dos Autômatos Finitos Determinísticos na análise léxica:\n\nManutenção de Estado: O exemplo da paridade mostra como informações agregadas podem ser mantidas em estados finitos.\nReconhecimento de Sequências: O exemplo da senha ilustra o reconhecimento determinístico de strings específicas, fundamental para palavras-chave.\nTratamento de Ambiguidades: O exemplo dos operadores demonstra como resolver conflitos de prefixos por meio da regra da correspondência mais longa.\n\nA atenta leitora observará que estes padrões são ubíquos na construção de analisadores léxicos em aplicações práticas, formando os blocos que definem o reconhecimento de identificadores, números, palavras-chave e operadores em linguagens de programação reais.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autômatos Finitos Determinísticos</span>"
    ]
  },
  {
    "objectID": "02-lexico.html#propriedades-matemáticas-dos-autômatos-finitos-determinísticos",
    "href": "02-lexico.html#propriedades-matemáticas-dos-autômatos-finitos-determinísticos",
    "title": "4  Autômatos Finitos Determinísticos",
    "section": "4.6 Propriedades Matemáticas dos Autômatos Finitos Determinísticos",
    "text": "4.6 Propriedades Matemáticas dos Autômatos Finitos Determinísticos\nOs Autômatos Finitos Determinísticos não são apenas modelos computacionais elegantes; eles formam uma estrutura algébrica rica, dotada de propriedades matemáticas que fundamentam sua aplicabilidade prática. A perspicaz leitora descobrirá que estas propriedades não apenas garantem a robustez teórica do modelo, mas também possibilitam otimizações e transformações essenciais para implementações eficientes.\n\n4.6.1 Propriedades de Fechamento\nUma das características notáveis da classe dos Autômatos Finitos Determinísticos é que ela é fechada sob as operações fundamentais da teoria dos conjuntos. Isto significa que, ao combinarmos Autômatos Finitos Determinísticos usando estas operações, o resultado é sempre outro Autômato Finito Determinístico. Esta propriedade tem implicações profundas tanto teóricas quanto práticas, especialmente na construção de analisadores léxicos que precisam reconhecer múltiplos padrões simultaneamente.\nConsidere o seguinte cenário prático: um analisador léxico precisa reconhecer tanto identificadores válidos quanto palavras-chave reservadas. Em vez de construir um único autômato complexo que trate ambos os casos, seria elegante construir dois autômatos simples e depois combiná-los. As propriedades de fechamento garantem que tal composição sempre resulta em outro Autômato Finito Determinístico válido, permitindo uma abordagem modular e sistemática para problemas complexos de reconhecimento.\n\n4.6.1.1 Fechamento sob União\nTeorema (Fechamento sob União): Sejam \\(M_1\\) e \\(M_2\\) dois Autômatos Finitos Determinísticos que reconhecem as linguagens \\(L_1\\) e \\(L_2\\), respectivamente. Então existe um Autômato Finito Determinístico \\(M\\) que reconhece \\(L_1 \\cup L_2\\).\nA intuição por trás da construção da união é elegante: executamos ambos os autômatos simultaneamente sobre a mesma entrada, mantendo o estado atual de cada um em um par ordenado. A atenta leitora pode visualizar isto como duas máquinas trabalhando em paralelo, processando o mesmo fluxo de símbolos. Se ao menos uma delas aceitar a entrada, o autômato combinado também aceita.\nConstrução Formal do Produto: Dados:\n\n\\(M_1 = (Q_1, \\Sigma, \\delta_1, q_{01}, F_1)\\);\n\\(M_2 = (Q_2, \\Sigma, \\delta_2, q_{02}, F_2)\\).\n\nConstruímos \\(M = (Q, \\Sigma, \\delta, q_0, F)\\) no qual:\n\n\\(Q = Q_1 \\times Q_2\\) (produto cartesiano dos conjuntos de estados);\n\\(q_0 = (q_{01}, q_{02})\\) (par dos estados iniciais);\n\\(F = (F_1 \\times Q_2) \\cup (Q_1 \\times F_2)\\) (aceita se pelo menos um dos autômatos aceita);\n\\(\\delta((q_1, q_2), a) = (\\delta_1(q_1, a), \\delta_2(q_2, a))\\) para todo \\((q_1, q_2) \\in Q\\) e \\(a \\in \\Sigma\\).\n\nPara entender esse processo, considere dois Autômatos Finitos Determinísticos sobre o alfabeto \\(\\Sigma = \\{a, b\\}\\):\n\nAutômato \\(M_1\\): Aceita strings que começam com ‘a’\n\n\nEstados: \\(Q_1 = \\{s_0, s_{\\text{aceita}}, s_{\\text{rejeita}}\\}\\)\nEstado inicial: \\(s_0\\)\nEstados finais: \\(F_1 = \\{s_{\\text{aceita}}\\}\\)\nTransições:\n\n\\(\\delta_1(s_0, a) = s_{\\text{aceita}}\\)\n\\(\\delta_1(s_0, b) = s_{\\text{rejeita}}\\)\n\\(\\delta_1(s_{\\text{aceita}}, x) = s_{\\text{aceita}}\\) para \\(x \\in \\{a,b\\}\\)\n\\(\\delta_1(s_{\\text{rejeita}}, x) = s_{\\text{rejeita}}\\) para \\(x \\in \\{a,b\\}\\)\n\n\n\nAutômato \\(M_2\\): Aceita strings com número par de ’b’s\n\n\nEstados: \\(Q_2 = \\{t_{\\text{par}}, t_{\\text{ímpar}}\\}\\)\nEstado inicial: \\(t_{\\text{par}}\\)\nEstados finais: \\(F_2 = \\{t_{\\text{par}}\\}\\)\nTransições:\n\n\\(\\delta_2(t_{\\text{par}}, a) = t_{\\text{par}}\\), \\(\\delta_2(t_{\\text{par}}, b) = t_{\\text{ímpar}}\\)\n\\(\\delta_2(t_{\\text{ímpar}}, a) = t_{\\text{ímpar}}\\), \\(\\delta_2(t_{\\text{ímpar}}, b) = t_{\\text{par}}\\)\n\n\nA construção do autômato união \\(M = M_1 \\cup M_2\\) resulta em:\nEstados: \\(Q = Q_1 \\times Q_2\\) possui \\(\\mid Q_1 \\mid \\times \\mid Q_2 \\mid = 3 \\times 2 = 6\\) estados\nEstados de Aceitação: um estado \\((s_i, t_j)\\) é final se \\(s_i \\in F_1\\) ou \\(t_j \\in F_2\\). Analisando cada par, teremos:\n\n\\((s_0, t_{\\text{par}})\\): aceita (porque \\(t_{\\text{par}} \\in F_2\\));\n\\((s_0, t_{\\text{ímpar}})\\): rejeita;\n\\((s_{\\text{aceita}}, t_{\\text{par}})\\): aceita (ambos aceitam);\n\\((s_{\\text{aceita}}, t_{\\text{ímpar}})\\): aceita (porque \\(s_{\\text{aceita}} \\in F_1\\));\n\\((s_{\\text{rejeita}}, t_{\\text{par}})\\): aceita (porque \\(t_{\\text{par}} \\in F_2\\));\n\\((s_{\\text{rejeita}}, t_{\\text{ímpar}})\\): rejeita.\n\nA Table 4.5 apresenta a tabela de transições completa:\n\n\n\nTable 4.5: Tabela de transições para o autômato união \\(M_1 \\cup M_2\\)\n\n\n\n\n\n\n\n\n\n\nEstado\na\nb\n\n\n\n\n\\(\\rightarrow *(s_0, t_{\\text{par}})\\)\n\\((s_{\\text{aceita}}, t_{\\text{par}})\\)\n\\((s_{\\text{rejeita}}, t_{\\text{ímpar}})\\)\n\n\n\\((s_0, t_{\\text{ímpar}})\\)\n\\((s_{\\text{aceita}}, t_{\\text{ímpar}})\\)\n\\((s_{\\text{rejeita}}, t_{\\text{par}})\\)\n\n\n\\(*(s_{\\text{aceita}}, t_{\\text{par}})\\)\n\\((s_{\\text{aceita}}, t_{\\text{par}})\\)\n\\((s_{\\text{aceita}}, t_{\\text{ímpar}})\\)\n\n\n\\(*(s_{\\text{aceita}}, t_{\\text{ímpar}})\\)\n\\((s_{\\text{aceita}}, t_{\\text{ímpar}})\\)\n\\((s_{\\text{aceita}}, t_{\\text{par}})\\)\n\n\n\\(*(s_{\\text{rejeita}}, t_{\\text{par}})\\)\n\\((s_{\\text{rejeita}}, t_{\\text{par}})\\)\n\\((s_{\\text{rejeita}}, t_{\\text{ímpar}})\\)\n\n\n\\((s_{\\text{rejeita}}, t_{\\text{ímpar}})\\)\n\\((s_{\\text{rejeita}}, t_{\\text{ímpar}})\\)\n\\((s_{\\text{rejeita}}, t_{\\text{par}})\\)\n\n\n\n\n\n\nVerificação: Testemos a string “ba”:\n\n\\((s_0, t_{\\text{par}}) \\xrightarrow{b} (s_{\\text{rejeita}}, t_{\\text{ímpar}}) \\xrightarrow{a} (s_{\\text{rejeita}}, t_{\\text{ímpar}})\\);\nEstado final: \\((s_{\\text{rejeita}}, t_{\\text{ímpar}})\\) não é de aceitação;\n\\(M_1\\) rejeita (não começa com ‘a’), \\(M_2\\) rejeita (número ímpar de ’b’s);\nUnião rejeita corretamente.\n\nA matemática é ótima, contudo, a construção da união pode requerer um pouco de prática para ser compreendida completamente. Para treinar, vamos construir a união de dois autômatos simples, ainda sobre o alfabeto \\(\\Sigma = \\{a, b\\}\\), com uma pequena alteração do exemplo anterior, para solidificar o processo algébrico. Considere os autômatos a seguir:\nAutômato \\(M_1\\): Aceita strings que começam com ‘a’: um autômato que reconhece a linguagem \\(L_1 = \\{a, aa, ab, aaa, aab, aba, aaaa, ...\\}\\). Algebricamente, teremos:\n\nEstados: \\(Q_1 = \\{s_0, s_1, s_2\\}\\) nos quais:\n\n\\(s_0\\): estado inicial (ainda não leu nada);\n\\(s_1\\): leu ‘a’ como primeiro símbolo (aceita);\n\\(s_2\\): leu ‘b’ como primeiro símbolo (rejeita).\n\nEstado inicial: \\(s_0\\).\nEstados finais: \\(F_1 = \\{s_1\\}\\).\nTransições:\n\n\\(\\delta_1(s_0, a) = s_1\\) (primeiro símbolo é ‘a’ → aceita);\n\\(\\delta_1(s_0, b) = s_2\\) (primeiro símbolo é ‘b’ → rejeita);\n\\(\\delta_1(s_1, a) = s_1\\) (já aceitou, continua aceitando);\n\\(\\delta_1(s_1, b) = s_1\\) (já aceitou, continua aceitando);\n\\(\\delta_1(s_2, a) = s_2\\) (já rejeitou, continua rejeitando);\n\\(\\delta_1(s_2, b) = s_2\\) (já rejeitou, continua rejeitando);\n\n\nAutômato \\(M_2\\): Aceita strings com número ímpar de ’b’s: um autômato que reconhece a linguagem \\(L_2 = \\{b, ab, aab, aba, aaab, aababb, ...\\}\\). Que pode ser especificado como:\n\nEstados: \\(Q_2 = \\{t_0, t_1\\}\\) onde:\n\n\\(t_0\\): número par de ’b’s (incluindo zero);\n\\(t_1\\): número ímpar de ’b’s.\n\nEstado inicial: \\(t_0\\).\nEstados finais: \\(F_2 = \\{t_1\\}\\).\nTransições:\n\n\\(\\delta_2(t_0, a) = t_0\\) (‘a’ não afeta a contagem de ’b’s);\n\\(\\delta_2(t_0, b) = t_1\\) (de par para ímpar);\n\\(\\delta_2(t_1, a) = t_1\\) (‘a’ não afeta a contagem);\n\\(\\delta_2(t_1, b) = t_0\\) (de ímpar para par).\n\n\nOs autômatos \\(M_1\\) e \\(M_2\\) podem ser vistos, representados como diagramas de transição na Figure 4.5.\n\n\n\n\n\n\nDiagrama de transição do autômato \\(M_1\\) e \\(M_2\\)\n\n\n\n\nFigure 4.5\n\n\n\nPara construir o autômato união \\(M = M_1 \\cup M_2\\), a esforçada leitora deve seguir os seguintes passos:\n1. Passo 1: Criar o conjunto de estados: o conjunto de estados do autômato união será dado pelo o produto cartesiano \\(Q = Q_1 \\times Q_2\\). Enumerando todos os pares possíveis teremos:\n\n\\((s_0, t_0)\\): \\(M_1\\) no estado inicial e \\(M_2\\) com número par de ’b’s;\n\\((s_0, t_1)\\): \\(M_1\\) no estado inicial e \\(M_2\\) com número ímpar de ’b’s;\n\\((s_1, t_0)\\): \\(M_1\\) aceitando (começou com ‘a’) e \\(M_2\\) com número par de ’b’s;\n\\((s_1, t_1)\\): \\(M_1\\) aceitando e \\(M_2\\) com número ímpar de ’b’s;\n\\((s_2, t_0)\\): \\(M_1\\) rejeitando (começou com ‘b’) e \\(M_2\\) com número par de ’b’s;\n\\((s_2, t_1)\\): \\(M_1\\) rejeitando e \\(M_2\\) com número ímpar de ’b’s.\n\nTotal: \\(6\\) estados \\((3 \\times 2 = 6)\\).\n2. Passo 2: Determinar o estado inicial: o estado inicial é simplesmente o par dos estados iniciais:\n\n\\(q_0 = (s_0, t_0)\\)\n\n3. Passo 3: Determinar os estados finais: para a união, um estado é final se pelo menos um dos autômatos originais estaria em estado final. Para determinar os estados finais, verificamos quais pares \\((s_i, t_j)\\) satisfazem a condição de aceitação. Para cada estado perguntamos:“\\(M_1\\) aceita OU \\(M_2\\) aceita?”\n\n\\((s_0, t_0)\\): \\(s_0 \\notin F_1\\) e \\(t_0 \\notin F_2\\) → NÃO é final;\n\\((s_0, t_1)\\): \\(s_0 \\notin F_1\\) mas \\(t_1 \\in F_2\\) → É FINAL;\n\\((s_1, t_0)\\): \\(s_1 \\in F_1\\) mas \\(t_0 \\notin F_2\\) → É FINAL;\n\\((s_1, t_1)\\): \\(s_1 \\in F_1\\) e \\(t_1 \\in F_2\\) → É FINAL;\n\\((s_2, t_0)\\): \\(s_2 \\notin F_1\\) e \\(t_0 \\notin F_2\\) → NÃO é final;\n\\((s_2, t_1)\\): \\(s_2 \\notin F_1\\) mas \\(t_1 \\in F_2\\) → É FINAL.\n\nAssim, os Estados finais são: \\(F = \\{(s_0, t_1), (s_1, t_0), (s_1, t_1), (s_2, t_1)\\}\\)\n4. Passo 4: Construir a função de transição: para cada estado \\((s_i, t_j)\\) e cada símbolo \\(x \\in \\{a, b\\}\\), calculamos: \\[\\delta((s_i, t_j), x) = (\\delta_1(s_i, x), \\delta_2(t_j, x))\\]\nTransições com ‘a’:\n\n\\(\\delta((s_0, t_0), a) = (\\delta_1(s_0, a), \\delta_2(t_0, a)) = (s_1, t_0)\\);\n\\(\\delta((s_0, t_1), a) = (\\delta_1(s_0, a), \\delta_2(t_1, a)) = (s_1, t_1)\\);\n\\(\\delta((s_1, t_0), a) = (\\delta_1(s_1, a), \\delta_2(t_0, a)) = (s_1, t_0)\\);\n\\(\\delta((s_1, t_1), a) = (\\delta_1(s_1, a), \\delta_2(t_1, a)) = (s_1, t_1)\\);\n\\(\\delta((s_2, t_0), a) = (\\delta_1(s_2, a), \\delta_2(t_0, a)) = (s_2, t_0)\\);\n\\(\\delta((s_2, t_1), a) = (\\delta_1(s_2, a), \\delta_2(t_1, a)) = (s_2, t_1)\\).\n\nTransições com ‘b’:\n\n\\(\\delta((s_0, t_0), b) = (\\delta_1(s_0, b), \\delta_2(t_0, b)) = (s_2, t_1)\\);\n\\(\\delta((s_0, t_1), b) = (\\delta_1(s_0, b), \\delta_2(t_1, b)) = (s_2, t_0)\\);\n\\(\\delta((s_1, t_0), b) = (\\delta_1(s_1, b), \\delta_2(t_0, b)) = (s_1, t_1)\\);\n\\(\\delta((s_1, t_1), b) = (\\delta_1(s_1, b), \\delta_2(t_1, b)) = (s_1, t_0)\\);\n\\(\\delta((s_2, t_0), b) = (\\delta_1(s_2, b), \\delta_2(t_0, b)) = (s_2, t_1)\\);\n\\(\\delta((s_2, t_1), b) = (\\delta_1(s_2, b), \\delta_2(t_1, b)) = (s_2, t_0)\\).\n\n5. Passo 5: Construir a Tabela de Transições da União:\n\n\n\nEstado\na\nb\n\n\n\n\n→(s₀,t₀)\n(s₁,t₀)\n(s₂,t₁)\n\n\n*(s₀,t₁)\n(s₁,t₁)\n(s₂,t₀)\n\n\n*(s₁,t₀)\n(s₁,t₀)\n(s₁,t₁)\n\n\n*(s₁,t₁)\n(s₁,t₁)\n(s₁,t₀)\n\n\n(s₂,t₀)\n(s₂,t₀)\n(s₂,t₁)\n\n\n*(s₂,t₁)\n(s₂,t₁)\n(s₂,t₀)\n\n\n\n6. Passo 6: Depuração: vamos testar algumas strings para verificar que o autômato funciona corretamente:\n\nString ab (começa com ‘a’ e tem um ‘b’, número ímpar de ’b’s):\n\n\n\\((s_0,t_0) \\xrightarrow{a} (s_1,t_0) \\xrightarrow{b} (s_1,t_1)\\) ACEITA;\n\\(M_1\\) aceita (começa com ‘a’);\n\\(M_2\\) aceita (um ‘b’ = ímpar);\nUnião aceita.\n\n\nString b (não começa com ‘a’ mas tem número ímpar de ’b’s):\n\n\n\\((s_0,t_0) \\xrightarrow{b} (s_2,t_1)\\) ACEITA;\n\\(M_1\\) rejeita (começa com ‘b’);\n\\(M_2\\) aceita (um ‘b’ = ímpar);\nUnião aceita (pelo menos um aceita).\n\n\nString bb (não começa com ‘a’ e tem número par de ’b’s):\n\n\n\\((s_0,t_0) \\xrightarrow{b} (s_2,t_1) \\xrightarrow{b} (s_2,t_0)\\) REJEITA;\n\\(M_1\\) rejeita (começa com ‘b’);\n\\(M_2\\) rejeita (dois ’b’s = par);\nUnião rejeita (nenhum aceita).\n\n\nString aaa (começa com ‘a’ mas zero ’b’s):\n\n\n\\((s_0,t_0) \\xrightarrow{a} (s_1,t_0) \\xrightarrow{a} (s_1,t_0) \\xrightarrow{a} (s_1,t_0)\\) ACEITA;\n\\(M_1\\) aceita (começa com ‘a’);\n\\(M_2\\) rejeita (zero ’b’s = par);\nUnião aceita (pelo menos um aceita).\n\nO autômato resultante da união entre \\(M_1\\) e \\(M_2\\) terá estados que representam todas as combinações possíveis de estados de \\(M_1\\) e \\(M_2\\) e pode ser visto na Figure 4.6.\n\n\n\n\n\n\nFigure 4.6\n\n\n\nA atenta leitora deve notar que a construção por produto cartesiano funciona como se estivéssemos executando ambos os autômatos simultaneamente em duas trilhas paralelas. A cada símbolo lido:\n\nAtualizamos ambas as trilhas: cada componente do par de estados evolui independentemente segundo sua própria função de transição;\nMantemos a história completa: o estado \\((s_i, t_j)\\) nos diz exatamente onde cada autômato original estaria após processar a entrada até aquele ponto;\nDecidimos aceitação com lógica OR: Para a união, basta que uma das trilhas aceite para o autômato combinado aceitar.\n\nEste algoritmo de construção em seis passos, com um passo específico para depuração, sempre funciona, servindo como evidência de que a classe das linguagens regulares é fechada sob união.\n\n4.6.1.1.1 Exercícios 6\n1. Considere dois Autômatos Finitos Determinísticos sobre \\(\\Sigma=\\{a,b\\}\\): \\(M_1\\) aceita strings que contêm aa e \\(M_2\\) aceita strings de comprimento ímpar. Quantos estados terá o autômato produto \\(M = M_1 \\cup M_2\\) se \\(\\mid Q_1 \\mid = 3\\) e \\(\\mid Q_2 \\mid = 2\\)?\n2. No autômato união, um estado \\((p, q)\\) é de aceitação quando \\(p \\in F_1\\) ou \\(q \\in F_2\\). Se modificássemos a condição para \\(p \\in F_1\\) e \\(q \\in F_2\\), que operação estaríamos realizando?\n3. Construa a tabela de transições completa para a união de: \\(M_1\\) que aceita apenas a e \\(M_2\\) que aceita apenas b, sobre \\(\\Sigma = \\{a, b\\}\\).\n4. Considere dois autômatos sobre \\(\\Sigma = \\{0, 1\\}\\): \\(M_1\\) aceita strings que terminam em ‘01’ e \\(M_2\\) aceita strings com número par de ’1’s. Construa a tabela de transições para o autômato união \\(M_1 \\cup M_2\\).\n5. Dados dois autômatos binários \\(M_1\\) e \\(M_2\\) onde \\(M_1\\) aceita strings que começam com ‘1’ (2 estados) e \\(M_2\\) aceita strings de comprimento par (2 estados). Construa o passo a passo completo para determinar o autômato união \\(M_1 \\cup M_2\\) sobre \\(\\Sigma = \\{0, 1\\}\\).\n\n\n\n4.6.1.2 Fechamento sob Interseção\nTeorema (Fechamento sob Interseção): Sejam \\(M_1\\) e \\(M_2\\) dois Autômatos Finitos Determinísticos que reconhecem as linguagens \\(L_1\\) e \\(L_2\\), respectivamente. Então existe um Autômato Finito Determinístico \\(M\\) que reconhece \\(L_1 \\cap L_2\\).\nA elegante leitora notará que a construção da interseção é notavelmente similar à da união. A diferença crucial reside apenas na definição dos estados finais: enquanto a união implementa uma lógica OU, a interseção implementa uma lógica E.\nConstrução Formal: Utilizamos a mesma estrutura de produto cartesiano, mas com:\n\\[F = F_1 \\times F_2\\]\nOu seja, um estado \\((q_1, q_2)\\) é final se, e somente se, ambos \\(q_1 \\in F_1\\) e \\(q_2 \\in F_2\\).\nVamos construir um exemplo mais elaborado para solidificar o entendimento da construção de interseção. Considere os autômatos \\(M_1\\) e \\(M_2\\) descritos a seguir:\nAutômato \\(M_1\\): aceita strings que começam com ‘a’. A linguagem formal é:\n\\[ L_1 = \\{ w \\in \\{a, b\\}^* \\mid w = a \\cdot x \\text{ para algum } x \\in \\{a, b\\}^* \\} \\]\n\nEstados: \\(Q_1 = \\{s_0, s_1, s_2\\}\\) onde:\n\n\\(s_0\\): estado inicial (ainda não leu nada)\n\\(s_1\\): leu ‘a’ como primeiro símbolo (aceita)\n\\(s_2\\): leu ‘b’ como primeiro símbolo (rejeita)\n\nEstado inicial: \\(s_0\\)\nEstados finais: \\(F_1 = \\{s_1\\}\\)\nTransições:\n\n\\(\\delta_1(s_0, a) = s_1\\), \\(\\delta_1(s_0, b) = s_2\\)\n\\(\\delta_1(s_1, a) = s_1\\), \\(\\delta_1(s_1, b) = s_1\\)\n\\(\\delta_1(s_2, a) = s_2\\), \\(\\delta_1(s_2, b) = s_2\\)\n\n\nAutômato \\(M_2\\): aceita strings com número par de ’b’s. A linguagem formal é:\n\\[ L_2 = \\{ w \\in \\{a, b\\}^* \\mid \\#_b(w) \\equiv 0 \\pmod{2} \\} \\]\nno qual \\(\\#_b(w)\\) denota o número de ’b’s na string \\(w\\).\n\nEstados: \\(Q_2 = \\{t_{par}, t_{ímpar}\\}\\) onde:\n\n\\(t_{par}\\): número par de ’b’s (incluindo zero)\n\\(t_{ímpar}\\): número ímpar de ’b’s\n\nEstado inicial: \\(t_{par}\\)\nEstados finais: \\(F_2 = \\{t_{par}\\}\\)\nTransições:\n\n\\(\\delta_2(t_{par}, a) = t_{par}\\), \\(\\delta_2(t_{par}, b) = t_{ímpar}\\)\n\\(\\delta_2(t_{ímpar}, a) = t_{ímpar}\\), \\(\\delta_2(t_{ímpar}, b) = t_{par}\\)\n\n\nPasso a Passo para a Construção do Autômato Interseção:\nPasso 1: Estados: \\(Q = Q_1 \\times Q_2 = \\{(s_0, t_{par}), (s_0, t_{ímpar}), (s_1, t_{par}), (s_1, t_{ímpar}), (s_2, t_{par}), (s_2, t_{ímpar})\\}\\)\nPasso 2: Estado inicial: \\(q_0 = (s_0, t_{par})\\)\nPasso 3: Estados finais: Para a interseção, um estado é final apenas se ambos os componentes estão em estados finais: - \\(F = F_1 \\times F_2 = \\{s_1\\} \\times \\{t_{par}\\} = \\{(s_1, t_{par})\\}\\)\nPasso 4: Tabela de Transições:\n\n\n\nEstado\na\nb\n\n\n\n\n\\(\\rightarrow (s_0, t_{par})\\)\n\\((s_1, t_{par})\\)\n\\((s_2, t_{ímpar})\\)\n\n\n\\((s_0, t_{ímpar})\\)\n\\((s_1, t_{ímpar})\\)\n\\((s_2, t_{par})\\)\n\n\n\\(*(s_1, t_{par})\\)\n\\((s_1, t_{par})\\)\n\\((s_1, t_{ímpar})\\)\n\n\n\\((s_1, t_{ímpar})\\)\n\\((s_1, t_{ímpar})\\)\n\\((s_1, t_{par})\\)\n\n\n\\((s_2, t_{par})\\)\n\\((s_2, t_{par})\\)\n\\((s_2, t_{ímpar})\\)\n\n\n\\((s_2, t_{ímpar})\\)\n\\((s_2, t_{ímpar})\\)\n\\((s_2, t_{par})\\)\n\n\n\nPasso 5: Verificação: Vamos testar algumas strings:\nString “aa” (começa com ‘a’ e tem zero ‘b’s - par): - \\((s_0, t_{par}) \\xrightarrow{a} (s_1, t_{par}) \\xrightarrow{a} (s_1, t_{par})\\) ACEITA; - \\(M_1\\) aceita (começa com ’a’); - \\(M_2\\) aceita (zero ’b’s = par); - Interseção aceita (ambos aceitam).\nString “ab” (começa com ‘a’ mas tem um ‘b’ - ímpar): - \\((s_0, t_{par}) \\xrightarrow{a} (s_1, t_{par}) \\xrightarrow{b} (s_1, t_{ímpar})\\) REJEITA; - \\(M_1\\) aceita (começa com ‘a’); - \\(M_2\\) rejeita (um ‘b’ = ímpar); - Interseção rejeita (nem todos aceitam).\nString “abb” (começa com ‘a’ e tem dois ’b’s - par): - \\((s_0, t_{par}) \\xrightarrow{a} (s_1, t_{par}) \\xrightarrow{b} (s_1, t_{ímpar}) \\xrightarrow{b} (s_1, t_{par})\\) ACEITA\nO autômato da interseção aceita apenas strings que começam com ‘a’ E têm número par de ’b’s.\n\n4.6.1.2.1 Exercícios 7\n1. Considere dois autômatos sobre \\(\\Sigma = \\{0, 1\\}\\): - \\(M_1\\) aceita strings que terminam em ‘10’ - \\(M_2\\) aceita strings com número ímpar de ’0’s\nConstrua o conjunto de estados finais para \\(M_1 \\cap M_2\\) e determine a linguagem aceita pela interseção.\n2. É possível que \\(L(M_1 \\cup M_2) = L(M_1 \\cap M_2)\\)? Em que condição isso ocorreria?\n3. Se \\(F_1 = Q_1\\) (todos os estados de \\(M_1\\) são finais), o que podemos afirmar sobre \\(L(M_1 \\cap M_2)\\) em relação a \\(L(M_2)\\)?\n4. Considere \\(M_1\\) que aceita strings terminadas em ‘ab’ e \\(M_2\\) que aceita strings com número par de ’a’s. Construa o autômato interseção \\(M_1 \\cap M_2\\) e determine sua linguagem.\n5. Dados três autômatos \\(M_1\\), \\(M_2\\) e \\(M_3\\), tal que:\n\n\\(M_1\\) aceita strings que começam com ‘a’ (3 estados)\n\\(M_2\\) aceita strings de comprimento múltiplo de 3 (3 estados)\n\n\\(M_3\\) aceita strings que contêm ‘bb’ (3 estados)\n\nDetermine quantos estados teria \\((M_1 \\cup M_2) \\cap M_3\\) e construa o passo a passo completo desta operação composta.\n\n\n\n4.6.1.3 Fechamento sob Complemento\nTeorema (Fechamento sob Complemento): Seja \\(M\\) um Autômato Finito Determinístico que reconhece a linguagem \\(L\\). Então existe um Autômato Finito Determinístico \\(M'\\) que reconhece \\(\\overline{L} = \\Sigma^* - L\\).\nA construção do complemento é surpreendentemente elegante em sua simplicidade. Para a atenta leitora que acompanhou as construções anteriores, esta parecerá quase trivial: simplesmente invertemos quais estados são finais.\nConstrução Formal: Dado \\(M = (Q, \\Sigma, \\delta, q_0, F)\\), construímos:\n\\[M' = (Q, \\Sigma, \\delta, q_0, Q - F)\\]\nCondição Crítica: Esta construção requer que o autômato seja completo. Se não for, devemos primeiro adicionar um estado de erro \\(q_{\\text{erro}}\\) com:\n\\[\\forall a \\in \\Sigma: \\delta(q_{\\text{erro}}, a) = q_{\\text{erro}}\\]\nExemplo Ilustrativo: Complemento de “strings com número par de ’a’s”\nAutômato original \\(M\\):\n\n\n\nTable 4.6: Autômato que aceita número par de ’a’s\n\n\n\n\n\n\n\n\n\n\nEstado\na\nb\n\n\n\n\n\\(\\rightarrow *q_{\\text{par}}\\)\n\\(q_{\\text{ímpar}}\\)\n\\(q_{\\text{par}}\\)\n\n\n\\(q_{\\text{ímpar}}\\)\n\\(q_{\\text{par}}\\)\n\\(q_{\\text{ímpar}}\\)\n\n\n\n\n\n\nAutômato complemento \\(\\overline{M}\\):\n\n\n\nTable 4.7: Autômato que aceita número ímpar de ’a’s\n\n\n\n\n\nEstado\na\nb\n\n\n\n\n\\(\\rightarrow q_{\\text{par}}\\)\n\\(q_{\\text{ímpar}}\\)\n\\(q_{\\text{par}}\\)\n\n\n\\(*q_{\\text{ímpar}}\\)\n\\(q_{\\text{par}}\\)\n\\(q_{\\text{ímpar}}\\)\n\n\n\n\n\n\nA perspicaz leitora observará que apenas trocamos os estados finais: \\(F' = \\{q_{\\text{ímpar}}\\}\\).\nPropriedades Algébricas: O fechamento sob complemento, combinado com união e interseção, estabelece que as linguagens regulares formam uma álgebra booleana:\n\nLeis de De Morgan: \\(\\overline{L_1 \\cup L_2} = \\overline{L_1} \\cap \\overline{L_2}\\)\nDupla Negação: \\(\\overline{\\overline{L}} = L\\)\nDiferença: \\(L_1 - L_2 = L_1 \\cap \\overline{L_2}\\)\n\n\n4.6.1.3.1 Exercícios 8\n1. Considere o Autômato Finito Determinístico \\(M\\) com \\(Q = \\{q_0, q_1\\}\\), \\(F = \\{q_0\\}\\), \\(\\delta(q_0, a) = q_1\\), \\(\\delta(q_0, b) = q_0\\), \\(\\delta(q_1, a) = q_0\\), \\(\\delta(q_1, b) = q_1\\). Construa a tabela de transições de \\(\\overline{M}\\).\n2. Por que a completude é essencial para a construção do complemento? Dê um exemplo no qual a construção falharia sem completude.\n3. Prove usando as propriedades de fechamento que se \\(L_1\\) e \\(L_2\\) são regulares, então \\(L_1 - L_2\\) também é regular.\n4. Se \\(L\\) é regular e \\(\\overline{L} = \\emptyset\\), o que podemos concluir sobre \\(L\\)?\n5. Usando as leis de De Morgan, expresse \\(\\overline{L_1 \\cap L_2 \\cap L_3}\\) em termos de uniões e complementos individuais.\n\n\n\n\n4.6.2 Implicações Práticas das Propriedades de Fechamento\nAs propriedades de fechamento que a dedicada leitora acaba de estudar não são meras curiosidades matemáticas. Elas fundamentam técnicas essenciais na construção de compiladores e processadores de texto:\n\nComposição Modular: Permite construir reconhecedores complexos a partir de componentes simples e bem testados.\nOtimização de Consultas: Em sistemas de busca, a interseção permite refinar resultados, enquanto a união expande o escopo.\nAnálise Léxica Incremental: Novos padrões podem ser adicionados a um analisador existente por meio de união, sem reconstruir todo o autômato.\nVerificação de Propriedades: Para verificar se dois autômatos são equivalentes, basta testar se \\((L_1 - L_2) \\cup (L_2 - L_1) = \\emptyset\\).\n\nA compreensão profunda destas propriedades capacita a engenheira a fazer escolhas arquiteturais informadas, sabendo que a modularidade não compromete a eficiência ou a correção do sistema final.### Minimização de Estados\nUm dos resultados mais importantes e práticos da teoria dos Autômatos Finitos Determinísticos é que toda linguagem regular possui um único Autômato Finito Determinístico mínimo (a menos de renomeação de estados). Este resultado não apenas tem valor teórico, mas permite otimizações significativas em implementações práticas.\n\n4.6.2.1 O Conceito de Estados Equivalentes\nDois estados \\(p\\) e \\(q\\) são equivalentes (denotado \\(p \\equiv q\\)) se, para toda string \\(w \\in \\Sigma^*\\):\n\\[\\delta^*(p, w) \\in F \\iff \\delta^*(q, w) \\in F\\]\nIntuitivamente, dois estados são equivalentes se, a partir deles, o autômato aceita exatamente as mesmas continuações. Esta relação de equivalência particiona o conjunto de estados em classes de equivalência.\n\n\n4.6.2.2 O Algoritmo de Minimização por Refinamento de Partições\nO algoritmo clássico para minimização de Autômatos Finitos Determinísticos opera refinando iterativamente uma partição dos estados até alcançar o ponto fixo:\nAlgoritmo de Minimização:\n1. Inicialização: Particione Q em dois conjuntos:\n   - P₀ = F (estados de aceitação)\n   - P₁ = Q - F (estados de não-aceitação)\n\n2. Refinamento: Enquanto a partição mudar:\n   Para cada classe de equivalência P na partição atual:\n     Para cada símbolo a ∈ Σ:\n       Divida P baseado em δ(q, a) para q ∈ P\n       \n3. Construção: Cada classe de equivalência torna-se\n   um estado do Autômato Finito Determinístico mínimo\nExemplo de Minimização: Considere um Autômato Finito Determinístico com estados redundantes que reconhece \\((a|b)*abb\\):\nEstados originais: \\(\\{q_0, q_1, q_2, q_3, q_4, q_5\\}\\) nos quais alguns estados são equivalentes.\nApós aplicar o algoritmo: 1. Partição inicial: \\(\\{\\{q_3\\}, \\{q_0, q_1, q_2, q_4, q_5\\}\\}\\) 2. Após refinamentos: \\(\\{\\{q_3\\}, \\{q_2\\}, \\{q_1, q_4\\}, \\{q_0, q_5\\}\\}\\) 3. Autômato Finito Determinístico mínimo tem apenas 4 estados ao invés de 6\n\n\n\n4.6.3 Exercícios 9\n1. Considere o alfabeto \\(\\Sigma = \\{0, 1, +, -, *, /, (, )\\}\\) representando tokens simplificados de expressões aritméticas binárias. Sejam os autômatos:\n\n\\(M_1\\): aceita strings que começam com ‘(’ e terminam com ‘)’;\n\\(M_2\\): aceita strings que contêm pelo menos um operador aritmético (+, -, * ou /);\n\\(M_3\\): aceita strings de comprimento par.\n\nConstrua o autômato que reconhece \\((M_1 \\cap M_2) \\cup \\overline{M_3}\\) e determine três strings aceitas e três rejeitadas.\n2. Sobre o alfabeto \\(\\Sigma = \\{a, b, \\_, 0, 1\\}\\) representando caracteres válidos em identificadores, considere:\n\n\\(M_{id}\\): aceita strings que começam com letra (a ou b) ou underscore, seguidos de qualquer combinação de letras, underscore ou dígitos;\n\\(M_{kw}\\): aceita exatamente as strings {“a”, “ab”, “b0”, “b1”} (palavras-chave).\n\nConstrua o autômato \\(M_{id} - M_{kw} = M_{id} \\cap \\overline{M_{kw}}\\) que aceita identificadores válidos que não são palavras-chave. Quantos estados tem o autômato mínimo resultante?\n3. Considere \\(\\Sigma = \\{/, *, \", \\text{outro}\\}\\) onde ‘outro’ representa qualquer caractere diferente dos anteriores. Sejam:\n\n\\(M_{comment}\\): aceita strings da forma /* … */ (comentários de bloco);\n\\(M_{string}\\): aceita strings da forma ” … ” (strings literais);\n\\(M_{nested}\\): aceita strings que contêm pelo menos um ’/*’ dentro de aspas.\n\nProve que \\(L(M_{comment}) \\cap L(M_{string}) = \\emptyset\\) e construa \\(M = (M_{comment} \\cup M_{string}) \\cap \\overline{M_{nested}}\\).\n4. Sobre \\(\\Sigma = \\{0, 1, \\&, |, !, (, )\\}\\) representando expressões booleanas:\n\n\\(M_{balanced}\\): aceita strings com parênteses balanceados (mesmo número de ‘(’ e ‘)’);\n\\(M_{op}\\): aceita strings onde todo operador binário (&, |) é precedido e seguido por 0, 1, ou );\n\\(M_{not}\\): aceita strings onde todo ‘!’ é seguido por 0, 1, ou (.\n\nConstrua passo a passo o autômato \\((M_{balanced} \\cap M_{op} \\cap M_{not}) \\cup \\overline{(M_{op} \\cup M_{not})}\\). Esta construção resulta em um autômato que aceita que tipo de strings?\n5. Dados três autômatos sobre \\(\\Sigma = \\{&lt;, &gt;, =, !, 0, 1\\}\\):\n\n\\(M_1\\) com 4 estados, reconhece operadores relacionais válidos (&lt;, &gt;, &lt;=, &gt;=, ==, !=);\n\\(M_2\\) com 3 estados, reconhece strings que começam com operador;\n\\(M_3\\) com 2 estados, reconhece strings de comprimento ímpar.\n\n\nDetermine o número máximo de estados do autômato \\((M_1 \\cup M_2) \\cap \\overline{M_3}\\) antes da minimização.\nSe \\(L(M_1) \\subseteq L(M_2)\\), simplifique a expressão \\((M_1 \\cap M_2) \\cup (\\overline{M_1} \\cap M_3)\\) e justifique algebricamente.\nProve que \\(\\overline{M_1 \\cup M_2 \\cup M_3} = \\overline{M_1} \\cap \\overline{M_2} \\cap \\overline{M_3}\\) usando as leis de De Morgan e construa o autômato resultante.\n\n\n\n4.6.4 A Relação de Myhill-Nerode\nA relação de Myhill-Nerode fornece uma caracterização fundamental das linguagens regulares e estabelece uma conexão profunda entre a estrutura algébrica de uma linguagem e o autômato mínimo que a reconhece.\n\n4.6.4.1 Definição da Relação\nDada uma linguagem \\(L \\subseteq \\Sigma^*\\), definimos a relação de equivalência de Myhill-Nerode sobre \\(\\Sigma^*\\):\n\\[x \\equiv_L y \\iff \\forall z \\in \\Sigma^* : xz \\in L \\leftrightarrow yz \\in L\\]\nDuas strings \\(x\\) e \\(y\\) são equivalentes se elas são indistinguíveis com respeito a \\(L\\): qualquer sufixo \\(z\\) que, concatenado com \\(x\\), produza uma string em \\(L\\), também produzirá uma string em \\(L\\) quando concatenado com \\(y\\), e vice-versa.\n\n\n4.6.4.2 O Teorema de Myhill-Nerode\nTeorema (Myhill-Nerode): Uma linguagem \\(L\\) é regular se, e somente se, a relação \\(\\equiv_L\\) tem um número finito de classes de equivalência. Além disso, o número de estados do Autômato Finito Determinístico mínimo para \\(L\\) é exatamente igual ao número de classes de equivalência de \\(\\equiv_L\\).\nEste teorema tem implicações profundas:\n\nCaracterização Algébrica: Fornece uma condição necessária e suficiente para regularidade sem mencionar autômatos ou expressões regulares.\nLimite Inferior: Estabelece o número mínimo de estados necessários para reconhecer uma linguagem.\nTécnica de Prova: Oferece um método alternativo para provar que uma linguagem não é regular.\n\nExemplo de Aplicação: Para a linguagem \\(L = \\{a^nb^n | n \\geq 0\\}\\):\nAs strings \\(\\epsilon, a, aa, aaa, ...\\) estão todas em classes de equivalência distintas porque: - \\(a^i \\cdot b^i \\in L\\) mas \\(a^j \\cdot b^i \\notin L\\) para \\(i \\neq j\\)\nComo existem infinitas classes de equivalência, \\(L\\) não é regular. Esta é uma prova alternativa ao Lema do Bombeamento!\n\n\n\n4.6.5 Equivalência entre Modelos\nUm dos resultados fundamentais da teoria das linguagens formais é o Teorema de Kleene, que estabelece a equivalência entre três formalismos aparentemente distintos.\n\n4.6.5.1 O Teorema de Kleene\nTeorema (Kleene): As seguintes classes de linguagens são idênticas: 1. Linguagens reconhecidas por Autômatos Finitos Determinísticos 2. Linguagens reconhecidas por Autômatos Finitos Não-Determinísticos 3. Linguagens denotadas por Expressões Regulares\nA demonstração deste teorema envolve três construções algorítmicas:\n1. Expressão Regular → Autômato Finito Determinístico (Construção de Thompson): Para cada operador da expressão regular, existe uma construção sistemática: - Base: \\(a \\in \\Sigma\\) produz um Autômato Finito Determinístico de dois estados; - União: \\(r|s\\) combina Autômatos Finitos Determinísticos com transições-\\(\\epsilon\\); - Concatenação: \\(rs\\) conecta Autômatos Finitos Determinísticos em sequência; - Fechamento: \\(r^*\\) adiciona ciclo com transições-\\(\\epsilon\\).\n2. Autômato Finito Determinístico (Construção de Subconjuntos): O algoritmo de subconjuntos constrói um Autômato Finito Determinístico no qual cada estado representa um conjunto de estados do Autômato Finito Determinístico original. Embora potencialmente exponencial, garante determinismo.\n3. Autômato Finito Determinístico → Expressão Regular (Eliminação de Estados): Remove sistematicamente estados do Autômato Finito Determinístico, substituindo-os por expressões regulares nas transições, até restar apenas uma expressão entre o estado inicial e os finais.\n\n\n\n4.6.6 Teste de Equivalência entre Autômatos Finitos Determinísticos\nDeterminar se dois Autômatos Finitos Determinísticos reconhecem a mesma linguagem é um problema fundamental com aplicações práticas importantes, especialmente na otimização de compiladores.\n\n4.6.6.1 Algoritmo de Teste de Equivalência\nMétodo 1: Via Minimização: 1. Minimize ambos os Autômatos Finitos Determinísticos 2. Verifique isomorfismo entre os Autômatos Finitos Determinísticos mínimos\nMétodo 2: Via Construção do Produto: 1. Construa Autômato Finito Determinístico para \\((L_1 - L_2) \\cup (L_2 - L_1)\\) 2. Verifique se a linguagem resultante é vazia\nComplexidade: a complexidade do teste de equivalência entre dois Autômatos Finitos Determinísticos (\\(M_1\\) e \\(M_2\\)) depende do método utilizado.\n\nO método via minimização tem sua complexidade dominada pelo algoritmo de minimização. Utilizando o algoritmo de Hopcroft, a complexidade é de \\(O(k \\cdot n \\log n)\\), na qual \\(n\\) é o número de estados e \\(k\\) é o tamanho do alfabeto. Se o alfabeto é tratado como uma constante, a complexidade é frequentemente citada como \\(O(n \\log n)\\).\nO método via construção do produto, que verifica se a linguagem \\((L_1 \\setminus L_2) \\cup (L_2 \\setminus L_1)\\) é vazia, possui uma complexidade de \\(O(n_1 \\cdot n_2)\\), na qual \\(n_1\\) é o número de estados de \\(M_1\\) e \\(n_2\\) é o número de estados de \\(M_2\\).\n\n\n\n\n4.6.7 Implicações Práticas das Propriedades\nA diligente leitora observará que estas propriedades matemáticas têm consequências diretas na engenharia de compiladores:\n\nComposição Modular: As propriedades de fechamento permitem construir analisadores léxicos complexos a partir de componentes simples, sabendo que o resultado permanecerá tratável.\nOtimização Garantida: A minimização garante o menor autômato possível, otimizando tanto memória quanto tempo de execução.\nVerificação Formal: O teste de equivalência permite verificar se otimizações preservam a semântica.\nGeração Automática: A equivalência entre formalismos permite escolher a representação mais conveniente para cada fase do desenvolvimento.\n\nEstas propriedades fundamentais estabelecem os Autômatos Finitos Determinísticos não apenas como um modelo teórico elegante, mas como uma ferramenta prática poderosa para o processamento eficiente de linguagens. A compreensão profunda destas propriedades capacita a engenheira de software a fazer escolhas informadas sobre representações, otimizações e implementações em sistemas reais de análise léxica.\n\n\n4.6.8 Exercícios 10\n1.: Considere a linguagem \\(L = \\{w \\in \\{0,1\\}^* \\mid w \\text{ contém } 00 \\text{ como substring}\\}\\).\n\nDetermine todas as classes de equivalência da relação \\(\\equiv_L\\).\nPara cada classe, forneça um representante e explique por que strings dessa classe são equivalentes.\nConstrua o Autômato Finito Determinístico mínimo para \\(L\\) usando as classes de equivalência encontradas.\nVerifique que o número de estados do Autômato Finito Determinístico mínimo corresponde ao número de classes.\n\n2.: Use a relação de Myhill-Nerode para provar que a linguagem \\(L = \\{0^i1^j \\mid i &gt; j \\geq 0\\}\\) não é regular.\nDica: Considere as strings \\(0^n\\) para diferentes valores de \\(n\\) e mostre que estão em classes distintas.\n3.:Considere o Autômato Finito Determinístico \\(M = (\\{q_0, q_1, q_2\\}, \\{a, b\\}, \\delta, q_0, \\{q_2\\})\\) com transições:\n\n\\(\\delta(q_0, a) = q_1\\), \\(\\delta(q_0, b) = q_0\\);\n\\(\\delta(q_1, a) = q_1\\), \\(\\delta(q_1, b) = q_2\\);\n\\(\\delta(q_2, a) = q_2\\), \\(\\delta(q_2, b) = q_2\\).\n\nUse o método de eliminação de estados para encontrar a expressão regular equivalente. Mostre cada passo da eliminação.\n4.: Dados dois Autômatos Finitos Determinísticos sobre \\(\\Sigma = \\{a, b\\}\\):\nAutômato Finito Determinístico \\(M_1\\): Aceita strings com número par de \\(a\\)’s - Estados: \\(\\{p_0, p_1\\}\\) - Estado inicial: \\(p_0\\) - Estados finais: \\(\\{p_0\\}\\) - \\(\\delta_1(p_0, a) = p_1\\), \\(\\delta_1(p_0, b) = p_0\\) - \\(\\delta_1(p_1, a) = p_0\\), \\(\\delta_1(p_1, b) = p_1\\)\nAutômato Finito Determinístico \\(M_2\\): Aceita strings onde a posição de cada \\(a\\) é ímpar (primeira posição = 1) - Estados: \\(\\{q_{par}, q_{impar}, q_{aceita}, q_{rejeita}\\}\\) - Estado inicial: \\(q_{par}\\) - Estados finais: \\(\\{q_{par}, q_{aceita}\\}\\)\n\nDetermine se \\(L(M_1) = L(M_2)\\) usando o método do produto.\nVerifique sua resposta testando as strings: \\(\\epsilon\\), \\(a\\), \\(b\\), \\(aa\\), \\(ab\\), \\(ba\\), \\(aba\\).\n\n5. Considere um Autômato Finito Determinístico \\(M\\) com 8 estados que reconhece strings sobre \\(\\{0, 1\\}\\) terminadas em \\(01\\).\n\nAplique o algoritmo de minimização de Hopcroft para encontrar o Autômato Finito Determinístico mínimo.\nCalcule a complexidade temporal da minimização considerando \\(|\\Sigma| = 2\\) e \\(n = 8\\).\nSe tivéssemos dois Autômatos Finitos Determinísticos com 8 e 10 estados respectivamente, compare a complexidade de testar equivalência via minimização versus via construção do produto.\n\n6. Você está projetando um analisador léxico que deve reconhecer três tipos de tokens:\n\nInteiros: sequências de dígitos (0-9);\nIdentificadores: começam com letra, seguidos de letras ou dígitos;\nOperadores: ++, --, +=, -=.\n\n\nConstrua Autômatos Finitos Determinísticos individuais para cada tipo.\nUse as propriedades de fechamento para criar um único Autômato Finito Determinístico que reconheça qualquer um dos três tipos.\nDiscuta como você implementaria a distinção entre os tipos de tokens no Autômato Finito Determinístico combinado.\n\n\n\n4.6.9 Limitações dos Autômatos Finitos\nApesar de sua grande aplicabilidade na análise léxica e no reconhecimento de padrões, os Autômatos Finitos Determinísticos possuem uma limitação fundamental que reside na própria natureza de sua memória: ela é finita. Um Autômato Finito Determinístico só consegue se lembrar de informações por meio do estado em que se encontra. Como o conjunto de estados \\(Q\\) é finito, a capacidade de memorização da máquina é limitada.\nEsta limitação impede que os Autômatos Finitos Determinísticos reconheçam linguagens que exigem uma memória ilimitada ou a capacidade de contar sem um teto predefinido. O exemplo mais clássico de uma linguagem que um Autômato Finito Determinístico não pode reconhecer é a linguagem de parênteses balanceados ou, de forma análoga, a linguagem \\(L\\):\n\\[\nL = \\{0^n1^n \\mid n \\ge 0\\}\n\\]\nEsta linguagem consiste em qualquer número \\(n\\) de símbolos ‘0’, seguido pela mesma quantidade \\(n\\) de símbolos ‘1’. Alguns exemplos de strings em \\(L\\) são \\(\\epsilon\\) (a string vazia, com \\(n=0\\)), \\(01\\), \\(0011\\), \\(000111\\), e assim por diante.\nPara que um autômato pudesse reconhecer esta linguagem, ele precisaria ler todos os ’0’s, contar quantos foram lidos, e então verificar se a quantidade de ’1’s subsequentes é exatamente a mesma. Se \\(n\\) pode ser um número arbitrariamente grande, a máquina precisaria de um número infinito de estados para memorizar cada contagem possível de ’0’s, o que viola a definição de um autômato finito.\nEssa incapacidade de lidar com estruturas de aninhamento ou contagens ilimitadas é a razão pela qual modelos computacionais mais poderosos, como os Autômatos de Pilha, são necessários para as fases seguintes da compilação, como a análise sintática.\n\n\n\n\n[1] AHO, A. V. et al. Compilers: Principles, techniques, and tools. 2nd. ed. [s.l.] Pearson; Addison-Wesley, 2007.",
    "crumbs": [
      "Analisadores Léxicos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Autômatos Finitos Determinísticos</span>"
    ]
  },
  {
    "objectID": "04-Gramaticas.html",
    "href": "04-Gramaticas.html",
    "title": "5  Linguagens Livres de Contexto",
    "section": "",
    "text": "5.1 Hierarquia de Chomsky: História e Contexto\nA Hierarquia de Chomsky representa um dos marcos mais elegantes da ciência do século XX, estabelecendo pontes entre linguística, matemática e ciência da computação. Desenvolvida por Noam Chomsky entre 1956 e 1959, esta classificação não apenas transformou os estudos linguísticos de uma disciplina descritiva em uma ciência formal rigorosa, mas também forneceu as bases teóricas para a construção de compiladores e o processamento computacional de linguagens. A hierarquia emergiu de uma convergência única de fatores históricos: a revolução cognitiva dos anos 1950, o desenvolvimento da teoria da computação, e a genialidade de um jovem linguista que soube sintetizar influências matemáticas diversas em uma estrutura unificada que perdura até hoje.\nA história da Hierarquia de Chomsky começa com um encontro transformador que ocorreu em 1947, quando Chomsky, então com 19 anos e considerando abandonar a universidade, conheceu Zellig S. Harris na Universidade da Pensilvânia. Harris era um dos fundadores da linguística estrutural americana e tinha estabelecido o primeiro departamento moderno de linguística dos Estados Unidos em 1946. Sob a orientação de Harris, Chomsky também estudou matemática com Nathan Fine em Harvard. Essas influências foram cruciais para moldar sua abordagem aos sistemas formais e metodologia científica. Sua tese de mestrado de 1951, A Morfofonêmica do Hebraico Moderno, e especialmente seu trabalho em A Estrutura Lógica da Teoria Linguística (LSLT), escrito enquanto era fellow júnior em Harvard (1951-55), começaram a transformar a abordagem estrutural de Harris em algo inteiramente novo.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linguagens Livres de Contexto</span>"
    ]
  },
  {
    "objectID": "04-Gramaticas.html#hierarquia-de-chomsky-história-e-contexto",
    "href": "04-Gramaticas.html#hierarquia-de-chomsky-história-e-contexto",
    "title": "5  Linguagens Livres de Contexto",
    "section": "",
    "text": "5.1.1 A revolução cognitiva e o ambiente do MIT\nOs anos 1950 representavam um período de fermentação intelectual que se tornaria conhecido como a Revolução Cognitiva. George Miller, uma das figuras-chave, datou o início dessa revolução em 11 de setembro de 1956, quando pesquisadores de psicologia experimental, ciência da computação e linguística teórica apresentaram trabalhos sobre temas relacionados à ciência cognitiva em uma reunião do Special Interest Group in Information Theory no MIT.\nDurante esse período, vários paradigmas dominantes estavam sendo desafiados. O behaviorismo era a doutrina dominante em psicologia e linguística, enfatizando organização de dados e classificação taxonômica enquanto rejeitava o estudo de estados mentais internos. A linguística estrutural, liderada por figuras como Leonard Bloomfield, tratava a linguagem como um fenômeno social estudado por meio de análise de corpus.\nA Hierarquia de Chomsky emergiu de uma convergência sofisticada de lógica matemática, teoria dos autômatos e teoria das funções recursivas desenvolvidas entre as décadas de 1930 e 1950. Chomsky construiu sistematicamente sobre trabalhos matemáticos anteriores, criando uma síntese notável.\nO trabalho de Emil Post (1936-1944) foi fundamental. Post desenvolveu sistemas canônicos de Post usando técnicas de reescrita de _string_s que se tornaram fundamentais para a abordagem de Chomsky. Seu trabalho de 1936 Processos combinatórios finitos – Formulação 1 criou modelos computacionais essencialmente equivalentes às máquinas de Turing. Post queria derivar mecanicamente inferências de uma sentença axiomática inicial, e Chomsky aplicou diretamente essa estrutura lógica para descrever conjuntos de _string_s na linguagem humana.\nO modelo de McCulloch-Pitts de 1943 forneceu a ponte computacional. Seu trabalho Um Cálculo Lógico das Ideias Imanentes na Atividade Nervosa ofereceu o primeiro modelo matemático conectando redes neurais à computação, levando diretamente à noção de autômatos finitos que se tornaram o Tipo-3 na Hierarquia de Chomsky. Eles demonstraram que neurônios poderiam funcionar como portas lógicas, estabelecendo a conexão entre sistemas biológicos e modelos computacionais formais.\nO trabalho de Stephen Cole Kleene estabeleceu a equivalência fundamental entre expressões regulares e máquinas de estado finito (Teorema de Kleene), fornecendo a base matemática para as gramáticas do Tipo-3. Suas propriedades de fechamento para linguagens regulares sob união, concatenação e operação estrela de Kleene criaram o framework teórico necessário.\nA estrutura teórica se baseou explicitamente na teoria das funções recursivas. Chomsky declarou que a gramática gerativa se desenvolveu dentro de uma teoria matemática particular, a saber, a teoria das funções recursivas. Isso foi importante porque a teoria das funções recursivas nas décadas de 1930-1940 formalizou a noção de computação, construindo sobre a tese de Church-Turing e as provas de equivalência entre diferentes modelos de computação.\n\n\n5.1.2 A elegância matemática dos quatro tipos\nO desenvolvimento dos quatro tipos específicos de gramáticas emergiu da aplicação de restrições matemáticas crescentes. A classificação não foi arbitrária, mas resultou de uma lógica matemática rigorosa que Chomsky articulou em seus trabalhos seminais de 1956 (Três Modelos para a Descrição da Linguagem) e 1959 (Sobre Certas Propriedades Formais das Gramáticas).\nA estrutura hierárquica aninhada representa uma beleza matemática notável. Cada classe é um subconjunto próprio da próxima classe menos restritiva: \\(\\text{Tipo 3} ⊂ \\text{Tipo 2} ⊂ \\text{Tipo 1} ⊂ \\text{Tipo 0}\\). Cada tipo de gramática corresponde exatamente a uma classe de autômatos com poder computacional correspondente, criando uma correspondência perfeita entre restrições gramaticais e modelos computacionais.\n\nO Tipo 0 (Gramáticas Irrestritas) não possui restrições nas regras de produção (\\(\\alpha \\rightarrow \\beta\\)), sendo equivalente às máquinas de Turing e capaz de gerar linguagens recursivamente enumeráveis.\nO Tipo 1 (Gramáticas Sensíveis ao Contexto) adiciona a propriedade não-contrativa (\\(\\mid \\alpha \\mid ≤ \\mid \\beta \\mid\\)), correspondendo aos autômatos linearmente limitados.\nO Tipo 2 (Gramáticas Livres de Contexto)(Gramáticas Livres de Contextos) re_string_e o lado esquerdo a um único não-terminal (\\(A \\rightarrow γ\\)), equivalendo aos autômatos de pilha.\nO Tipo 3 (Gramáticas Regulares) impõe as restrições mais específicas em padrões terminal/não-terminal, correspondendo aos autômatos finitos.\n\nA elegância conceitual reside na relação inversa entre liberdade gramatical e complexidade computacional: gramáticas mais restritas (Tipo 3) requerem menor poder computacional (autômatos finitos), enquanto gramáticas menos restritas (Tipo 0) necessitam do maior poder computacional (máquinas de Turing).\nNa ciência da computação, a hierarquia estabeleceu as bases matemáticas para a teoria das linguagens formais, criando uma classificação sistemática que permanece fundamental para a ciência da computação teórica. A realização de Chomsky de que a descrição de um tipo de autômato e de um tipo de linguagem estão relacionadas conectou campos anteriormente desconectados da linguística e computação.\n\n\n5.1.3 Aplicações práticas em compiladores e processamento de linguagem\nNa análise léxica, gramáticas do Tipo-3 (regulares) e autômatos finitos tornaram-se fundamentais para tokenizar código fonte, identificando palavras-chave, _string_s e identificadores. Na análise sintática, gramáticas do Tipo-2 (livres de contexto) formam a base teórica para analisar a maioria das linguagens de programação, lidando com estruturas aninhadas como parênteses, chamadas de função e estruturas de controle.\nGeradores de parser modernos usam especificações de Gramáticas Livres de Contextos para gerar automaticamente parsers, com ferramentas implementando diretamente conceitos da Hierarquia de Chomsky. Subconjuntos de gramáticas livres de contexto (LL, LR) tornaram-se padrão para parsing eficiente, tornando a compilação tratável mantendo poder expressivo suficiente.\nA Forma de Backus-Naur (BNF) desenvolvida por John Backus e Peter Naur para ALGOL 60 foi criada com conhecimento explícito do trabalho anterior de Chomsky sobre Gramáticas Livres de Contexto, tornando-se o padrão da indústria para definir sintaxe de linguagens de programação e implementando diretamente conceitos de gramática formal de Chomsky.\nNão bastasse o que a atenta leitora viu até o momento, cada nível da hierarquia possui propriedades de complexidade computacional bem definidas. Linguagens regulares (Tipo 3) podem ser reconhecidas em tempo linear \\(O(n)\\) com espaço constante. Linguagens livres de contexto (Tipo 2) são decidíveis em tempo polinomial \\(O(n^3)\\) usando o algoritmo CYK, formando a base para a maioria dos compiladores de linguagens de programação. Linguagens sensíveis ao contexto (Tipo 1) são PSPACE-completas, com complexidade de tempo exponencial no pior caso, mas complexidade de espaço linear. Linguagens recursivamente enumeráveis (Tipo 0) são indecidíveis em geral.\n\nAlgoritmo CYK (Cocke-Younger-Kasami)\nO algoritmo CYK é um método de programação dinâmica para determinar se uma string pertence a uma linguagem livre de contexto. Desenvolvido independentemente por Cocke, Younger e Kasami, o algoritmo requer que a gramática esteja na Forma Normal de Chomsky, onde todas as produções têm a forma \\(A \\rightarrow BC\\) ou \\(A \\rightarrow a\\).\nO algoritmo constrói uma tabela triangular de tamanho \\(n \\times n\\), onde \\(n\\) é o comprimento da string de entrada. Cada entrada \\((i,j)\\) da tabela indica quais não-terminais podem derivar a sub_string_ que inicia na posição \\(i\\) com comprimento \\(j\\). O preenchimento ocorre bottom-up: primeiro as sub_string_s de comprimento 1, depois 2, e assim sucessivamente até \\(n\\).\nA complexidade \\(O(n^3)\\) resulta de três loops aninhados: dois para percorrer a tabela (\\(O(n^2)\\) entradas) e um terceiro para testar todas as possíveis divisões de cada sub_string_ (\\(O(n)\\) divisões por entrada). Esta complexidade polinomial torna o CYK prático para parsing de linguagens de programação, sendo fundamental na construção de parsers bottom-up.\n\n\nPSPACE-Completas\nPSPACE é a classe de complexidade que contém todos os problemas de decisão solucionáveis por uma máquina de Turing determinística usando espaço polinomial. Um problema é PSPACE-completo se pertence ao PSPACE e qualquer problema em PSPACE pode ser reduzido a ele em tempo polinomial.\nAs linguagens sensíveis ao contexto (Tipo 1) são PSPACE-completas porque seu reconhecimento requer espaço proporcional ao comprimento da entrada, mas pode demandar tempo exponencial. Isso ocorre porque um autômato linearmente limitado (que reconhece essas linguagens) possui número finito de configurações possíveis, mas esse número cresce exponencialmente com o tamanho da entrada.\nA aparente contradição entre “espaço linear” e “PSPACE-completo” se resolve considerando que PSPACE inclui problemas que usam espaço polinomial, e espaço linear é um caso particular de espaço polinomial (\\(O(n) \\subset O(n^k)\\)). O tempo exponencial surge porque, mesmo com espaço limitado, o número de estados alcançáveis pode crescer exponencialmente, exigindo exploração exaustiva no pior caso.\n\nAlém disso, os lemas de bombeamento para cada classe fornecem ferramentas matemáticas para provar que linguagens NÃO pertencem às classes inferiores. As propriedades de fechamento variam sistematicamente: linguagens regulares são fechadas sob todas as operações padrão, linguagens livres de contexto são fechadas sob união, concatenação e estrela de Kleene mas NÃO sob interseção e complemento.\n\n\n5.1.4 O Poder e as Limitações das Linguagens Regulares (Tipo 3)\nAs linguagens regulares representam a classe mais fundamental e restrita da hierarquia. Elas são formalmente definidas como o conjunto de linguagens que podem ser descritas por expressões regulares ou, equivalentemente, geradas por gramáticas regulares.\nA estrutura de uma gramática regular é estritamente limitada. Suas regras de produção, que ditam como os símbolos podem ser reescritos, devem aderir a um formato rígido. Em uma gramática linear à direita, por exemplo, todas as regras devem ser da forma \\(A \\rightarrow tN\\) ou \\(A \\rightarrow t\\), onde \\(A\\) e \\(N\\) são símbolos não terminais e \\(t\\) é uma string de símbolos terminais que pode ser vazia. Uma restrição similar se aplica às gramáticas lineares à esquerda. Esta limitação estrutural não é meramente uma convenção; ela é a fonte da principal limitação computacional das linguagens regulares: a incapacidade de modelar dependências aninhadas ou recursivas.\nO autômato finito, o reconhecedor para a classe das linguagens regulares, opera sem uma memória externa; seu único histórico está contido no estado atual em que se encontra. Consequentemente, ele não pode lembrar ou contar ocorrências de símbolos para garantir correspondências em uma string. O exemplo canônico das limitações de um autômato finito são os parênteses aninhados. Um autômato finito não pode verificar se uma string contém um número igual de parênteses abertos e fechados se estes parênteses puderem ser aninhados.\n\n\n5.1.5 A Ascensão às Linguagens Livres de Contexto (Tipo 2)\nA necessidade de modelar estruturas sintáticas mais complexas, como expressões aritméticas com parênteses aninhados, blocos de código delimitados (´begin´…´end´) ou estruturas de dados recursivas, revela a insuficiência das linguagens regulares. Para superar essas limitações, ascendemos na hierarquia para as linguagens livres de contexto.\nAs Linguagens Livres de Contexto formam um superconjunto estrito das linguagens regulares; toda linguagem regular é, por definição, livre de contexto, mas o inverso não é verdadeiro. O poder expressivo adicional das Linguagens Livres de Contexto emana diretamente de uma flexibilização nas regras de produção de suas gramáticas. Em uma gramática livre de contexto (Gramáticas Livres de Contextos), uma regra de produção tem a forma\nA→β, onde A é um único símbolo não-terminal e β é uma string qualquer de símbolos terminais e não terminais. A ausência de restrições sobre a posição dos não terminais em β permite a definição de recursão central, como na regra S→aSb, que é a chave para modelar estruturas aninhadas.2\n\n\n5.1.6 Exemplos Práticos que Distinguem as Duas Classes\nA distinção teórica entre essas duas classes é melhor ilustrada pelo exemplo canônico da linguagem \\(L={a^n b^n \\mid n \\geq 0}\\), que consiste em strings com um número de ’a’s seguido pelo mesmo número de ’b’s.\nEsta linguagem não é regular. A tentativa de construir um autômato finito para reconhecê-la falha porque a máquina precisaria de uma quantidade infinita de estados para lembrar o número exato de ’a’s lidos e garantir que o número de ’b’s corresponda. Uma prova formal de que \\(L\\) não é uma linguagem regular pode ser rigorosamente construída usando o Lema do Bombeamento para linguagens regulares. Este lema afirma que, para qualquer string suficientemente longa em uma linguagem regular, existe uma sub_string_ que pode ser bombeada (repetida um número arbitrário de vezes) e a nova string resultante ainda pertencerá à linguagem. Ao aplicar o lema à string \\(apbp\\), no qual \\(p\\) é o comprimento de bombeamento, a sub_string_ bombeada consistirá inteiramente de ’a’s, quebrando o equilíbrio entre ’a’s e ’b’s e provando que a linguagem não pode ser regular. No entanto, \\(L\\) é uma linguagem livre de contexto por excelência. Ela pode ser gerada pela gramática extremamente simples e elegante:\n\\[S \\rightarrow aSb \\mid \\epsilon\\]\nNesta gramática o símbolo \\(\\epsilon\\) representa a string vazia. A regra recursiva \\(S \\rightarrow aSb\\) encapsula perfeitamente a capacidade de aninhamento que define as Linguagens Livres de Contexto. Outros exemplos práticos que exigem o poder das Linguagens Livres de Contexto incluem a linguagem dos palíndromos (e.g., radar) e a validação estrutural de documentos XML, no qual as tags de abertura e fechamento devem ser corretamente aninhadas, uma tarefa impossível para expressões regulares.\nA atenta leitora deve observar que essa progressão não é apenas um exercício teórico. A necessidade prática de analisar a sintaxe de linguagens de programação, que são repletas de construções aninhadas como laços, condicionais e chamadas de função, é a força motriz que torna as linguagens regulares insuficientes e as linguagens livres de contexto absolutamente essenciais para a ciência da computação. A Tabela #tbl-resumo1 condensa as diferenças entre linguagens regulares e linguagens livres de contexto.\n\nResumo das características e diferenças das linguagens regulares e livres de contexto.{#tbl-resumo1}\n\n\n\n\n\n\n\nCaracterística\nLinguagens Regulares (Tipo 3)\nLinguagens Livres de Contexto (Tipo 2)\n\n\n\n\nTipo de Gramática\nGramática Regular\nGramática Livre de Contexto\n\n\nFormato das Regras\nRestrito (e.g., A→aB ou A→a)\nIrrestrito (e.g., A→β, onde β é qualquer string)\n\n\nAutômato Reconhecedor\nAutômato Finito (AF)\nAutômato com Pilha (AP)\n\n\nCapacidade de Memória\nNenhuma (limitada a estados finitos)\nIlimitada (via pilha LIFO)\n\n\nExemplo Característico\na(ba)∗\nanbn para n≥0\n\n\nExemplo Não-Pertencente\nanbn para n≥0\nanbncn para n≥0",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linguagens Livres de Contexto</span>"
    ]
  },
  {
    "objectID": "04-Gramaticas.html#sec-anatomia-gramaticas-livres-de-contexto",
    "href": "04-Gramaticas.html#sec-anatomia-gramaticas-livres-de-contexto",
    "title": "5  Linguagens Livres de Contexto",
    "section": "5.2 A Anatomia das Gramáticas Livres de Contexto",
    "text": "5.2 A Anatomia das Gramáticas Livres de Contexto\nPara analisar e processar linguagens com estruturas aninhadas, é imprescindível o uso de um formalismo matemático preciso. Uma Gramática Livre de Contexto fornece a base formal necessária.\nO termo livre de contexto é fundamental para a compreensão da natureza das gramáticas que a atenta leitora estudará neste capítulo. Esse termo significa que a aplicação de uma regra de produção \\(A\\rightarrow \\beta\\) a um símbolo não-terminal \\(A\\) é incondicional; ela pode ocorrer independentemente dos símbolos que cercam \\(A\\), seu contexto, em uma forma sentencial intermediária. Esta propriedade simplifica a análise sintática em comparação com gramáticas mais complexas, como as sensíveis ao contexto (Tipo 1).\nFormalmente, uma Gramática Livre de Contexto é definida como uma quádrupla \\(G=(N,\\Sigma,P,S)\\), na qual cada componente é um conjunto e tem um papel específico na geração das _string_s da linguagem. Os quatro componentes da tupla que define uma Gramáticas Livres de Contextos são:\n\nN: O Conjunto de Símbolos Não Terminais. Este é um conjunto finito de variáveis que representam as diferentes construções sintáticas ou categorias gramaticais da linguagem. Por exemplo, em uma gramática para uma linguagem de programação, os não terminais podem incluir EXPRESSÃO, COMANDO e DECLARAÇÃO. Eles são os elementos que podem ser substituídos ou expandidos durante o processo de derivação e que aqui, neste livro, serão representados por letras maiúsculas do alfabeto latino.\n\\(\\Sigma\\): O Conjunto de Símbolos Terminais. Este é um conjunto finito, disjunto de N, que constitui o alfabeto da linguagem. Os terminais são os símbolos literais, os átomos que compõem as _string_s finais da linguagem e não podem ser mais decompostos. Exemplos incluem palavras-chave (if, while, etc.), operadores (+, \\, *, etc.) e identificadores (contador, salario, etc.).\nP: O Conjunto de Regras de Produção. Este é um conjunto finito de regras que definem como os não terminais podem ser substituídos. Cada regra tem a forma \\(A \\rightarrow β\\), onde \\(A\\in N\\) é um único não-terminal (a cabeça da produção) e \\(\\beta \\in (N∪Σ)^∗\\) é uma string, possivelmente vazia, de símbolos terminais e/ou não terminais que chamaremos de corpo da produção. Essas regras são o motor do sistema gerativo.\nS: O Símbolo Inicial. Um não-terminal especial, \\(S \\in N\\), que serve como ponto de partida para todas as derivações. Ele geralmente representa a construção sintática mais abrangente da linguagem, como um programa ou uma sentença.\n\nA linguagem gerada por uma gramática \\(G\\), denotada por \\(L(G)\\), é o conjunto de todas as _string_s de símbolos terminais que podem ser derivadas a partir do símbolo inicial \\(S\\) por meio da aplicação sucessiva das regras de produção em \\(P\\).\nEmbora a definição formal de uma Gramáticas Livres de Contextos seja inerentemente gerativa, descrevendo como construir sentenças válidas a partir de \\(S\\), sua aplicação primária em compiladores é reconhecedora. O objetivo de um analisador sintático não é gerar programas aleatórios, mas sim verificar se uma dada sequência de tokens, produzida pelo programador, pode ser gerada pela gramática. A gramática, portanto, atua como a especificação formal contra a qual o processo de reconhecimento é executado. O parser, na prática, tenta reverter o processo de derivação para validar a estrutura do programa fonte.\n\n5.2.1 Exemplo Canônico: A Linguagem dos Palíndromos\nPara solidificar esses conceitos abstratos, vamos construir uma Gramáticas Livres de Contextos para a linguagem dos palíndromos sobre o alfabeto \\(\\Sigma = \\{0,1\\}\\). Só para refrescar a memória: um palíndromo é uma string que se lê da mesma forma da esquerda para a direita e da direita para a esquerda.\nA estrutura recursiva dos palíndromos pode ser definida da seguinte forma:\n\nCasos Base: A string vazia (\\(\\epsilon\\)), ‘0’ e ‘1’ são palíndromos.\nPasso Indutivo: Se \\(w\\) é um palíndromo, então \\(0w0\\) e \\(1w1\\) também são palíndromos.\n\nEsta definição recursiva pode ser traduzida diretamente em um conjunto de regras de produção para uma Gramáticas Livres de Contextos. Seja \\(K\\) o nosso símbolo não-terminal para palíndromo:\n\n\\(K \\rightarrow \\epsilon\\)\n\n\\(K \\rightarrow 0\\)\n\n\\(K \\rightarrow 1\\)\n\n\\(K \\rightarrow 0K0\\)\n\n\\(K \\rightarrow 1K1\\)\n\nNeste exemplo, a gramática completa \\(G_{pal}\\) é definida pela quádrupla:\n\n\\(N = \\{K\\}\\)\n\\(\\Sigma = \\{0,1\\}\\)\n\\(P\\) é o conjunto das cinco regras listadas acima.\n\\(S = K\\)\n\nEsta gramática pode gerar qualquer palíndromo sobre \\(\\{0,1\\}\\). Por exemplo, a string 0110 pode ser derivada da seguinte forma:\nA derivação da string 0110 a partir do símbolo inicial \\(K\\) é feita aplicando-se as regras da gramática sequencialmente.\n\nPasso 1: Iniciar com o símbolo inicial.\n\nComeçamos com o símbolo inicial da gramática, que é \\(K\\).\n\\(K\\)\n\nPasso 2: Aplicar a Regra 4 (\\(K \\rightarrow 0K0\\)).\n\nPara gerar uma string que começa e termina com 0, aplicamos a regra 4.\n\\(K \\Rightarrow 0K0\\)\n\nPasso 3: Aplicar a Regra 5 (\\(K \\rightarrow 1K1\\)).\n\nAgora, precisamos gerar a parte interna do palíndromo. Substituímos o \\(K\\) restante pela regra 5 para obter os 1s internos.\n\\(0K0 \\Rightarrow 0(1K1)0 = 01K10\\)\n\nPasso 4: Aplicar a Regra 1 (\\(K \\rightarrow \\epsilon\\)).\n\nO centro do palíndromo 0110 é vazio. Para finalizar a derivação, substituímos o último \\(K\\) pela cadeia vazia, \\(\\epsilon\\) (epsilon), usando a regra 1.\n\\(01K10 \\Rightarrow 01(\\epsilon)10 = 0110\\)\n\n\nA sequência completa da derivação será:\n\\[K \\Rightarrow 0K0 \\Rightarrow 01K10 \\Rightarrow 01\\epsilon10 \\Rightarrow 0110\\]\n\n\n5.2.2 Exercícios de Derivação\n\n5.2.2.1 Exercício 1: Palíndromo Ímpar\nDada a gramática de palíndromos \\(G_{pal}\\):\n\n\\(N = \\{K\\}\\)\n\\(\\Sigma = \\{0,1\\}\\)\n\\(P = \\{ K \\rightarrow \\epsilon, K \\rightarrow 0, K \\rightarrow 1, K \\rightarrow 0K0, K \\rightarrow 1K1 \\}\\)\n\\(S = K\\)\n\nFaça a derivação da string 101.\n\n\n5.2.2.2 Exercício 2: Expressão Aritmética Simples\nConsidere uma gramática simplificada para expressões aritméticas, \\(G_{exp}\\) dada por:\n\n\\(N = \\{E\\}\\)\n\\(\\Sigma = \\{id, +, *, (, )\\}\\)\n\\(P = \\{ E \\rightarrow E + E, E \\rightarrow E * E, E \\rightarrow (E), E \\rightarrow id \\}\\)\n\\(S = E\\)\n\nFaça a derivação da string id * id + id.\n\n\n5.2.2.3 Exercício 3: Palíndromo de Comprimento Par e Aninhado\nUsando a gramática de palíndromos \\(G_{pal}\\) dada a seguir, faça a derivação da string 011110.\n\n\\(N = \\{K\\}\\)\n\\(\\Sigma = \\{0,1\\}\\)\n\\(P = \\{ K \\rightarrow \\epsilon, K \\rightarrow 0, K \\rightarrow 1, K \\rightarrow 0K0, K \\rightarrow 1K1 \\}\\)\n\\(S = K\\)\n\n\n\n5.2.2.4 Exercício 4: Linguagem \\(a^nb^n\\)\nConsidere a gramática \\(G_{ab}\\) que gera strings com um número de a’s seguido pelo mesmo número de b’s:\n\n\\(N = \\{S\\}\\)\n\\(\\Sigma = \\{a, b\\}\\)\n\\(P = \\{ S \\rightarrow aSb, S \\rightarrow \\epsilon \\}\\)\n\\(S = S\\)\n\nFaça a derivação da string aaabbb.\n\n\n5.2.2.5 Exercício 5: Comando Condicional if-else\nSeja uma gramática para um comando if-else simplificado, \\(G_{if}\\):\n\n\\(N = \\{C, A\\}\\)\n\\(\\Sigma = \\{ \\text{if}, \\text{then}, \\text{else}, id, :=, 0 \\}\\)\n\\(P = \\{ C \\rightarrow \\text{if } id \\text{ then } A \\text{ else } A, A \\rightarrow id := 0 \\}\\)\n\\(S = C\\)\n\nFaça a derivação da string if id then id := 0 else id := 0.\n\n\n5.2.2.6 Exercício 6: Parênteses Balanceados\nConsidere a gramática \\(G_{par}\\) para gerar sequências de parênteses balanceados:\n\n\\(N = \\{B\\}\\)\n\\(\\Sigma = \\{ (, ) \\}\\)\n\\(P = \\{ B \\rightarrow (B), B \\rightarrow BB, B \\rightarrow \\epsilon \\}\\)\n\\(S = B\\)\n\nFaça a derivação da string ()(()).\n\n\n5.2.2.7 Exercício 7: Palíndromo Vazio\nUsando a gramática de palíndromos \\(G_{pal}\\) do primeiro exercício, faça a derivação da string vazia, \\(\\epsilon\\).",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linguagens Livres de Contexto</span>"
    ]
  },
  {
    "objectID": "04-Gramaticas.html#geração-de-sentenças-derivações-árvores-e-ambiguidade",
    "href": "04-Gramaticas.html#geração-de-sentenças-derivações-árvores-e-ambiguidade",
    "title": "5  Linguagens Livres de Contexto",
    "section": "5.3 Geração de Sentenças: Derivações, Árvores e Ambiguidade",
    "text": "5.3 Geração de Sentenças: Derivações, Árvores e Ambiguidade\nUma derivação é a sequência de passos que transforma o símbolo inicial em uma string final de terminais por meio da aplicação das regras de produção. Exatamente o processo que a esforçada leitora aprendeu na seção Section 5.2. Em cada passo, um não-terminal é escolhido e substituído pelo corpo de uma da regras de produção deste não-terminal. Como uma forma sentencial intermediária pode conter múltiplos não terminais, precisaremos de uma convenção para determinar qual deles expandir. Isso leva a duas estratégias de derivação:\n\nDerivação Mais à Esquerda (Leftmost Derivation): em cada passo, o símbolo não-terminal que aparece mais à esquerda na forma sentencial é sempre o escolhido para ser substituído.\n\nDerivação Mais à Direita (Rightmost Derivation): em cada passo, o símbolo não-terminal que aparece mais à direita é o escolhido para ser substituído.\n\nPara uma gramática não ambígua, embora as sequências de passos sejam diferentes, tanto a derivação mais à esquerda quanto a mais à direita para uma dada sentença resultarão na mesma estrutura sintática subjacente. Como exemplo, a atenta leitora pode estudar o exemplo a seguir:\nExemplo 1: Gramática Não Ambígua para Expressões Aritméticas\nConsidere a seguinte gramática:\n\nSímbolos não-terminais (N): \\(\\{E, T, F\\}\\)\nSímbolos terminais (): \\(\\{id, +, *, (, )\\}\\)\nSímbolo inicial (S): \\(E\\)\nRegras de produção (P):\n\n\\(E \\rightarrow E + T\\)\n\\(E \\rightarrow T\\)\n\\(T \\rightarrow T * F\\)\n\\(T \\rightarrow F\\)\n\\(F \\rightarrow (E)\\)\n\\(F \\rightarrow id\\)\n\n\nVamos fazer a Derivação da string “id + id * id”, primeiro usando a derivação Mais à Esquerda (Leftmost):\n\\[\n\\begin{align}\nE &\\Rightarrow E + T                &&\\text{[regra 1: expandir E]}\\\n  &\\Rightarrow T + T                &&\\text{[regra 2: expandir E mais à esquerda]}\\\n  &\\Rightarrow F + T                &&\\text{[regra 4: expandir T mais à esquerda]}\\\n  &\\Rightarrow id + T               &&\\text{[regra 6: expandir F mais à esquerda]}\\\n  &\\Rightarrow id + T * F           &&\\text{[regra 3: expandir T]}\\\n  &\\Rightarrow id + F * F           &&\\text{[regra 4: expandir T mais à esquerda]}\\\n  &\\Rightarrow id + id * F          &&\\text{[regra 6: expandir F mais à esquerda]}\\\n  &\\Rightarrow id + id * id         &&\\text{[regra 6: expandir F]}\n\\end{align}\n\\]\nAgora vamos derivar a mesma string usando a derivação Mais à Direita (Rightmost):\n\\[\n\\begin{align}\nE &\\Rightarrow E + T                &&\\text{[regra 1: expandir E]}\\\n  &\\Rightarrow E + T * F            &&\\text{[regra 3: expandir T mais à direita]}\\\n  &\\Rightarrow E + T * id           &&\\text{[regra 6: expandir F mais à direita]}\\\n  &\\Rightarrow E + F * id           &&\\text{[regra 4: expandir T mais à direita]}\\\n  &\\Rightarrow E + id * id          &&\\text{[regra 6: expandir F mais à direita]}\\\n  &\\Rightarrow T + id * id          &&\\text{[regra 2: expandir E]}\\\n  &\\Rightarrow F + id * id          &&\\text{[regra 4: expandir T]}\\\n  &\\Rightarrow id + id * id         &&\\text{[regra 6: expandir F]}\n\\end{align}\n\\]\nObservando as duas derivações é possível perceber que a gramática usada neste exemplo, não é ambígua. Observe que:\n\nPrecedência clara: A multiplicação \\((*)\\) tem precedência maior que a adição \\((+)\\)\n\n\\(T\\) (termo) gera multiplicações\n\\(E\\) (expressão) gera adições de termos\n\nAssociatividade definida: Operadores associam à esquerda\n\n\\(E \\rightarrow E + T\\) (não \\(E \\rightarrow T + E\\))\n\\(T \\rightarrow T * F\\) (não \\(T \\rightarrow F * T\\))\n\nEstrutura única: Para qualquer string válida, existe exatamente uma árvore de derivação, independentemente da ordem de derivação (leftmost ou rightmost).\n\nA atenta leitora deve notar que embora a sequência de passos seja diferente nas duas derivações, ambas produzem a mesma estrutura sintática: \\(id + (id * id)\\), onde a multiplicação tem precedência sobre a adição.\n\n5.3.1 A Construção de Árvores de Derivação\nO processo mais intuitivo de visualizar a estrutura hierárquica imposta por uma gramática a uma sentença é por meio de uma árvore de derivação, ou árvore sintática. Uma árvore de derivação é uma representação gráfica de uma derivação que abstrai a ordem em que as produções foram aplicadas. Suas propriedades são definidas por:\n\nA raiz da árvore é rotulada com o símbolo inicial \\(S\\).\nCada nó interno é rotulado com um símbolo não-terminal.\nCada folha é rotulada com um símbolo terminal ou com \\(\\epsilon\\).\nSe um nó interno é rotulado com \\(A\\) e seus filhos, da esquerda para a direita, são rotulados com \\(X_1\\), \\(X_2\\), …, \\(X_n\\), então deve existir uma regra de produção \\(A \\rightarrow X_1 X_2 ... X_n\\) na gramática.\n\nA concatenação das folhas da árvore, lidas da esquerda para a direita, forma a sentença gerada, também conhecida como yield da árvore. A árvore de derivação captura a estrutura sintática essencial da sentença, tornando explícitas as relações entre suas subpartes.\nExemplo 2: Gramática Não Ambígua para Listas {#sec-derivacao-listas-exemplo2}\nConsidere a gramática definida como:\n\nSímbolos não-terminais (N): \\(\\{L, E\\}\\)\nSímbolos terminais (Σ): \\(\\{a, b, [, ], ,\\}\\)\nSímbolo inicial (S): \\(L\\)\nRegras de produção (P):\n\n\\(L \\rightarrow [E]\\)\n\\(L \\rightarrow [\\,]\\)\n\\(E \\rightarrow E, a\\)\n\\(E \\rightarrow E, b\\)\n\\(E \\rightarrow a\\)\n\\(E \\rightarrow b\\)\n\n\nVamos derivar a string “[a, b, a]”. Primeiro com a Derivação Mais à Esquerda (Leftmost):\n\\[\n\\begin{align}\nL &\\Rightarrow [E]                  &&\\text{[regra 1: expandir L]}\\\\\n  &\\Rightarrow [E, a]               &&\\text{[regra 3: expandir E]}\\\\\n  &\\Rightarrow [E, b, a]            &&\\text{[regra 4: expandir E mais à esquerda]}\\\\\n  &\\Rightarrow [a, b, a]            &&\\text{[regra 5: expandir E mais à esquerda]}\n\\end{align}\n\\]\nAgora com a Derivação Mais à Direita (Rightmost):\n\\[\n\\begin{align}\nL &\\Rightarrow [E]                  &&\\text{[regra 1: expandir L]}\\\\\n  &\\Rightarrow [E, a]               &&\\text{[regra 3: expandir E]}\\\\\n  &\\Rightarrow [E, b, a]            &&\\text{[regra 4: expandir E]}\\\\\n  &\\Rightarrow [a, b, a]            &&\\text{[regra 5: expandir E]}\n\\end{align}\n\\]\nA Figure 5.1 apresenta as duas árvores de derivação para a string “[a, b, a]”.\n\n\n\n\n\n\nFigure 5.1: Apresentação das duas árvores de derivação de “[a, b, a]” de forma gráfica.\n\n\n\nExemplo 3: Considerando a gramática do Exemplo 2 ?sec-derivacao-listas-exemplo2, faça a derivação da string “[b]”\nNovamente, começando com a Derivação Mais à Esquerda (Leftmost):\n\\[\n\\begin{align}\nL &\\Rightarrow [E]                  &&\\text{[regra 1: expandir L]}\\\\\n  &\\Rightarrow [b]                  &&\\text{[regra 6: expandir E]}\n\\end{align}\n\\]\nFinalmente, a Derivação Mais à Direita (Rightmost):\n\\[\n\\begin{align}\nL &\\Rightarrow [E]                  &&\\text{[regra 1: expandir L]}\\\\\n  &\\Rightarrow [b]                  &&\\text{[regra 6: expandir E]}\n\\end{align}\n\\]\nA atenta leitora deve perceber que esta gramática não é ambígua. Considerando que:\n\nEstrutura hierárquica clara:\n\n\\(L\\) gera apenas a estrutura de lista com colchetes;\n\\(E\\) gera apenas a sequência de elementos separados por vírgula.\n\nAssociatividade única:\n\nAs regras \\(E \\rightarrow E, a\\) e \\(E \\rightarrow E, b\\) forçam associatividade à esquerda;\nNão há regras como \\(E \\rightarrow a, E\\) que criariam ambiguidade.\n\nSem sobreposição de produções:\n\nCada não-terminal tem um papel específico e não conflitante;\nLista vazia \\([\\,]\\) é tratada separadamente, evitando ambiguidade.\n\n\nNota: A gramática usada nos exemplos 2 e 3 garante que elementos são adicionados sempre à direita da lista, construindo-a da esquerda para a direita. A estrutura \\([a, b, a]\\) só pode ser interpretada de uma forma: uma lista contendo três elementos na ordem especificada.\n\n\n5.3.2 O Problema da Ambiguidade\nUma gramática é considerada ambígua se existe pelo menos uma sentença em sua linguagem que pode ser gerada por duas ou mais árvores de derivação distintas. Equivalentemente, uma gramática é ambígua se alguma sentença possui duas ou mais derivações mais à esquerda (ou mais à direita) distintas.22\nA ambiguidade é um problema grave no projeto de linguagens de programação. A existência de uma ambiguidade implica que uma única sentença pode ter múltiplas interpretações sintáticas e, consequentemente, múltiplos significados semânticos.\n\nExemplo Clássico (Expressões Aritméticas): Considere a gramática Gexpr​:\nE→E+E ∣ E∗E ∣ id\nA sentença id + id * id pode ser derivada de duas maneiras 23:\n\nE⇒E+E⇒id+E⇒id+E∗E⇒…⇒id+id∗id (Interpretação: id+(id∗id))\n\nE⇒E∗E⇒E+E∗E⇒…⇒id+id∗id (Interpretação: (id+id)∗id)\nCada derivação corresponde a uma árvore de derivação diferente, uma impondo a precedência da multiplicação e a outra, a da adição.\n\nExemplo Clássico (Dangling Else): Em muitas linguagens, a construção if-then-else pode levar à ambiguidade. Para a sentença if (c1) if (c2) s1 else s2, não fica claro se o else se associa ao primeiro if ou ao segundo. Uma gramática ambígua permite ambas as interpretações, o que pode levar a erros lógicos graves e difíceis de depurar em tempo de execução.22\n\nA ambiguidade sintática é a causa raiz da ambiguidade semântica. A estrutura da árvore de derivação não é um mero artefato teórico; ela dita diretamente a ordem de avaliação e, portanto, o significado do programa. As fases subsequentes do compilador, como a análise semântica e a geração de código, operam sobre a estrutura hierárquica fornecida pela árvore.26 Se múltiplas árvores são possíveis, o compilador não tem como determinar a intenção do programador, tornando impossível a geração de um código objeto correto e determinístico. Por essa razão, a eliminação da ambiguidade — tipicamente por meio da reescrita da gramática para impor regras de precedência e associatividade de operadores — não é uma formalidade, mas um pré-requisito absoluto para a construção de um compilador funcional.23 A sintaxe precede e comanda a semântica.\n\n\n5.3.3 Capítulo 4: Autômatos com Pilha: A Máquina por Trás das Linguagens Livres de Contexto\n\n5.3.3.1 4.1 A Limitação dos Autômatos Finitos\nComo estabelecido anteriormente, os autômatos finitos (AFs), os reconhecedores para linguagens regulares, são fundamentalmente limitados por sua falta de memória. A capacidade de lembrar está restrita ao conjunto finito de estados da máquina. Essa limitação os impede de reconhecer linguagens que exigem a correspondência de símbolos ou contagem, como a linguagem L={anbn∣n≥0}.\n\n\n5.3.3.2 4.2 O Autômato com Pilha (AP): Adicionando Memória\nPara reconhecer a classe mais ampla das linguagens livres de contexto, é necessário um modelo de computação mais poderoso. O autômato com pilha (AP) é esse modelo. Um AP pode ser concebido como um autômato finito não determinístico (AFND) ao qual foi adicionada uma memória auxiliar: uma pilha (stack).28\nA pilha é uma estrutura de dados com acesso restrito, operando no modo LIFO (Last-In, First-Out), o que significa que o último elemento inserido é o primeiro a ser removido. As transições de um AP são mais complexas que as de um AF. A decisão de qual transição tomar depende de três fatores: o estado atual, o próximo símbolo na string de entrada e o símbolo que está no topo da pilha.28\nEm cada transição, além de mudar de estado e consumir um símbolo de entrada (opcionalmente), o AP pode realizar uma de três operações na pilha:\n\nEmpilhar (Push): Adicionar um ou mais símbolos ao topo da pilha.\nDesempilhar (Pop): Remover o símbolo do topo da pilha.\nManter: Não alterar o conteúdo da pilha.\n\nEsta capacidade de armazenar e recuperar informações de forma estruturada confere ao AP seu poder computacional superior.28\n\n\n5.3.3.3 4.3 A Equivalência Fundamental\nO resultado mais importante da teoria das Linguagens Livres de Contexto é a equivalência formal entre gramáticas livres de contexto e autômatos com pilha. Uma linguagem é livre de contexto se, e somente se, existe um autômato com pilha que a reconhece.12\nEsta equivalência é a espinha dorsal da análise sintática. Ela garante que para qualquer sintaxe de linguagem de programação que possa ser descrita por uma Gramáticas Livres de Contextos, podemos construir um mecanismo computacional (o AP) para reconhecer programas escritos nessa linguagem. É importante notar que a classe de linguagens reconhecidas por APs não determinísticos é estritamente maior que a classe reconhecida por APs determinísticos. São os APs não determinísticos que são equivalentes em poder às Gramáticas Livres de Contextoss em geral.31\nA escolha de uma pilha como o mecanismo de memória para reconhecer Linguagens Livres de Contexto não é acidental. A estrutura LIFO de uma pilha espelha perfeitamente a natureza recursiva e aninhada das derivações em uma Gramáticas Livres de Contextos. Considere novamente a gramática S→aSb para a linguagem anbn. A derivação de aabb é S⇒aSb⇒aaSbb⇒aabb. Observe como a estrutura se expande simetricamente de dentro para fora. Um AP para esta linguagem implementa essa simetria de forma operacional: ao ler um ‘a’, ele empilha um símbolo de marcador (e.g., X); ao ler o próximo ‘a’, empilha outro X. Quando começa a ler os ‘b’s, ele desempilha um X para cada ’b’ lido. Se a entrada terminar exatamente quando a pilha se esvaziar, a string é aceita. O processo de empilhar na primeira metade e desempilhar na ordem inversa na segunda metade é a encarnação mecânica da recursão gramatical. A pilha lembra as obrigações sintáticas (gerar um ‘b’ correspondente para cada ‘a’) e as descarrega na ordem correta, tornando-a a estrutura de dados canónica para processar estruturas livres de contexto.\n\n\n\n5.3.4 Capítulo 5: As Fronteiras do Contexto Livre: O Lema do Bombeamento\n\n5.3.4.1 5.1 Introdução ao Lema do Bombeamento para Linguagens Livres de Contexto\nAssim como existe uma ferramenta para provar que uma linguagem não é regular, existe um análogo para as linguagens livres de contexto: o Lema do Bombeamento para Linguagens Livres de Contexto, também conhecido como Lema de Bar-Hillel.32 Sua principal aplicação é demonstrar, por contradição, que uma determinada linguagem\nnão é livre de contexto.34\nA intuição por trás do lema está enraizada na estrutura finita das gramáticas e na natureza das árvores de derivação. Para uma Gramáticas Livres de Contextos com um número finito de não terminais, qualquer string suficientemente longa gerada por ela deve ter uma árvore de derivação alta. Pelo princípio da casa dos pombos, um caminho longo da raiz a uma folha nessa árvore deve necessariamente conter pelo menos um não-terminal repetido. Essa repetição cria uma sub-árvore que pode ser excisada ou duplicada, bombeando a string de uma maneira específica.35\n\n\n5.3.4.2 5.2 Explicação Detalhada do Lema\nO lema afirma formalmente que para qualquer linguagem livre de contexto L, existe um inteiro p≥1 (o comprimento de bombeamento) tal que qualquer string s∈L com comprimento ∣s∣≥p pode ser decomposta em cinco sub_string_s, s=uvxyz, que devem satisfazer as seguintes três condições 32:\n\n∣vxy∣≤p: A sub_string_ que contém as partes bombeáveis não é excessivamente longa.\n∣vy∣≥1: Pelo menos uma das duas sub_string_s bombeáveis (v ou y) não é vazia. Isso garante que o bombeamento realmente altera a string.\nuvnxynz∈L para todo inteiro n≥0: As duas sub_string_s v e y podem ser bombeadas (repetidas) em conjunto um número arbitrário de vezes (incluindo zero, o que corresponde a removê-las), e a string resultante permanecerá na linguagem L.\n\n\n\n5.3.4.3 5.3 Aplicação Prática: Prova de que L = {aⁿbⁿcⁿ | n ≥ 0} não é Livre de Contexto\nA linguagem L={anbncn∣n≥0} é o exemplo canônico de uma linguagem que está além do alcance das Gramáticas Livres de Contextoss. Podemos provar isso rigorosamente usando o lema do bombeamento.32\nA prova segue por contradição:\n\nSuposição: Suponha que L é livre de contexto.\nInvocação do Lema: Pelo lema, deve existir um comprimento de bombeamento p. \nEscolha da string: Selecionamos a string s=apbpcp. Claramente, s∈L e seu comprimento, 3p, é maior ou igual a p. \nAnálise da Decomposição: O lema garante que s pode ser decomposta como s=uvxyz, sujeita às condições do lema. A condição ∣vxy∣≤p é a chave. Dada a estrutura de s (um bloco de ‘a’s, seguido por um bloco de ’b’s, seguido por um bloco de ’c’s), esta condição implica que a sub_string_ vxy não pode conter ocorrências de todos os três símbolos (’a’, ‘b’ e ‘c’). Ela pode estar inteiramente dentro do bloco de ’a’s, inteiramente dentro do de ’b’s, ou abranger a fronteira entre ’a’s e ’b’s, ou entre ’b’s e ’c’s.\nContradição: A condição ∣vy∣≥1 garante que o bombeamento adicionará (ou removerá) pelo menos um símbolo. Vamos considerar o bombeamento para cima, com n=2, resultando na string s′=uv2xy2z.\n\nSe vxy contivesse apenas ’a’s, então v e y conteriam apenas ’a’s. A string s′ teria mais ’a’s do que ’b’s e ’c’s, violando a condição da linguagem.\nSe vxy contivesse uma mistura de ‘a’s e ’b’s, então v e y poderiam conter ’a’s e ’b’s, mas nenhum ’c’. A string s′ teria um número aumentado de ’a’s e/ou ’b’s, mas o número de ’c’s permaneceria p. Novamente, a igualdade n=n=n seria quebrada.\nO mesmo raciocínio se aplica a todas as outras localizações possíveis de vxy. Em nenhum caso, o bombeamento pode aumentar o número de ’a’s, ’b’s e ’c’s na mesma proporção.\n\nConclusão: A string bombeada s′ não pertence a L. Isso contradiz a terceira condição do lema. Portanto, a suposição inicial de que L é livre de contexto deve ser falsa.\n\nA estrutura de bombeamento duplo (v e y) do lema não é arbitrária. Ela revela a limitação fundamental das Linguagens Livres de Contexto a dependências de, no máximo, dois pontos. Uma Gramáticas Livres de Contextos, por meio de recursão como S→aSb, pode correlacionar duas partes de uma string (os ’a’s no início e os ’b’s no fim), que correspondem às partes v e y que são bombeadas em conjunto. A linguagem anbncn exige uma dependência de três pontos. Um autômato com pilha gasta sua memória para verificar a correspondência entre ’a’s e ’b’s, não restando capacidade para verificar os ’c’s contra a contagem original. O lema do bombeamento formaliza essa limitação, mostrando que o bombeamento inevitavelmente quebra essa dependência tripla.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linguagens Livres de Contexto</span>"
    ]
  },
  {
    "objectID": "04-Gramaticas.html#parte-ii-aplicações-em-compiladores-e-análise-sintática",
    "href": "04-Gramaticas.html#parte-ii-aplicações-em-compiladores-e-análise-sintática",
    "title": "5  Linguagens Livres de Contexto",
    "section": "5.4 Parte II: Aplicações em Compiladores e Análise Sintática",
    "text": "5.4 Parte II: Aplicações em Compiladores e Análise Sintática\n\n5.4.1 Capítulo 6: A Arquitetura de um Compilador Moderno\n\n5.4.1.1 6.1 Visão Geral do Processo de Compilação\nUm compilador é um programa complexo que atua como um tradutor, convertendo um programa escrito em uma linguagem de programação de alto nível (como C++, Java ou Python) em um programa equivalente em uma linguagem de baixo nível, tipicamente o código de máquina de um processador específico.39 Para gerenciar essa complexidade, a arquitetura de um compilador moderno é modular, dividida em uma série de fases sequenciais. Conceitualmente, essas fases são agrupadas em duas partes principais: o\nFront-End e o Back-End.39\n\nFront-End (Análise): Responsável por analisar o código fonte para verificar sua validade e construir uma representação intermediária do programa. É dependente da linguagem fonte, mas em grande parte independente da máquina alvo.\nBack-End (Síntese): Responsável por otimizar a representação intermediária e gerar o código de máquina para a arquitetura alvo. É dependente da máquina alvo, mas em grande parte independente da linguagem fonte.\n\n\n\n5.4.1.2 6.2 O Front-End (Análise)\nO Front-End decompõe o programa fonte em suas partes constituintes e impõe uma estrutura lógica a elas por meio de três fases principais:\n\nAnálise Léxica (Scanning): Esta é a primeira fase do compilador. O analisador léxico, ou scanner, lê o fluxo de caracteres do código fonte e o agrupa em sequências significativas chamadas tokens. Por exemplo, a sequência de caracteres pos = init + rate * 60; seria convertida em um fluxo de tokens como (id, pos), (assign, =), (id, init), (op, +), (id, rate), (op, *), (num, 60), (semicolon, ;). Durante este processo, elementos irrelevantes como espaços em branco e comentários são descartados.17 A especificação dos padrões para os tokens é tipicamente feita usando expressões regulares.\nAnálise Sintática (Parsing): A segunda fase, e o foco principal deste relatório. O analisador sintático, ou parser, recebe o fluxo de tokens do scanner e verifica se eles formam uma estrutura gramaticalmente válida de acordo com as regras da linguagem de programação. O parser impõe uma estrutura hierárquica aos tokens, geralmente construindo uma Árvore Sintática Abstrata (AST) que representa a estrutura lógica do programa.16 É nesta fase que as gramáticas livres de contexto são aplicadas.\nAnálise Semântica: A fase final do Front-End. O analisador semântico utiliza a AST e uma estrutura de dados chamada tabela de símbolos (que armazena informações sobre identificadores como variáveis e funções) para verificar a consistência semântica do código. Isso inclui tarefas como a checagem de tipos (e.g., garantir que não se está somando um inteiro com uma string), a verificação de declaração de variáveis e a correspondência de parâmetros em chamadas de função.26\n\n\n\n5.4.1.3 6.3 O Back-End (Síntese)\nO Back-End pega a representação intermediária (geralmente a AST anotada pela análise semântica) e a traduz em código executável.\n\nGeração de Código Intermediário: A AST é traduzida para uma representação de baixo nível, mas ainda independente da máquina, como o código de três endereços. Este código se assemelha a uma linguagem assembly genérica, facilitando as otimizações subsequentes.26\n\nOtimização de Código: Esta fase aplica uma variedade de transformações ao código intermediário para melhorar seu desempenho (tornando-o mais rápido) ou reduzir seu tamanho. As otimizações podem ser locais (em pequenos blocos de código) ou globais (em todo o programa).39\n\nGeração de Código Final: A fase final traduz o código intermediário otimizado para a linguagem de montagem (assembly) ou código de máquina relocável da arquitetura alvo. Esta fase lida com detalhes específicos da máquina, como alocação de registradores e seleção de instruções.39\n\nO processo de compilação pode ser visto como um pipeline que primeiro aumenta o nível de abstração e estrutura — transformando uma sequência linear de caracteres em tokens e, em seguida, em uma árvore sintática hierárquica — e depois o diminui progressivamente — achatando a árvore em um código intermediário linear e, finalmente, traduzindo-o para o código de máquina. A análise sintática representa o ápice deste processo, o ponto em que a estrutura lógica e hierárquica máxima do programa é capturada e validada.\n\n\n\n5.4.2 Capítulo 7: O Coração do Compilador: Análise Sintática (Parsing)\n\n5.4.2.1 7.1 Posicionamento e Objetivos\nA análise sintática, ou parsing, é a segunda fase do processo de compilação, posicionada entre a análise léxica e a análise semântica.16 Seu papel é fundamental: atua como o guardião da gramática da linguagem. Enquanto o analisador léxico verifica a ortografia (se as palavras, ou tokens, são válidas), o parser verifica a gramática (se a sequência de tokens forma sentenças estruturalmente corretas).17 O objetivo principal do parser é determinar se o fluxo de tokens de entrada pode ser gerado pela gramática livre de contexto que define a linguagem e, em caso afirmativo, construir uma representação explícita dessa estrutura.\n\n\n5.4.2.2 7.2 Entrada e Saída\nA entrada para o analisador sintático é o fluxo de tokens produzido pelo analisador léxico. A saída, para um programa sintaticamente correto, é uma estrutura de dados que representa a estrutura hierárquica do código. Embora a árvore de derivação seja a representação teórica direta, na prática, os compiladores constroem uma Árvore Sintática Abstrata (AST). A AST é uma versão condensada e mais abstrata da árvore de derivação, que omite detalhes sintáticos intermediários (como parênteses para agrupamento ou não terminais que apenas passam a derivação adiante) e captura a estrutura lógica e semântica essencial do programa, tornando-a mais adequada para as fases subsequentes de análise e geração de código.16\n\n\n5.4.2.3 7.3 As Duas Grandes Famílias de Analisadores\nA maneira como a árvore de derivação é construída em relação à entrada define as duas principais estratégias de parsing, cada uma com suas próprias características, pontos fortes e limitações 41:\n\nAnálise Descendente (Top-Down Parsing): A construção da árvore de derivação começa no topo (a raiz, que é o símbolo inicial da gramática) e avança para baixo, em direção às folhas (a string de tokens de entrada). Este método tenta encontrar a derivação mais à esquerda para a entrada.41\n\nAnálise Ascendente (Bottom-Up Parsing): A construção da árvore de derivação começa na base (as folhas, que são a string de tokens de entrada) e avança para cima, em direção à raiz (o símbolo inicial). Este método efetivamente reverte uma derivação mais à direita.41\n\nUm erro sintático detectado pelo parser é um erro fatal que interrompe o processo de compilação. Sem uma estrutura sintática válida e inequívoca representada pela AST, as fases subsequentes, que dependem dessa estrutura para realizar a verificação de tipos e a geração de código, não podem prosseguir.\n\n\n\n5.4.3 Capítulo 8: Estratégias de Análise Descendente (Top-Down)\n\n5.4.3.1 8.1 Conceito\nA análise descendente tenta construir uma árvore de derivação para a string de entrada começando pela raiz (símbolo inicial) e criando os nós da árvore em pré-ordem. Isso equivale a encontrar uma derivação mais à esquerda para a string de entrada.41\n\n\n5.4.3.2 8.2 Analisadores de Descida Recursiva (Recursive-Descent)\nUma das implementações mais diretas e intuitivas de um parser descendente é o analisador de descida recursiva. Nesta abordagem, um conjunto de procedimentos mutuamente recursivos é escrito, geralmente um para cada não-terminal na gramática. O procedimento associado a um não-terminal A é responsável por reconhecer na entrada uma sub_string_ que pode ser derivada de A.43 A simplicidade e a facilidade de implementação manual tornam esta técnica atraente. No entanto, analisadores de descida recursiva ingênuos podem ser ineficientes. Estes analisadores podem exigir retrocesso ( backtracking) se a escolha de uma produção se revelar incorreta. Além disso, eles não conseguem lidar com gramáticas que contêm recursão à esquerda (regras da forma A→Aβ). Isso levaria a uma recursão infinita.\n\n\n5.4.3.3 8.3 Analisadores Preditivos (LL)\nPara superar as desvantagens do retrocesso, foi desenvolvida uma classe de parsers descendentes chamada de analisadores preditivos. Estes são parsers que podem prever qual produção aplicar a um não-terminal olhando para a frente na string de entrada, sem precisar adivinhar e retroceder.44 O tipo mais comum é o\nparser LL(1).45 A notação LL(1) significa:\n\nL (primeiro): A entrada é lida da Esquerda (Left) para a direita.\nL (segundo): O parser constrói uma derivação mais à Esquerda (Leftmost).\n(1): Ele usa 1 símbolo de lookahead (antecipação) para tomar suas decisões.\n\nUm parser LL(1) opera com três componentes: uma pilha de análise, um ponteiro de entrada e uma tabela de análise.42 A tabela de análise é uma matriz onde as linhas correspondem aos não terminais e as colunas aos terminais. Cada célula\nM[A,a] contém a regra de produção que deve ser usada se o não-terminal A estiver no topo da pilha e o terminal a for o próximo símbolo de entrada (o lookahead). O algoritmo é determinístico: para cada par (não-terminal no topo da pilha, símbolo de lookahead), há no máximo uma ação a ser tomada. Se a célula estiver vazia, um erro sintático é detectado.\nOs parsers LL são ansiosos. No momento em que um não-terminal A está no topo da pilha, eles devem se comprometer imediatamente com uma única regra A→β, baseando sua decisão exclusivamente no próximo token de entrada. Essa necessidade de uma decisão precoce e inequívoca é a razão pela qual as gramáticas LL(1) não podem ter ambiguidades, recursão à esquerda ou prefixos comuns (duas produções para o mesmo não-terminal que começam com o mesmo símbolo). Tais características tornam impossível para o parser fazer uma escolha determinística com apenas um símbolo de lookahead.\nTabela de Análise LL(1) para uma Gramática de Expressões Simplificada |\n:— | id | + | * | ( | ) | $ |\nE | E→TE′ | | | E→TE′ | | |\nE’ | | E′→+TE′ | | | E′→ϵ | E′→ϵ |\nT | T→FT′ | | | T→FT′ | | |\nT’ | | T′→ϵ | T′→∗FT′ | | T′→ϵ | T′→ϵ |\nF | F→id | | | F→(E) | | |\n\n\n\n5.4.4 Capítulo 9: Estratégias de Análise Ascendente (Bottom-Up)\n\n5.4.4.1 9.1 Conceito\nEm contraste com a abordagem descendente, a análise ascendente constrói a árvore de derivação a partir das folhas (a string de entrada) em direção à raiz (o símbolo inicial). O processo pode ser visto como uma tentativa de reduzir a string de entrada de volta ao símbolo inicial, essencialmente traçando uma derivação mais à direita ao contrário.41\n\n\n5.4.4.2 9.2 A Abordagem Shift-Reduce\nO mecanismo fundamental por trás da maioria dos parsers ascendentes é o shift-reduce (deslocar-reduzir). O parser utiliza uma pilha para armazenar símbolos da gramática e toma uma de quatro ações possíveis em cada passo 49:\n\nShift (Deslocar): O próximo símbolo de entrada é movido (deslocado) para o topo da pilha.\nReduce (Reduzir): O parser reconhece que uma sequência de símbolos β no topo da pilha corresponde ao lado direito de uma regra de produção A→β. Ele então substitui (reduz) β na pilha pelo não-terminal A.\nAccept (Aceitar): A análise é concluída com sucesso. Isso ocorre quando a entrada foi totalmente consumida e a pilha contém apenas o símbolo inicial.\nError (Erro): Um erro sintático é encontrado, e o parser não pode continuar.\n\nA principal dificuldade em um parser shift-reduce é decidir quando deslocar e quando reduzir (um conflito shift/reduce) ou, ao decidir reduzir, qual regra usar se múltiplas corresponderem (um conflito reduce/reduce).49\n\n\n5.4.4.3 9.3 A Família de Analisadores LR\nA classe mais poderosa e amplamente utilizada de parsers ascendentes é a família LR. Eles são capazes de analisar uma classe de gramáticas significativamente maior do que os parsers LL.50 A notação LR significa:\n\nL: A entrada é lida da Esquerda (Left) para a direita.\nR: O parser constrói uma derivação mais à diReita (Rightmost) ao contrário.\n\nExistem várias variantes de parsers LR, que diferem principalmente na forma como suas tabelas de análise são construídas e na quantidade de informação de lookahead que utilizam para resolver conflitos 48:\n\nLR(0): O mais simples, não usa lookahead.\nSLR (Simple LR): Usa os conjuntos FOLLOW do não-terminal para decidir sobre as reduções.\nLALR(1) (Look-Ahead LR): Uma versão otimizada do LR(1) com tabelas menores, mas poder de reconhecimento ligeiramente reduzido. É a base para ferramentas como YACC e Bison.\nLR(1) Canônico: O mais poderoso da família, mas que gera tabelas de análise muito grandes.\n\nAo contrário dos parsers LL ansiosos, os parsers LR são pacientes. Eles não precisam decidir qual regra de produção usar no momento em que veem o primeiro símbolo de seu lado direito. Em vez disso, eles continuam a deslocar símbolos para a pilha até que o lado direito completo de uma produção (conhecido como handle) esteja no topo da pilha. Somente então eles realizam a redução. Essa capacidade de adiar a decisão até que mais contexto esteja disponível é a fonte de seu maior poder e de sua capacidade de lidar com uma gama mais ampla e natural de gramáticas sem a necessidade de reescritas extensivas.\nTabela de Ação/Desvio (Action/Goto) para um Parser LR |\n:— | AÇÃO | DESVIO |\nEstado | id | + | * | $ | E | T |\n0 | s2 | | | | 1 | 3 |\n1 | | s4 | | acc | | |\n2 | | r3 | r3 | r3 | | |\n3 | | r1 | s5 | r1 | | |\n4 | s2 | | | | 6 | 3 |\n… |… |… |… |… |… |… |\n(s = shift, r = reduce, acc = accept)\n\n\n\n5.4.5 Capítulo 10: Automatizando a Construção: Ferramentas Geradoras de Parsers\n\n5.4.5.1 10.1 O Conceito de Compilador de Compiladores\nA construção manual de um analisador sintático, especialmente um parser LR, é uma tarefa complexa, tediosa e propensa a erros. Para mitigar essa complexidade, foram desenvolvidas ferramentas especializadas conhecidas como geradores de parsers ou, mais ambiciosamente, compiladores de compiladores.52 Essas ferramentas automatizam o processo de criação de um parser a partir de uma especificação de alto nível da gramática da linguagem.\n\n\n5.4.5.2 10.2 YACC e GNU Bison\nAs ferramentas mais conhecidas e influentes nesta categoria são YACC e seu sucessor, Bison.\n\nYACC (Yet Another Compiler-Compiler): Desenvolvido na Bell Labs, YACC é a ferramenta canónica para gerar parsers LALR(1).52 Ele se tornou um padrão de fato em sistemas Unix.\nGNU Bison: É a implementação do projeto GNU de um gerador de parsers. É amplamente compatível com YACC, mas oferece recursos adicionais, como a geração de parsers GLR (Generalized LR) para lidar com gramáticas ambíguas e a geração de código em múltiplas linguagens (C, C++, Java).53\n\n\n\n5.4.5.3 10.3 Funcionamento\nO fluxo de trabalho com YACC ou Bison é o seguinte 52:\n\nEntrada: O desenvolvedor cria um arquivo de especificação (geralmente com a extensão .y). Este arquivo contém três seções: declarações, a gramática da linguagem escrita em uma notação semelhante à BNF, e uma seção de código auxiliar. O desenvolvedor pode associar ações de código (em C ou C++) a cada regra da gramática.\nProcessamento: A ferramenta (YACC ou Bison) lê o arquivo de especificação. Ela analisa a gramática, constrói o autômato LR e a tabela de análise correspondente, e verifica a existência de conflitos (shift/reduce ou reduce/reduce).\nSaída: Se a gramática for adequada (e.g., LALR(1)), a ferramenta gera um arquivo de código fonte (e.g., y.tab.c) que implementa a função do parser (tipicamente chamada yyparse). Esta função implementa o algoritmo shift-reduce dirigido pela tabela gerada. As ações de código fornecidas pelo desenvolvedor são incorporadas à função e são executadas sempre que a regra correspondente é reduzida. Essas ações são tipicamente usadas para construir a Árvore Sintática Abstrata.\n\n\n\n5.4.5.4 10.4 Sinergia com Lex/Flex\nYACC/Bison são projetados para lidar com a análise sintática e quase sempre são usados em conjunto com um gerador de analisador léxico, como Lex ou seu sucessor Flex. O Flex recebe uma especificação de padrões de tokens (usando expressões regulares) e gera um scanner (a função yylex). O parser gerado pelo Bison chama yylex para obter o próximo token da entrada, formando um pipeline coeso que transforma o texto fonte em uma AST.52\nO uso de ferramentas como YACC e Bison representa uma mudança de paradigma fundamental na construção de compiladores: da programação imperativa para a declarativa. Em vez de implementar manualmente o complexo algoritmo de um parser shift-reduce (a abordagem imperativa), o desenvolvedor fornece uma especificação de alto nível da gramática da linguagem (a abordagem declarativa). A ferramenta se encarrega de gerar a implementação de baixo nível. Essa separação de interesses — o quê (a sintaxe da linguagem) do como (o algoritmo de parsing) — aumenta drasticamente a produtividade, a robustez e a manutenibilidade do compilador.\n\n\n5.4.5.5 Referências citadas\n\nterminologia - O que é uma linguagem livre de contexto? - Stack …, acessado em agosto 19, 2025, https://pt.stackoverflow.com/questions/180927/o-que-%C3%A9-uma-linguagem-livre-de-contexto\n\nDúvida acerca da Hierarquia de Chomsky - Stack Overflow em Português, acessado em agosto 19, 2025, https://pt.stackoverflow.com/questions/372178/d%C3%BAvida-acerca-da-hierarquia-de-chomsky\n\nLinguagem regular – Wikipédia, a enciclopédia livre, acessado em agosto 19, 2025, https://pt.wikipedia.org/wiki/Linguagem_regular\n\nGramática regular – Wikipédia, a enciclopédia livre, acessado em agosto 19, 2025, https://pt.wikipedia.org/wiki/Gram%C3%A1tica_regular\n\nLinguagens Livres de Contexto - Marcus Vinícius Midena Ramos, acessado em agosto 19, 2025, https://www.marcusramos.com.br/univasf/livro-lfa-slides/cap4.pdf\n\nLema do Bombeamento | DECOM-UFOP, acessado em agosto 19, 2025, http://www.decom.ufop.br/anderson/BCC242/LemaBombeamento.pdf\n\nLema do Bombeamento - Aplicação para Linguagens Regulares e Livres de Contexto - facom/ufu, acessado em agosto 19, 2025, https://www.facom.ufu.br/~madriana/TC/LemaBomb.pdf\n\nteoria da computação unidade 2: autômatos e linguagens aula 1: lema do bombeamento professor - CIn UFPE, acessado em agosto 19, 2025, https://www.cin.ufpe.br/~lfsc/cursos/teoriadainformacao/unidade%202/cap%201_9%20-%20lema%20do%20bombeamento.pdf\n\nLema do Bombeamento - Douglas O. Cardoso, acessado em agosto 19, 2025, https://docardoso.github.io/project/lfa/04-bombeamento.pdf\n\nLinguagens Formais e Autômatos - Gitea -- aleph0, acessado em agosto 19, 2025, https://aleph0.info/cursos/lf/notas/lfa.pdf\n\nExercícios Resolvidos, acessado em agosto 19, 2025, http://wiki.icmc.usp.br/images/f/f1/Gramáticas Livres de Contextos.pdf\n\nLinguagens formais e autômatos, acessado em agosto 19, 2025, http://cm-kls-content.s3.amazonaws.com/201702/INTERATIVAS_2_0/LINGUAGENS_FORMAIS_E_AUTOMATOS/U1/LIVRO_UNICO.pdf\n\nAs linguagens livres de contexto são geradas por gramáticas livres de contexto? - Academia EITCA - EITCA Academy, acessado em agosto 19, 2025, https://pt.eitca.org/c%C3%ADber-seguran%C3%A7a/eitc-%C3%A9-cctf-fundamentos-da-teoria-da-complexidade-computacional/gram%C3%A1ticas-e-linguagens-livres-de-contexto/introdu%C3%A7%C3%A3o-a-gram%C3%A1ticas-e-linguagens-livres-de-contexto/s%C3%A3o-linguagens-livres-de-contexto-geradas-por-gram%C3%A1ticas-livres-de-contexto/\n\nGramática, acessado em agosto 19, 2025, http://www.din.uem.br/yandre/TC/gramatica-mini.pdf\n\nSímbolos terminais e não terminais – Wikipédia, a enciclopédia livre, acessado em agosto 19, 2025, https://pt.wikipedia.org/wiki/S%C3%ADmbolos_terminais_e_n%C3%A3o_terminais\n\nAnálise sintática (computação) – Wikipédia, a enciclopédia livre, acessado em agosto 19, 2025, https://pt.wikipedia.org/wiki/An%C3%A1lise_sint%C3%A1tica_(computa%C3%A7%C3%A3o)\n\nFases Da Compilação | PDF - Scribd, acessado em agosto 19, 2025, https://id.scribd.com/document/92994660/Fases-da-compilacao\n\nQual é a diferença entre uma derivação mais à esquerda e uma derivação mais à direita? - EITCA Academy, acessado em agosto 19, 2025, https://pt.eitca.org/c%C3%ADber-seguran%C3%A7a/eitc-%C3%A9-cctf-fundamentos-da-teoria-da-complexidade-computacional/gram%C3%A1ticas-e-linguagens-livres-de-contexto/introdu%C3%A7%C3%A3o-a-gram%C3%A1ticas-e-linguagens-livres-de-contexto/exame-revis%C3%A3o-introdu%C3%A7%C3%A3o-ao-contexto-gram%C3%A1ticas-e-idiomas-livres/qual-%C3%A9-a-diferen%C3%A7a-entre-uma-deriva%C3%A7%C3%A3o-mais-%C3%A0-esquerda-e-uma-deriva%C3%A7%C3%A3o-mais-%C3%A0-direita/\n\nGramáticas Livres de Contexto, acessado em agosto 19, 2025, https://www.inf.ufrgs.br/~johann/comp/aula06.Gramáticas Livres de Contextos.pdf\n\nCompiladores Capítulo 3: Análise Sintática 3.1 - Introdução - Facom-UFMS, acessado em agosto 19, 2025, http://www.facom.ufms.br/~ricardo/Courses/CompilerI-2009/Materials/Sintatic_Analisys.pdf\n\nÁrvores de Derivação | LL e LR | Ambiguidade | Compiladores - YouTube, acessado em agosto 19, 2025, https://www.youtube.com/watch?v=MLyFS7ClMf0\n\nGramáticas Independentes de Contexto - Autómatos e Linguagens de Programação, acessado em agosto 19, 2025, https://home.uevora.pt/~fc/alp/03-gramaticas_automatos_pilha/03.01-gramaticas_independentes_contexto.html\n\nAmbigüidade, acessado em agosto 19, 2025, https://www.inf.ufes.br/~tavares/labcomp2000/aula63.htm\n\nIntrodução a Gramáticas e Linguagens, acessado em agosto 19, 2025, https://web.icmc.usp.br/SCATUSU/RT/Notas_Didaticas/nd_10.pdf\n\nCurso de Teoria da Computação - Árvore de derivação e Ambiguidade - YouTube, acessado em agosto 19, 2025, https://www.youtube.com/watch?v=bbWgP_khQZg\n\nEstrutura de um Compilador · Compiladores para Humanos, acessado em agosto 19, 2025, https://johnidm.gitbooks.io/compiladores-para-humanos/content/part1/structure-of-a-compiler.html\n\nCompiladores - Ciência da Computação - Resumo da Prova PosComp, acessado em agosto 19, 2025, https://cienciadacomputacao.wiki.br/19_Tecnologia_Compiladores.html\n\nAutômato com pilha – Wikipédia, a enciclopédia livre, acessado em agosto 19, 2025, https://pt.wikipedia.org/wiki/Aut%C3%B4mato_com_pilha\n\nSCC-5832 - Capítulo 2 Linguagens Livres de Contexto e Autômatos de Pilha - USP, acessado em agosto 19, 2025, http://wiki.icmc.usp.br/images/a/a2/SCC5832Cap2.pdf\n\nAutômatos e Pilha & Gramáticas Livres de Contexto - YouTube, acessado em agosto 19, 2025, https://www.youtube.com/watch?v=1Xe2TUvah1I\n\nAula 13 – Autômato com Pilha - Universidade Federal de Alfenas, acessado em agosto 19, 2025, https://www.bcc.unifal-mg.edu.br/~humberto/disciplinas/2011_1_lfa/aulas/aula_13_AutomatoComPilha.pdf\n\nLema do bombeamento para linguagens livre de contexto …, acessado em agosto 19, 2025, https://pt.wikipedia.org/wiki/Lema_do_bombeamento_para_linguagens_livre_de_contexto\n\nLema do bombeamento para linguagens livres de contexto - Wikipédia, acessado em agosto 19, 2025, https://pt.wikipedia.org/wiki/Lema_do_bombeamento_para_linguagens_livres_de_contexto\n\nTeoria da Computação 40 - Lema do Bombeamento para linguagens regulares - YouTube, acessado em agosto 19, 2025, https://www.youtube.com/watch?v=0hbLhEdxuss\n\nLinguagens Livres de Contexto: Lema do Bombeamento e Propriedades de Fechamento - Andrei Rimsa Alvares, acessado em agosto 19, 2025, http://rimsa.com.br/documents/lectures/decom035/c2/lessons/Aula10.pdf\n\n*c*} = {abncn, n _!$# %‘& (*) + , -. /+ /’ ’/+-. ão fechadas sob 1, acessado em agosto 19, 2025, https://www.cos.ufrj.br/~rps/monitoria/lfa/com162[1]_list2.pdf\n\nQual é a relação entre linguagens decidíveis e linguagens livres de contexto?, acessado em agosto 19, 2025, https://pt.eitca.org/c%C3%ADber-seguran%C3%A7a/eitc-%C3%A9-cctf-fundamentos-da-teoria-da-complexidade-computacional/gram%C3%A1ticas-e-linguagens-livres-de-contexto/exemplos-de-gram%C3%A1ticas-livres-de-contexto/exemplos-de-revis%C3%A3o-de-exame-de-gram%C3%A1ticas-livres-de-contexto/qual-%C3%A9-a-rela%C3%A7%C3%A3o-entre-linguagens-decid%C3%ADveis-e-linguagens-livres-de-contexto/\n\nLema do Bombeamento Linguagens Livres de Contexto - DECOM-UFOP |, acessado em agosto 19, 2025, http://www.decom.ufop.br/anderson/2_2011/BCC244/LemaDoBombeamentoFCPL.pdf\n\nAs fases de um compilador, acessado em agosto 19, 2025, http://wiki.icmc.usp.br/images/3/32/SCC_605_Estrutura_de_um_Compilador.pdf\n\nCompiladores/Projecto de Compiladores/Fases Desenvolvimento - Wiki**3, acessado em agosto 19, 2025, https://web.tecnico.ulisboa.pt/~david.matos/w/pt/index.php/Compiladores/Projecto_de_Compiladores/Fases_Desenvolvimento\n\ncompiladores - O que é e como funciona a análise sintática …, acessado em agosto 19, 2025, https://pt.stackoverflow.com/questions/181635/o-que-%C3%A9-e-como-funciona-a-an%C3%A1lise-sint%C3%A1tica-ascendente-e-descendente\n\nTop-Down Parsing - Wiki**3, acessado em agosto 19, 2025, https://web.tecnico.ulisboa.pt/~david.matos/w/pt/index.php/Top-Down_Parsing\n\nAnálise Sintática - IC-Unicamp, acessado em agosto 19, 2025, https://ic.unicamp.br/~sandro/cursos/mc910/2009/slides/cap3-parser.pdf\n\nParsers LL(1) o mundo obscuro da análise sintática - Frank de Alcantara, acessado em agosto 19, 2025, https://frankalcantara.com/parsers-ll(1)/\n\nAnálise Sintática Descendente, acessado em agosto 19, 2025, https://erinaldosn.files.wordpress.com/2011/06/aula-10-anc3a1lise-sintc3a1tica-descendente.pdf\n\nCompiladores 9 - Algoritmo de Análise Sintática LL(1) ou Analisador Sintático LL(1) ou Parser LL(1) - YouTube, acessado em agosto 19, 2025, https://www.youtube.com/watch?v=yTgrbMkKk6M\n\nComo funciona a análise sintática ascendente (bottom-up parsing)? : r/golang - Reddit, acessado em agosto 19, 2025, https://www.reddit.com/r/golang/comments/kwju6j/how_the_bottom_up_parsing_works/?tl=pt-br\n\nBottom-Up Parsing - Wiki**3, acessado em agosto 19, 2025, https://web.tecnico.ulisboa.pt/~david.matos/w/pt/index.php/Bottom-Up_Parsing\n\nAnálise Sintática (Cap. 04) Análise Sintática Ascendente BottomUp - Facom-UFMS, acessado em agosto 19, 2025, http://www.facom.ufms.br/~ricardo/Courses/CompilerI-2009/Lectures/CompilersI_Lec06_SyntaticAnalysis%283%29.pdf\n\nCompiladores (CC3001) Aula 6: Análise sintática bottom-up - DCC/FCUP, acessado em agosto 19, 2025, https://www.dcc.fc.up.pt/~pbv/aulas/compiladores/teoricas/aula06.pdf\n\nCompila10 - Análise sintática LR, Transição e fechamento - YouTube, acessado em agosto 19, 2025, https://www.youtube.com/watch?v=mGD7YpFSX-c\n\nYacc - Wikipedia, acessado em agosto 19, 2025, https://en.wikipedia.org/wiki/Yacc\n\nGNU Bison - Wikipedia, acessado em agosto 19, 2025, https://en.wikipedia.org/wiki/GNU_Bison\n\nBison - GNU Project - Free Software Foundation, acessado em agosto 19, 2025, https://www.gnu.org/software/bison/\n\nbison.pdf - GNU, acessado em agosto 19, 2025, https://www.gnu.org/s/bison/manual/bison.pdf",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linguagens Livres de Contexto</span>"
    ]
  },
  {
    "objectID": "04-Gramaticas.html#chomsky-referências",
    "href": "04-Gramaticas.html#chomsky-referências",
    "title": "5  Linguagens Livres de Contexto",
    "section": "5.5 chomsky referências",
    "text": "5.5 chomsky referências\n(article?){chomsky1959formal, title={On certain formal properties of grammars}, author={Chomsky, Noam}, journal={Information and Control}, volume={2}, number={2}, pages={137–167}, year={1959}, publisher={Elsevier} }\n(book?){chomsky1957syntactic, title={Syntactic structures}, author={Chomsky, Noam}, year={1957}, publisher={Mouton} }\n(article?){mcculloch1943logical, title={A logical calculus of the ideas immanent in nervous activity}, author={McCulloch, Warren S and Pitts, Walter}, journal={The bulletin of mathematical biophysics}, volume={5}, number={4}, pages={115–133}, year={1943}, publisher={Springer} }\n(article?){kleene1956representation, title={Representation of events in nerve nets and finite automata}, author={Kleene, Stephen Cole}, journal={Automata studies}, volume={34}, pages={3–41}, year={1956}, publisher={Princeton University Press} }\n(article?){post1936finite, title={Finite combinatory processes—formulation 1}, author={Post, Emil L}, journal={Journal of Symbolic Logic}, volume={1}, number={3}, pages={103–105}, year={1936}, publisher={Cambridge University Press} }\n(incollection?){chomsky1956three, title={Three models for the description of language}, author={Chomsky, Noam}, booktitle={IRE Transactions on information theory}, volume={2}, number={3}, pages={113–124}, year={1956}, publisher={IEEE} }\n(article?){harris1951methods, title={Methods in structural linguistics}, author={Harris, Zellig S}, year={1951}, publisher={University of Chicago Press} }\n(article?){chomsky1959review, title={A review of BF Skinner’s Verbal Behavior}, author={Chomsky, Noam}, journal={Language}, volume={35}, number={1}, pages={26–58}, year={1959}, publisher={Linguistic Society of America} }\n(misc?){wikipedia_chomsky_hierarchy, title={Chomsky hierarchy}, author={{Wikipedia contributors}}, year={2024}, howpublished={}, note={Accessed: 2025-08-19} }\n(misc?){britannica_chomsky, title={Noam Chomsky | Biography, Theories, Books, Psychology, & Facts}, author={{Encyclopædia Britannica}}, year={2024}, howpublished={}, note={Accessed: 2025-08-19} }\n(article?){pmc_formal_language_theory, title={Formal language theory: refining the Chomsky hierarchy}, author={Jäger, Gerhard and Rogers, James}, journal={Philosophical Transactions of the Royal Society B}, volume={367}, number={1598}, pages={1956–1970}, year={2012}, publisher={The Royal Society} }\n(misc?){devopedia_chomsky, title={Chomsky Hierarchy}, author={{Devopedia}}, year={2024}, howpublished={}, note={Accessed: 2025-08-19} }\n(article?){neural_networks_chomsky, title={Neural Networks and the Chomsky Hierarchy}, author={Deletang, Grégoire and Ruoss, Anian and Mediano, Pedro AM and Jaggi, Martin and others}, journal={arXiv preprint arXiv:2207.02098}, year={2022} }\n(book?){sipser2012introduction, title={Introduction to the Theory of Computation}, author={Sipser, Michael}, year={2012}, publisher={Cengage Learning}, edition={3rd} }\n(book?){hopcroft2006introduction, title={Introduction to automata theory, languages, and computation}, author={Hopcroft, John E and Motwani, Rajeev and Ullman, Jeffrey D}, year={2006}, publisher={Pearson/Addison Wesley}, edition={3rd} }\n(misc?){number_analytics_chomsky, title={Ultimate Guide to the Chomsky Hierarchy}, author={{Number Analytics}}, year={2024}, howpublished={}, note={Accessed: 2025-08-19} }\n(misc?){cognitive_revolution, title={The Cognitive Revolution}, author={{UCF Pressbooks}}, year={2024}, howpublished={}, note={Accessed: 2025-08-19} }\n(article?){backus1959syntax, title={The syntax and semantics of the proposed international algebraic language of the Zurich ACM-GAMM Conference}, author={Backus, John W}, journal={Proceedings of the International Conference on Information Processing}, pages={125–132}, year={1959}, organization={UNESCO} }\n(book?){naur1963revised, title={Revised report on the algorithmic language ALGOL 60}, author={Naur, Peter and others}, journal={Communications of the ACM}, volume={6}, number={1}, pages={1–17}, year={1963}, publisher={ACM} }",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linguagens Livres de Contexto</span>"
    ]
  },
  {
    "objectID": "parsers.html",
    "href": "parsers.html",
    "title": "6  Parsers LL(1): O Mundo da Análise Sintática",
    "section": "",
    "text": "6.1 Definição e Características dos Parsers LL(1)\nOs parsers preditivos são analisadores sintáticos descendentes (top-down) que utilizam um único símbolo de lookahead (antecipação) para determinar a regra de produção correta a ser aplicada em cada etapa da análise. Eles predizem qual regra usar com base no próximo símbolo da entrada e no não-terminal atualmente sendo analisado. O termo \\(LL(1)\\) significa:\nUm parser \\(LL(1)\\), requer uma gramática \\(LL(1)\\). Nesta gramática não pode existir qualquer ambiguidade na escolha da regra de produção que será aplicada a cada símbolo de lookahead. Além disso, a gramática não pode ter recursão à esquerda, seja esta recursão direta ou indireta. A recursão à esquerda é um desafio considerável. Existem duas formas de recursão à esquerda:\nAlém do perigo do laço de repetição infinito que faz o pobre Turing se revolver no túmulo, a recursão à esquerda impede o desenvolvimento de uma Tabela de Derivação. Graças a criação de regras em conflito. Duas ou mais regras, para a mesma combinação de símbolo terminal e símbolo não-terminal em um determinado momento do processo de parser.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parsers LL(1): O Mundo da Análise Sintática</span>"
    ]
  },
  {
    "objectID": "parsers.html#definição-e-características-dos-parsers-ll1",
    "href": "parsers.html#definição-e-características-dos-parsers-ll1",
    "title": "6  Parsers LL(1): O Mundo da Análise Sintática",
    "section": "",
    "text": "L: Left-to-right scan (varredura da esquerda para a direita) da entrada.\nL: Leftmost derivation (derivação mais à esquerda) da gramática.\n1: Um símbolo de lookahead (antecipação) para tomada de decisão.\n\n\n\nRecursão à Esquerda Direta: ocorre quando um símbolo não-terminal pode ser derivado em uma sequência que começa com ele mesmo. Por exemplo, na regra \\(A → Aa \\mid b\\), o símbolo \\(A\\) pode ser substituído no processo de derivação em \\(Aa\\), onde \\(A\\) aparece novamente no início da regra. E aqui está o laço infinito.\nRecursão à Esquerda Indireta: acontece quando um símbolo não-terminal pode ser derivado em uma sequência que começa com outro símbolo não-terminal, que por sua vez pode ser derivado de volta ao símbolo original. Ilustrando, nas regras \\(A → Ba\\) e \\(B → Ab\\), \\(A\\) deriva para \\(Ba\\), \\(B\\) deriva para \\(Ab\\) e \\(Ab\\) pode derivar novamente para \\(A\\), criando outro laço infinito.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parsers LL(1): O Mundo da Análise Sintática</span>"
    ]
  },
  {
    "objectID": "parsers.html#eliminação-da-recursão-à-esquerda",
    "href": "parsers.html#eliminação-da-recursão-à-esquerda",
    "title": "6  Parsers LL(1): O Mundo da Análise Sintática",
    "section": "6.2 Eliminação da Recursão à Esquerda",
    "text": "6.2 Eliminação da Recursão à Esquerda\nFelizmente, existem técnicas para eliminar a recursão à esquerda em gramáticas livres de contexto. Uma técnica comum e eficaz envolve a introdução de novos símbolos não terminais e a substituição de regras recursivas por regras equivalentes que não apresentem recursão. Em alguns casos, a substituição direta dos símbolos não terminais recursivos por suas respectivas regras pode ser suficiente para eliminar a recursão à esquerda direta. Outra técnica, a fatoração à esquerda, pode ser utilizada para eliminar ambiguidades na gramática, mas não resolve diretamente o problema da recursão.\nExemplo 1: eliminando a Recursão à Esquerda Direta, considere a regra \\(A → Aa \\mid b\\). Esta regra pode ser reescrita como:\n\n\\(A \\to bA'\\)\n\\(A' \\to aA' \\mid \\varepsilon\\)\n\nAgora, a gramática não contém mais recursão à esquerda direta. Este é um exemplo simples, adequado a este texto cujo objetivo é o parser em si. A nova regra inclui o não-terminal \\(A'\\), que permite zero ou mais repetições do símbolo \\(a\\). O uso de \\(\\varepsilon\\) (a produção vazia) permite terminar a derivação de \\(A'\\). Será?\nPara verificar a recursão à esquerda indireta, você precisa observar se é possível derivar uma recursão por meio de uma cadeia de substituições:\n\nSubstituindo \\(A\\):\n\n\\(A \\rightarrow bA'\\)\n\nSubstituindo \\(A'\\):\n\n\\(A' \\rightarrow aA'\\)\n\\(A' \\rightarrow \\varepsilon\\)\n\n\nObserve que substituindo \\(A\\) por \\(bA'\\) e deporque \\(A'\\) por \\(aA'\\) ou \\(\\varepsilon\\) não leva de volta a \\(A\\). E parece não haver recursão. Contudo, é necessário verificar se foi criada alguma recursão à esquerda indireta, focando em \\(A'\\):\n\nPrimeira substituição:\n\n\\(A \\rightarrow bA'\\)\n\nSubstituindo \\(A'\\) por \\(aA'\\):\n\n\\(bA' \\rightarrow b(aA')\\)\n\\(bA' \\rightarrow baA'\\)\n\nSubstituindo novamente \\(A'\\) por \\(aA'\\):\n\n\\(baA' \\rightarrow baaA'\\)\n\nE assim por diante… Aqui, \\(A'\\) substitui a si próprio com um prefixo \\(a\\), mas isto não cria recursão indireta ao \\(A'\\) inicial de forma a levar a uma cadeia circular que retorne ao símbolo inicial \\(A\\). A gramática transformada não apresenta recursão à esquerda indireta para \\(A'\\).\n\\(A\\): Não tem recursão à esquerda direta nem indireta, porque \\(A \\rightarrow bA'\\) começa com um terminal.\n\\(A'\\): A regra \\(A' \\rightarrow aA' \\mid \\varepsilon\\) apenas permite que \\(A'\\) produza cadeias de \\(a\\) seguidos possivelmente por \\(\\varepsilon\\), sem retornar a um estado anterior que causaria recursão indireta.\n\nPortanto, a transformação feita elimina a recursão à esquerda direta sem introduzir recursão à esquerda indireta. A recursão â esquerda indireta é mais complexa e requer um texto específico para o assunto. Mas, em linhas gerais você terá que refazer a gramática em face dos objetivos originais para eliminar este tipo de recursão. Vou deixar por sua conta resolver a recursão à esquerda indireta e a ambiguidade que surjam nas gramáticas que você criar.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parsers LL(1): O Mundo da Análise Sintática</span>"
    ]
  },
  {
    "objectID": "parsers.html#elementos-fundamentais-do-parser-ll1",
    "href": "parsers.html#elementos-fundamentais-do-parser-ll1",
    "title": "6  Parsers LL(1): O Mundo da Análise Sintática",
    "section": "6.3 Elementos Fundamentais do Parser LL(1)",
    "text": "6.3 Elementos Fundamentais do Parser LL(1)\nComo a classe de gramáticas para um parser \\(LL(1)\\) é limitada (nem todas as gramáticas livres de contexto são \\(LL(1)\\), é muito comum que seja necessário modificar a sua ideia original de gramática para eliminar ambiguidades e recursões à esquerda. Um parser \\(LL(1)\\), para funcionar, precisa dos seguintes elementos:\n\nTabela de Derivação: O parser \\(LL(1)\\) utiliza uma Tabela de Derivação, ou Tabela de Derivação, que mapeia cada combinação de não-terminal e terminal (ou símbolo de fim de entrada) para a regra de produção que deve ser aplicada. Essa tabela é construída a partir da gramática e dos conjuntos \\(FIRST\\) e \\(FOLLOW\\) e será o mapa que guiará todo o processo de análise sintática.\nPilha e Buffer: O parser mantém uma pilha e lê a entrada da esquerda para a direita, carácter por carácter. A pilha inicialmente contém o símbolo inicial da gramática e o símbolo de fim de entrada, \\(\\$\\). A entrada frequentemente é mantida em uma estrutura de dados com funcionalidades de buffer, que pode ser a própria string que está sendo analisada.\nAnálise: Em cada passo:\n\nO parser consulta a Tabela de Derivação usando como índices o não-terminal no topo da pilha e o próximo símbolo da entrada.\nA tabela indica a produção a ser aplicada.\nO não-terminal no topo da pilha é substituído pelos símbolos da produção (empilhados em ordem inversa).\nSe o topo da pilha for um terminal que coincide com o próximo símbolo da entrada, ambos são removidos da pilha e da entrada.\n\nSucesso ou Erro: A análise termina com sucesso quando a pilha e a entrada estão vazias. Caso contrário, ocorre um erro sintático. Erros poderão ocorrer durante o processo sempre que a combinação de símbolos na pilha e no buffer apontarem para uma célula vazia da Tabela de Derivação.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parsers LL(1): O Mundo da Análise Sintática</span>"
    ]
  },
  {
    "objectID": "parsers.html#conjuntos-first-follow-e-símbolos-nullable",
    "href": "parsers.html#conjuntos-first-follow-e-símbolos-nullable",
    "title": "6  Parsers LL(1): O Mundo da Análise Sintática",
    "section": "6.4 Conjuntos FIRST, FOLLOW e Símbolos Nullable",
    "text": "6.4 Conjuntos FIRST, FOLLOW e Símbolos Nullable\nCriar um parser \\(LL(1)\\) geralmente exige mais do que apenas os conjuntos \\(FIRST\\) e \\(FOLLOW\\). Este é um processo que envolve garantir a adequação da gramática e seguir etapas específicas para construir a Tabela de Derivação. Vou explorar esse processo de forma mais clara e detalhada, ainda neste texto.\nA gramática criada para o parser \\(LL(1)\\) não pode ter qualquer ambiguidade. Ou seja, cada string da linguagem deve ser derivada em uma e apenas uma árvore sintática. Acrescente-se a isso que a gramática não deve ter regras do tipo \\(A \\rightarrow A\\alpha\\) (recursão direta) nem \\(A \\rightarrow B\\alpha\\) e \\(B \\rightarrow AB\\) (recursão indireta). Ou seja, uma gramática sem recursão à esquerda.\nUma vez que tenha uma gramática adequada, será possível construir uma Tabela de Derivação \\(LL(1)\\). Para isso vamos precisar:\n\nEncontrar os conjuntos \\(FIRST\\) e \\(FOLLOW\\):\n\n\\(FIRST(A)\\): O conjunto de terminais que podem iniciar uma derivação de \\(A\\).\n\\(FOLLOW(A)\\): O conjunto de terminais que podem aparecer imediatamente após \\(A\\) em uma derivação.\n\nConsiderar Símbolos Nullable:\n\nUm símbolo é nullable se pode derivar a cadeia vazia (\\(\\epsilon\\)).\nO cálculo de \\(FIRST\\) e \\(FOLLOW\\) precisa levar em conta os símbolos nullable.\n\nPreencher a Tabela:\n\nPara cada produção \\(A \\rightarrow \\alpha\\):\n\nSe \\(\\alpha\\) começa com um terminal \\(a\\), adicione a produção à célula \\((A, a)\\).\nSe \\(\\alpha\\) é nullable e \\(b\\) está em \\(FOLLOW(A)\\), adicione a produção à célula \\((A, b)\\).\n\n\n\nUm símbolo não-terminal é nullable se ele pode ser derivado para a cadeia vazia (\\(\\varepsilon\\)), ou seja, se ele pode desaparecer durante uma derivação. Por exemplo, na produção \\(A \\rightarrow BC\\), se tanto \\(B\\) quanto \\(C\\) são nullable, então \\(A\\) também é nullable. A presença de símbolos nullable exige alguns ajustes nos algoritmos de identificação de \\(FIRST\\) e \\(FOLLOW\\):\n\n\\(FIRST\\): Se um símbolo é nullable, será necessário adicioná-lo \\(\\epsilon\\) ao seu conjunto \\(FIRST\\). Além disso, ao calcular o \\(FIRST\\) de uma sequência de símbolos, se um símbolo é nullable, também é necessário considerar o \\(FIRST\\) do próximo símbolo.\n\\(FOLLOW\\): Se um símbolo é nullable, ao calcular o \\(FOLLOW\\) de um símbolo que o precede, também é imprescindível considerar o \\(FOLLOW\\) do símbolo anterior.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parsers LL(1): O Mundo da Análise Sintática</span>"
    ]
  },
  {
    "objectID": "parsers.html#exemplo-com-símbolos-nullable",
    "href": "parsers.html#exemplo-com-símbolos-nullable",
    "title": "6  Parsers LL(1): O Mundo da Análise Sintática",
    "section": "6.5 Exemplo com Símbolos Nullable",
    "text": "6.5 Exemplo com Símbolos Nullable\nExemplo 2: Considere a gramática representada pelo conjunto de regras de produção a seguir:\n\n\\(S → AB\\)\n\\(A → \\varepsilon\\)\n\\(B → b \\mid \\varepsilon\\)\n\nCálculo de Nullable\n\n\\(A \\rightarrow \\epsilon\\), então \\(A\\) é nullable.\n\\(B \\rightarrow \\epsilon\\), então \\(B\\) é nullable.\nPara \\(S\\), ambos \\(A\\) e \\(B\\) são nullable, então \\(S\\) é nullable.\n\nCálculo de \\(FIRST\\)\n\n\\(FIRST(A) = \\{ \\epsilon \\}\\)\n\\(FIRST(B) = \\{ b, \\epsilon \\}\\)\n\\(FIRST(S) = FIRST(AB) = FIRST(A) ∪ FIRST(B) = \\{ \\epsilon \\} ∪ \\{ b, \\epsilon \\} = \\{ b, \\epsilon \\}\\)\n\nCálculo de \\(FOLLOW\\)\n\n\\(FOLLOW(S) = \\{ \\$ \\}\\)\n\\(FOLLOW(A) = FIRST(B) = \\{ b, \\epsilon \\} (excluindo \\quad \\epsilon)\\)\n\\(FOLLOW(B) = FOLLOW(S) = \\{ \\$ \\}\\)\n\nNeste exemplo, os símbolos nullable influenciam diretamente os conjuntos \\(FIRST\\) e \\(FOLLOW\\), permitindo a construção correta da tabela de parsing \\(LL(1)\\) e garantindo que a gramática pode ser analisada de forma preditiva com um símbolo de lookahead.\nTabela de Derivação \\(LL(1)\\)\n\n\n\n\n\\(b\\)\n\\(\\$\\)\n\n\n\n\n\\(S\\)\n\\(S \\rightarrow AB\\)\n\\(S \\rightarrow AB\\)\n\n\n\\(A\\)\n\\(A \\rightarrow \\varepsilon\\)\n\\(A \\rightarrow \\varepsilon\\)\n\n\n\\(B\\)\n\\(B \\rightarrow b\\)\n\\(B \\rightarrow \\varepsilon\\)\n\n\n\nObservações Importantes sobre as regras de produção:\n\n\\(S \\rightarrow AB\\): se o próximo símbolo de entrada for \\(b\\) ou \\(\\$\\), a produção \\(S \\rightarrow AB\\) deve ser aplicada.\n\\(A \\rightarrow \\epsilon\\): se o próximo símbolo de entrada for \\(b\\) ou \\(\\$\\), a produção \\(A \\rightarrow \\epsilon\\) deve ser aplicada.\n\\(B \\rightarrow b\\): se o próximo símbolo de entrada for \\(b\\), a produção \\(B \\rightarrow b\\) deve ser aplicada.\n\\(B \\rightarrow \\epsilon\\): se o próximo símbolo de entrada for \\(\\$\\), a produção \\(B \\rightarrow \\epsilon\\) deve ser aplicada.\n\nVoltando à gramática original: perceba que a gramática original é ambígua, porque a string vazia, \\(\\varepsilon\\), pode ser derivada de formas diferentes. Esta gramática foi criada, simples, para destacar a ambiguidade. Mas podemos modificar a esta gramática a nosso bel prazer. Lembre-se a linguagem é sua. Sendo assim:\n$1. S b $\nSe observarmos as regras de produção:\n\n\\(S \\rightarrow b\\): essa produção gerará a string \\(b\\).\n\\(S \\rightarrow \\epsilon\\): essa produção gerará a string vazia.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parsers LL(1): O Mundo da Análise Sintática</span>"
    ]
  },
  {
    "objectID": "parsers.html#tabela-de-derivação-ll1-para-a-gramática-modificada",
    "href": "parsers.html#tabela-de-derivação-ll1-para-a-gramática-modificada",
    "title": "6  Parsers LL(1): O Mundo da Análise Sintática",
    "section": "6.6 Tabela de Derivação \\(LL(1)\\) para a Gramática Modificada",
    "text": "6.6 Tabela de Derivação \\(LL(1)\\) para a Gramática Modificada\n\n\n\n\nb\n\\(\\$\\)\n\n\n\n\n\\(S\\)\n\\(S \\rightarrow b\\)\n\\(S \\rightarrow \\epsilon\\)\n\n\n\nObservações importantes sobre as regras de produção:\n\nSe o próximo símbolo de entrada for \\(b\\), a produção \\(S \\rightarrow b\\) será aplicada.\nSe o próximo símbolo de entrada for \\(\\$\\) (fim da entrada), a produção \\(S \\rightarrow \\epsilon\\) será aplicada.\n\nE pronto! Neste ponto do texto vimos tudo que precisamos ver e podemos parar. A pobre leitora, nesta altura do campeonato, deve estar pensando mal de mim. Eu sei, parece confuso. Talvez dois exemplos mais detalhados ajudem.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parsers LL(1): O Mundo da Análise Sintática</span>"
    ]
  },
  {
    "objectID": "parsers.html#o-exemplo-mais-comum-de-todos",
    "href": "parsers.html#o-exemplo-mais-comum-de-todos",
    "title": "6  Parsers LL(1): O Mundo da Análise Sintática",
    "section": "6.7 O Exemplo Mais Comum de Todos",
    "text": "6.7 O Exemplo Mais Comum de Todos\nO exemplo a seguir está em todos os sites, livros e aulas que eu já vi disponíveis na internet. É tão comum que não me dei ao trabalho de procurar sua origem. Meu instinto me diz que deve ser do livro do Aho, mas não fui conferir. É um exemplo tão bom que deve ser do Aho. Enfim, vamos ao trabalho:\n\n6.7.1 Gramática\nConsidere a gramática representada pelo conjunto de regras de produção a seguir:\n\\(1. S \\rightarrow E\\)\n\\(2. E \\rightarrow E + T \\mid T\\)\n\\(3. T \\rightarrow T * F \\mid F\\)\n\\(4. F \\rightarrow (E) \\mid id\\)\nQue permite criar os seguintes conjuntos:\n\n\n6.7.2 \\(FIRST\\)\n\n\\(FIRST(S) = FIRST(E)\\)\n\\(FIRST(E) = \\{ id, ( \\}\\)\n\\(FIRST(T) = \\{ id, ( \\}\\)\n\\(FIRST(F) = \\{ id, ( \\}\\)\n\n\n\n6.7.3 \\(FOLLOW\\)\n\n\\(FOLLOW(S) = \\{ \\$ \\}\\)\n\\(FOLLOW(E) = \\{ +, ), \\$ \\}\\)\n\\(FOLLOW(T) = \\{ +, *, ), \\$ \\}\\)\n\\(FOLLOW(F) = \\{ +, *, ), \\$ \\}\\)\n\n\n\n6.7.4 \\(NULLABLE\\)\nNenhuma das produções é nullable.\nOs quais, por sua vez, irão permitir a criação da seguinte Tabela de Derivação\n\n\n6.7.5 Tabela de Derivação \\(LL(1)\\)\n\n\n\n\nid\n+\n*\n(\n)\n$\n\n\n\n\nS\nE\n\n\nE\n\n\n\n\nE\nT\n\n\nT\n\n\n\n\nE\nT\nE + T\n\nT\nε\nε\n\n\nT\nF\nε\nT * F\nF\nε\nε\n\n\nF\nid\n\n\n(E)\n\n\n\n\n\nCom a Tabela de Derivação em mãos, podemos partir para a criação do parser. Não vou criar nenhum algoritmo agora, na verdade, vou apenas reproduzir o processo passo a passo, na esperança que a corajosa leitora entenda o algoritmo muito antes de vê-lo formalizado ou implementado.\n\n\n6.7.6 Processo de Parser Testando com \\(id + id * id\\)\nPara verificar se a string id + id  id* faz parte da linguagem representada na gramática dada pode ser identificada por um parser \\(LL(1)\\) usando a Tabela de Derivação acima. Seguiremos um processo de análise preditiva descendente, começando pelo símbolo inicial. Usaremos uma pilha e a própria string de entrada como buffer\nVou começar com a pilha contendo o símbolo inicial da gramática, \\(S\\), e a string de entrada id + id  id$, onde o \\(\\$\\) indica o final da entrada. Se você estiver fazendo o parser* na mão deverá seguir os seguintes passos:\n\nVerificar o topo da pilha (\\(S\\)) e o próximo símbolo de entrada (‘id’).\nUse a Tabela de Derivação para encontrar a produção apropriada, que, neste caso, será \\(S \\rightarrow E\\).\nSubstituir \\(S\\) pela produção \\(E\\) na pilha.\nO próximo passo é verificar o topo da pilha (\\(E\\)) e o próximo símbolo de entrada (‘id’).\nA Tabela indica a regra \\(E \\rightarrow T\\). Substitua \\(E\\) por \\(T\\).\nO próximo passo é verificar o topo da pilha (\\(T\\)) e o próximo símbolo de entrada (‘id’).\nA Tabela indica a regra \\(T \\rightarrow F\\). Substitua \\(T\\) por \\(F\\).\nO próximo passo é verificar o topo da pilha (\\(F\\)) e o próximo símbolo de entrada (‘id’).\nA Tabela indica \\(F \\rightarrow id\\). Substitua \\(F\\) por ‘id’.\nAgora a pilha tem ‘id’ e a entrada é “id + id * id$”. O próximo passo é consumir ‘id’ da entrada e da pilha, o valor na pilha e no buffer coincidem.\nO topo da pilha agora é \\(T\\) e o próximo símbolo de entrada é ‘+’.\nA Tabela indica \\(E \\rightarrow E + T\\). Substituir \\(E\\) por \\(E + T\\).\nAgora a pilha tem \\(T + T\\) e a entrada é “+ id * id$”. Consumir ‘+’ da entrada e da pilha, porque eles coincidem.\nO próximo passo é verificar o topo da pilha \\(T\\) e o próximo símbolo de entrada (‘id’).\nA Tabela indica \\(T \\rightarrow F\\). Substituir \\(T\\) por \\(F\\).\nO próximo passo é verificar o topo da pilha \\(F\\) e o próximo símbolo de entrada (‘id’).\nA Tabela indica \\(F \\rightarrow id\\). Substituir \\(F\\) por ‘id’.\nConsumir ‘id’ da entrada e da pilha, porque eles coincidem.\nO topo da pilha agora é \\(T\\) e o próximo símbolo de entrada é ’*’.\nA Tabela indica \\(T \\rightarrow T * F\\). Substituir \\(T\\) por \\(T * F\\).\nConsumir ’*’ da entrada e da pilha, porque eles coincidem.\nO próximo passo é verificar o topo da pilha \\(T\\) e o próximo símbolo de entrada (‘id’).\nA Tabela indica \\(T \\rightarrow F\\). Substituir \\(T\\) por \\(F\\).\nO próximo passo é verificar o topo da pilha \\(F\\) e o próximo símbolo de entrada (‘id’).\nA Tabela indica \\(F \\rightarrow id\\). Substituir \\(F\\) por ‘id’.\nConsumir ‘id’ da entrada e da pilha, porque eles coincidem.\nFinalmente, a pilha está vazia e a entrada foi completamente consumida.\n\nComo foi possível consumir toda string da entrada, id + id  id$, e esvaziar a pilha sem encontrar nenhum erro, a string* id + id  id* é de fato parte da linguagem identificada pelo parser \\(LL(1)\\) que poderá ser criado pela Tabela de Derivação que definimos. Mas estamos em universo onde o céu é sempre azul e as rosas estão sempre desbrochadas.\n\n\n6.7.7 Testando com a string id - id  id*\nVamos analisar a string id - id  id* utilizando a Tabela de Derivação \\(LL(1)\\) que foi estabelecida anteriormente, seguindo os passos do parser \\(LL(1)\\). Começamos com a pilha inicial contendo o símbolo inicial \\(S\\) e a entrada “id - id * id$”.\n\nVerificar o topo da pilha (\\(S\\)) e o próximo símbolo de entrada (‘id’).\nUse a Tabela de Derivação para encontrar a produção apropriada, que, neste caso, será \\(S \\rightarrow E\\).\nSubstituir \\(S\\) pela produção \\(E\\) na pilha.\nO próximo passo é verificar o topo da pilha (\\(E\\)) e o próximo símbolo de entrada (‘id’).\nA Tabela indica a regra \\(E \\rightarrow T\\). Substitua \\(E\\) por \\(T\\).\nO próximo passo é verificar o topo da pilha (\\(T\\)) e o próximo símbolo de entrada (‘id’).\nA Tabela indica a regra \\(T \\rightarrow F\\). Substitua \\(T\\) por \\(F\\).\nO próximo passo é verificar o topo da pilha (\\(F\\)) e o próximo símbolo de entrada (‘id’).\nA Tabela indica \\(F \\rightarrow id\\). Substitua \\(F\\) por ‘id’.\nAgora a pilha tem ‘id’ e a entrada é “id - id * id$”. O próximo passo é consumir ‘id’ da entrada e da pilha, o valor na pilha e no buffer coincidem.\nVerificar o topo da pilha ($) e o próximo símbolo de entrada (‘-’).\nA Tabela não fornece uma produção para \\(T\\) com entrada ‘-’, indicando que a string não é aceita pela gramática dada.\n\nPortanto, a string id - id id* não faz parte da linguagem definida pela gramática fornecida. O que deveria ser óbvio, já que a gramática não tem regras para lidar com o símbolo ‘-’. Para que a gramática suporte outros símbolos como o ‘-’, regras de produção adicionais devem ser incluídas.\nVimos, com a mesma gramática, as duas situações possíveis: ou a string faz parte da linguagem definida pela gramática, ou não. Simples assim.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parsers LL(1): O Mundo da Análise Sintática</span>"
    ]
  },
  {
    "objectID": "parsers.html#um-exemplo-nem-tão-comum",
    "href": "parsers.html#um-exemplo-nem-tão-comum",
    "title": "6  Parsers LL(1): O Mundo da Análise Sintática",
    "section": "6.8 Um Exemplo Nem Tão Comum",
    "text": "6.8 Um Exemplo Nem Tão Comum\nEste exemplo saiu das vozes da minha cabeça. Pode ser que exista em algum outro lugar, mas não me dei ao trabalho de verificar. É um exemplo que uso há muitos anos, em aula e já nem me preocupo com ele. Se souber a origem, fico grato.\n\n6.8.1 Gramática\nConsidere a seguinte gramática para expressões booleanas (\\(OR, AND, NOT\\)), definida pelo conjunto de regras de produção a seguir:\n\\(1. S \\rightarrow B\\)\n\\(2. B \\rightarrow B \\,\\, OR \\,\\, M \\mid M\\)\n\\(3. M \\rightarrow M \\,\\, AND \\,\\, N \\mid N\\)\n\\(4. N \\rightarrow NOT \\,\\, N \\mid (B) \\,\\mid id\\)\nNovamente, graças ao conjunto de regras de produção podemos criar os seguintes conjuntos:\n\n\n6.8.2 \\(FIRST\\)\n\n\\(FIRST(S) = FIRST(B)\\)\n\\(FIRST(B) = FIRST(M) = FIRST(N)\\)\n\\(FIRST(M) = FIRST(N)\\)\n\\(FIRST(N) = \\{ NOT, (, id \\}\\)\n\n\n\n6.8.3 \\(FOLLOW\\)\n\n\\(FOLLOW(S) = \\{ \\$ \\}\\)\n\\(FOLLOW(B) = \\{ OR, ), \\$ \\}\\)\n\\(FOLLOW(M) = \\{ OR, AND, ), \\$ \\}\\)\n\\(FOLLOW(N) = \\{ OR, AND, ), \\$ \\}\\)\n\n\n\n6.8.4 \\(NULLABLE\\)\nNenhuma das produções é nullable.\nCom os conjuntos já criados podemos criar uma Tabela de Derivação:\n\n\n6.8.5 Tabela de Derivação \\(LL(1)\\)\n\n\n\n\nid\nOR\nAND\nNOT\n(\n)\n$\n\n\n\n\nS\nB\n\n\nB\nB\n\n\n\n\nB\nM\n\n\nM\nM\n\n\n\n\nB\nM\nB OR M\n\nM\nM\nε\nε\n\n\nM\nN\nε\nM AND N\nN\nN\nε\nε\n\n\nN\nid\n\n\nNOT N\n(B)\n\n\n\n\n\nCom a Tabela de Derivação, podemos partir para o passo a passo do parser \\(LL(1)\\).\n\n\n6.8.6 Testando com id OR NOT id AND (id OR id)\nPara verificar se a string id OR NOT id AND (id OR id) faz parte da linguagem definida pela gramática dada e se pode ser analisada por um parser \\(LL(1)\\) usando a Tabela de Derivação encontrada, seguiremos o processo de análise preditiva descendente, começando pelo símbolo inicial, \\(S\\), utilizando a Tabela de Derivação para guiar a derivação.\nNovamente o processo iniciará com a pilha contendo o símbolo inicial da gramática, \\(S\\), e a string de entrada id OR NOT id AND (id OR id)$ será usada como buffer, onde o “$” indica o final da entrada. Siga os seguintes passos:\n\nVerificar o topo da pilha (\\(S\\)( e o próximo símbolo de entrada (‘id’).\nUsar a Tabela de Derivação para encontrar a produção apropriada: \\(S \\rightarrow B\\).\nSubstituir \\(S\\) pela produção \\(B\\) na pilha.\nO próximo passo é verificar o topo da pilha (\\(B\\)) e o próximo símbolo de entrada (‘id’).\nA Tabela indica \\(B \\rightarrow M\\). Substituir \\(B\\) por \\(M\\).\nO próximo passo é verificar o topo da pilha (\\(M\\)) e o próximo símbolo de entrada (‘id’).\nA Tabela indica \\(M \\rightarrow N\\). Substituir \\(M\\) por \\(N\\).\nO próximo passo é verificar o topo da pilha (\\(N\\)) e o próximo símbolo de entrada (‘id’).\nA Tabela indica \\(N \\rightarrow id\\). Substituir \\(N\\) por ‘id’.\nAgora a pilha tem ‘id’ e a entrada é id OR NOT id AND (id OR id)$. O próximo passo é consumir ‘id’ da entrada e da pilha, porque eles coincidem.\nO topo da pilha agora é \\(M\\) e o próximo símbolo de entrada é ‘OR’.\nA Tabela indica \\(B \\rightarrow B OR M\\). Substituir \\(B\\) por \\(B OR M\\).\nAgora a pilha tem \\(M OR M\\) e a entrada é “OR NOT id AND (id OR id)$”. Consumir ‘OR’ da entrada e da pilha, porque eles coincidem.\nO próximo passo é verificar o topo da pilha \\(M\\) e o próximo símbolo de entrada (‘NOT’).\nA Tabela indica \\(M \\rightarrow N\\). Substituir \\(M\\) por \\(N\\).\nO próximo passo é verificar o topo da pilha \\(N\\) e o próximo símbolo de entrada (‘NOT’).\nA Tabela indica \\(N \\rightarrow NOT N\\). Substituir \\(N\\) por \\(NOT N\\).\nConsumir ‘NOT’ da entrada e da pilha, porque eles coincidem.\nO próximo passo é verificar o topo da pilha \\(N\\) e o próximo símbolo de entrada (‘id’).\nA Tabela indica \\(N \\rightarrow id\\). Substituir \\(N\\) por ‘id’.\nConsumir ‘id’ da entrada e da pilha, porque eles coincidem.\nO topo da pilha agora é \\(M\\) e o próximo símbolo de entrada é ‘AND’.\nA Tabela indica \\(M \\rightarrow M AND N\\). Substituir \\(M\\) por \\(M AND N\\).\nConsumir ‘AND’ da entrada e da pilha, porque eles coincidem.\nO próximo passo é verificar o topo da pilha \\(M\\) e o próximo símbolo de entrada (‘(’).\nA Tabela indica \\(M \\rightarrow N\\). Substituir \\(M\\) por \\(N\\).\nO próximo passo é verificar o topo da pilha \\(N\\) e o próximo símbolo de entrada (‘(’).\nA Tabela indica \\(N \\rightarrow (B)\\). Substituir \\(N\\) por \\((B)\\).\nConsumir ‘(’ da entrada e da pilha, porque eles coincidem.\nO próximo passo é verificar o topo da pilha \\(B\\) e o próximo símbolo de entrada (‘id’).\nA Tabela indica \\(B \\rightarrow M\\). Substituir \\(B\\) por \\(M\\).\nO próximo passo é verificar o topo da pilha \\(M\\) e o próximo símbolo de entrada (‘id’).\nA Tabela indica \\(M \\rightarrow N\\). Substituir \\(M\\) por \\(N\\).\nO próximo passo é verificar o topo da pilha \\(N\\) e o próximo símbolo de entrada (‘id’).\nA Tabela indica \\(N \\rightarrow id\\). Substituir \\(N\\) por ‘id’.\nConsumir ‘id’ da entrada e da pilha, porque eles coincidem.\nO topo da pilha agora é \\(B\\) e o próximo símbolo de entrada é ‘OR’.\nA Tabela indica \\(B \\rightarrow B OR M\\). Substituir \\(B\\) por \\(B OR M\\).\nConsumir ‘OR’ da entrada e da pilha, porque eles coincidem.\nO próximo passo é verificar o topo da pilha \\(M\\) e o próximo símbolo de entrada (‘id’).\nA Tabela indica \\(M \\rightarrow N\\). Substituir \\(M\\) por \\(N\\).\nO próximo passo é verificar o topo da pilha \\(N\\) e o próximo símbolo de entrada (‘id’).\nA Tabela indica \\(N \\rightarrow id\\). Substituir \\(N\\) por ‘id’.\nConsumir ‘id’ da entrada e da pilha, porque eles coincidem.\nConsumir ‘)’ da entrada e da pilha, porque eles coincidem.\nFinalmente, a pilha está vazia e a entrada foi completamente consumida.\n\nComo foi possível consumir toda a string de entrada id OR NOT id AND (id OR id)$ e esvaziar a pilha sem encontrar nenhum erro, a string id OR NOT id AND (id OR id) é de fato parte da linguagem definida e pode ser identificada por um parser \\(LL(1)\\).\nAté agora, o conjuntos \\(FIRST\\), \\(FOLLOW\\) E \\(NULLABLE\\), e a Tabela de Derivação, caíram do céu. Se você é corajoso, sério, destemido e acompanhou todos os passos dos dois últimos exemplos, entendeu como o algoritmo de um parser \\(LL(1)\\) deve funcionar. Se era isso que estava procurado, pode parar por aqui. Agora se quiser entender como são criados os conjuntos e a Tabela de Derivação, tome uma água, respire fundo e continue. Seu próximo passo será entender como criamos os conjuntos \\(FIRST\\) e \\(FOLLOW\\)",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Parsers LL(1): O Mundo da Análise Sintática</span>"
    ]
  },
  {
    "objectID": "first-follow.html",
    "href": "first-follow.html",
    "title": "7  Conjuntos FIRST e FOLLOW",
    "section": "",
    "text": "7.1 O Conjunto FIRST\nO conjunto \\(FIRST\\) de um símbolo não-terminal é o conjunto de todos os terminais que podem aparecer no início de qualquer string derivada desse símbolo, incluindo o símbolo vazio (\\(\\varepsilon\\)) se o não-terminal puder derivar a string vazia. Para os símbolos terminais, o elemento do conjunto \\(FIRST\\) será o próprio símbolo terminal.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conjuntos FIRST e FOLLOW</span>"
    ]
  },
  {
    "objectID": "first-follow.html#o-conjunto-first",
    "href": "first-follow.html#o-conjunto-first",
    "title": "7  Conjuntos FIRST e FOLLOW",
    "section": "",
    "text": "7.1.1 Regras de Criação do Conjunto FIRST\nPara definir o conjunto \\(FIRST(X)\\) para todos os símbolos não-terminais \\(X\\) de uma gramática que esteja definida por um conjunto de regras de produção, podemos seguir os seguintes passos:\n\nPara símbolos terminais: o conjunto \\(FIRST\\) é o próprio símbolo terminal. Ou seja, se \\(a\\) é um terminal, então \\(FIRST(a) = {a}\\).\nPara um símbolo não-terminal \\(X\\): olhe para cada regra de produção \\(X \\rightarrow \\alpha\\) e siga as seguintes regras:\n\nSe \\(\\alpha\\) é um terminal, adicione \\(\\alpha\\) ao conjunto \\(FIRST(X)\\).\nSe \\(\\alpha\\) começa com um símbolo não-terminal \\(Y\\), adicione \\(FIRST(Y)\\) ao \\(FIRST(X)\\), exceto pelo símbolo de vazio \\((\\varepsilon\\)) se ele estiver presente.\nSe \\(\\alpha\\) consiste apenas em não-terminais e todos eles podem derivar em vazio (diretamente ou indiretamente), adicione \\(\\varepsilon\\) ao conjunto \\(FIRST(X)\\).\n\n\nO símbolo vazio \\(\\varepsilon\\) pertence ao conjunto FIRST(X) se, e somente se, \\(X\\) pode derivar a string vazia (diretamente ou indiretamente).\nRepita esses passos até que os conjuntos \\(FIRST\\) de todos os símbolos não-terminais não possam ser alterado.\n\n\n7.1.2 Exemplo 1: Criação de Conjuntos FIRST\nConsidere a gramática definida pelo seguinte conjunto de regras de produção:\n\\[\n\\begin{array}{cc}\n1. &S \\rightarrow aB \\vert  bA \\\\\n2. &A \\rightarrow c \\vert  d \\\\\n3. &B \\rightarrow e \\vert  f \\\\\n\\end{array}\n\\]\nEste conjunto de regras de produção permite criar:\n\n\n\nSímbolo\nFIRST\nExplicação\n\n\n\n\nS\n{a, b}\nS pode ser derivado em “aB” ou “bA”\n\n\nA\n{c, d}\nA pode ser derivado em “c” ou “d”\n\n\nB\n{e, f}\nB pode ser derivado em “e” ou “f”\n\n\n\nLogo: \\(FIRST =\\{(S,\\{a, b\\}),(A,\\{c, d\\}),(B,\\{e, f\\})\\}\\), um conjunto de tuplas.\nAgora que entendemos o algoritmo, podemos tentar criar um pseudocódigo para encontrar os elementos do conjunto \\(First\\).\n\n\n7.1.3 Algoritmo para calcular o conjunto FIRST\n## Algoritmo para calcular o conjunto FIRST para símbolos não-terminais\n\n# Entrada: Um conjunto de regras de produção P\n# Saída: Um dicionário FIRST, onde FIRST[X] é o conjunto FIRST do símbolo não-terminal X\n\nfunção calcular_FIRST(gramática):\n    FIRST = {}  # Inicializa o dicionário FIRST\n\n    # Passo 1: Inicialização para não-terminais\n    para cada símbolo não-terminal X na gramática:\n        FIRST[X] &lt;- {}\n\n    # Passo 2: Iteração para não-terminais\n    mudou = verdadeiro\n    enquanto mudou:\n        mudou = falso\n        para cada regra de produção X \\rightarrow Y1 Y2 ... Yn na gramática:\n            k = 0\n            adicionou_epsilon = verdadeiro\n            enquanto k &lt; n e adicionou_epsilon:\n                adicionou_epsilon = falso\n                Yk = Y[k]\n\n                # Se Yk é terminal, adicionar Yk ao FIRST[X]\n                se Yk é terminal:\n                    se Yk não está em FIRST[X]:\n                        adicionar Yk a FIRST[X]\n                        mudou = verdadeiro\n                # Se Yk é não-terminal, adicionar FIRST[Yk] ao FIRST[X], exceto \\varepsilon\n                senão:\n                    para cada símbolo t em FIRST[Yk]:\n                        se t != \"\\varepsilon\":\n                            se t não está em FIRST[X]:\n                                adicionar t a FIRST[X]\n                                mudou = verdadeiro\n                        senão:\n                            adicionou_epsilon = verdadeiro\n                k = k + 1\n\n            # Se todos os Y1, Y2, ..., Yn podem derivar \\varepsilon, adicionar \\varepsilon ao FIRST[X]\n            se k == n e adicionou_epsilon:\n                se \"\\varepsilon\" não está em FIRST[X]:\n                    adicionar \"\\varepsilon\" a FIRST[X]\n                    mudou = verdadeiro\n\n    retornar FIRST\nEste pseudocódigo, poderia ser criado em python com um código parecido com este:\ndef calcular_FIRST(producoes):\n    FIRST = {}\n\n    # Passo 1: Inicialização para não-terminais\n    # Identificamos todos os símbolos não-terminais presentes nas produções\n    nao_terminais = {regra.split('\\rightarrow')[0].strip() for regra in producoes}\n\n    # Inicializamos o conjunto FIRST de cada não-terminal como um conjunto vazio\n    for nao_terminal in nao_terminais:\n        FIRST[nao_terminal] = set()\n\n    mudou = True\n    # O loop continua até que não haja mais mudanças nos conjuntos FIRST\n    while mudou:\n        mudou = False\n        # Iteramos por todas as produções da gramática\n        for producao in producoes:\n            partes = producao.split('\\rightarrow')\n            X = partes[0].strip()  # Não-terminal do lado esquerdo da produção\n            Y = partes[1].strip().split()  # Lista de símbolos do lado direito da produção\n\n            k = 0\n            adicionou_epsilon = True  # Flag para controlar a adição de \\varepsilon\n            # Iteramos sobre os símbolos do lado direito da produção\n            while k &lt; len(Y) and adicionou_epsilon:\n                adicionou_epsilon = False\n                Yk = Y[k]\n\n                # Se Yk é um não-terminal, adicionamos seus FIRST ao FIRST de X\n                if Yk in nao_terminais:\n                    for simbolo in FIRST[Yk]:\n                        if simbolo != \"\\varepsilon\":\n                            if simbolo not in FIRST[X]:\n                                FIRST[X].add(simbolo)\n                                mudou = True\n                        else:\n                            adicionou_epsilon = True\n                else:\n                    # Se Yk é um terminal, adicionamos Yk ao FIRST de X\n                    if Yk not in FIRST[X]:\n                        FIRST[X].add(Yk)\n                        mudou = True\n                    adicionou_epsilon = False  # Paramos de adicionar se encontramos um terminal\n                k += 1\n\n            # Se todos os símbolos Y1, Y2, ..., Yn podem derivar \\varepsilon, adicionamos \\varepsilon ao FIRST de X\n            if k == len(Y) and adicionou_epsilon:\n                if \"\\varepsilon\" not in FIRST[X]:\n                    FIRST[X].add(\"\\varepsilon\")\n                    mudou = True\n\n    return FIRST\n\n# Exemplo de uso\nproducoes = [\n    \"S \\rightarrow a B\",\n    \"S \\rightarrow b A\",\n    \"A \\rightarrow c\",\n    \"A \\rightarrow d\",\n    \"B \\rightarrow e\",\n    \"B \\rightarrow f\"\n]\n\nFIRST = calcular_FIRST(producoes)\nfor nao_terminal in FIRST:\n    print(f\"FIRST({nao_terminal}) = {FIRST[nao_terminal]}\")",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conjuntos FIRST e FOLLOW</span>"
    ]
  },
  {
    "objectID": "first-follow.html#o-conjunto-follow",
    "href": "first-follow.html#o-conjunto-follow",
    "title": "7  Conjuntos FIRST e FOLLOW",
    "section": "7.2 O Conjunto FOLLOW",
    "text": "7.2 O Conjunto FOLLOW\nO conjunto \\(FOLLOW\\) de um símbolo não-terminal é o conjunto de terminais que podem aparecer imediatamente à direita (após, follow) desse não-terminal em alguma forma sentencial derivada, ou o símbolo de fim de entrada ($) se o não-terminal puder aparecer no final de uma forma sentencial.\nPara definir o conjunto \\(FOLLOW(A)\\) para cada não-terminal \\(A\\), siga estes passos:\n\nColoque o símbolo de fim de entrada \\((\\$)\\) no \\(FOLLOW\\) do símbolo inicial da gramática. Ao colocar o símbolo de fim de entrada ($) no \\(FOLLOW\\) do símbolo inicial da gramática, garantimos que o analisador sintático reconheça a última derivação da gramática como válida. Isso significa que o analisador estará preparado para encontrar o símbolo (\\(\\$\\)) ao final da string de entrada, indicando que a análise foi concluída com sucesso. Em outras palavras, o símbolo (\\(\\$\\)) no \\(FOLLOW\\) do símbolo inicial representa a expectativa de que a string de entrada seja completamente processada e que não existam símbolos após a última derivada.\nPara cada produção da forma \\(A \\rightarrow \\alpha B \\beta\\), onde \\(B\\) é um não-terminal:\n\n\nSe \\(\\beta\\) não deriva \\(\\varepsilon\\) (a string vazia), adicione \\(FIRST(\\beta)\\) (sem \\(\\varepsilon\\)) a \\(FOLLOW(B)\\).\nSe \\(\\beta\\) deriva \\(\\varepsilon\\) (a string vazia) ou \\(\\beta\\) é a string vazia, adicione \\(FOLLOW(A)\\) a \\(FOLLOW(B)\\).\n\nRepita esses passos até que os conjuntos \\(FOLLOW\\) de todos os símbolos não-terminais não mudem mais.\nExemplo: Considere a gramática definida por:\n\\[\n\\begin{array}{cc}\n1. & S \\rightarrow aB \\vert  bA \\\\\n2. & A \\rightarrow c \\vert  d \\\\\n3. & B \\rightarrow e \\vert  f \\\\\n\\end{array}\n\\]\nConjunto FIRST:\n\n\\(FIRST(S) = \\{a, b\\}\\) (S pode derivar em \\(aB\\) ou \\(bA\\))\n\\(FIRST(A) = \\{c, d\\}\\) (A pode derivar em \\(c\\) ou \\(d\\))\n\\(FIRST(B) = \\{e, f\\}\\) (B pode derivar em \\(e\\) ou \\(f\\))\n\nConjunto FOLLOW:\n\n\\(FOLLOW(S) = \\{\\$\\}\\) (\\(S\\) é o símbolo inicial)\n\\(FOLLOW(A) = \\{\\$, c, d\\}\\)\n\nA aparece em: \\(S \\to bA\\)\nComo não há nada após \\(A\\) na produção acima, adicionamos \\(FOLLOW(S)\\) a \\(FOLLOW(A)\\): \\(\\{\\$\\}\\)\nAlém disso, como \\(A\\) aparece após \\(b\\) na produção \\(S \\to bA\\), e \\(B\\) pode derivar em \\(c\\) ou \\(d\\) (\\(B \\to c \\vert  d\\)), então \\(c\\) e \\(d\\) também podem seguir \\(A\\).\n\n\\(FOLLOW(B) = \\{\\$\\}\\)\n\n\\(B\\) aparece em: \\(S \\rightarrow aB\\)\nComo não há nada após \\(B\\) na produção acima, adicionamos \\(FOLLOW(S)\\) a \\(FOLLOW(B)\\): \\(\\{\\$\\}\\)\n\n\nCriamos o conjunto \\(FIRST\\) porque este é necessário para a criação do conjunto \\(FOLLOW\\). Mas, neste momento nos interessa apenas o conjunto \\(FOLLOW\\). O conjunto resultante será:\n\n\n\n\n\n\n\n\nSímbolo\nFOLLOW\nExplicação\n\n\n\n\n\\(S\\)\n\\(\\{ \\$ \\}\\)\n\\(S\\) é o símbolo inicial, então \\(\\$\\) é o único terminal que pode aparecer à direita de \\(S\\) em uma forma sentencial derivada.\n\n\nA\n\\(\\{ \\$, c, d \\}\\)\n\\(A\\) pode ser seguido por \\(c\\) na regra \\(A \\to c\\), \\(d\\) na regra \\(A \\to d\\), ou pelo símbolo de fim de entrada \\(\\$\\) em regras que contêm \\(A\\).\n\n\n\\(B\\)\n\\(\\{ a, c, d, \\$ \\}\\)\n\\(B\\) pode ser seguido por \\(a\\) na regra \\(S \\to aB\\), \\(c\\) na regra \\(A \\to cB\\), \\(d\\) na regra \\(A \\to dB\\), ou pelo símbolo de fim de entrada \\(\\$\\) em regras que contêm \\(B\\).",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conjuntos FIRST e FOLLOW</span>"
    ]
  },
  {
    "objectID": "first-follow.html#algoritmo-para-calcular-o-conjunto-follow",
    "href": "first-follow.html#algoritmo-para-calcular-o-conjunto-follow",
    "title": "7  Conjuntos FIRST e FOLLOW",
    "section": "7.3 Algoritmo para calcular o conjunto FOLLOW",
    "text": "7.3 Algoritmo para calcular o conjunto FOLLOW\nAssim como fizemos com o \\(FIRST\\) podemos criar um algoritmo para criar o conjunto \\(FOLLOW\\):\n# Entrada: Um conjunto de regras de produção P e o símbolo inicial S\n# Saída: Um dicionário FOLLOW, onde FOLLOW[X] é o conjunto FOLLOW do símbolo não-terminal X\n\nfunção calcular_FOLLOW(producoes, simbolo_inicial):\n    FOLLOW = {}  # Inicializa o dicionário FOLLOW\n\n    # Passo 1: Inicialização para todos os não-terminais\n    para cada símbolo não-terminal X na gramática:\n        FOLLOW[X] &lt;- {}\n\n    # Passo 2: Colocar o símbolo de fim de entrada ($) no FOLLOW do símbolo inicial\n    FOLLOW[simbolo_inicial] &lt;- {$}\n\n    # Passo 3: Iteração para todos os não-terminais\n    mudou = verdadeiro\n    enquanto mudou:\n        mudou = falso\n        para cada regra de produção A \\rightarrow \\alpha na gramática:\n            \\alpha = \\alpha.split()  # Divide a produção em símbolos individuais\n            para cada símbolo B na produção \\alpha:\n                se B é um não-terminal:\n                    # Verifica os símbolos após B na produção\n                    para cada símbolo após B na produção:\n                        se o símbolo é terminal:\n                            se símbolo não está em FOLLOW[B]:\n                                adicionar símbolo a FOLLOW[B]\n                                mudou = verdadeiro\n                            pare\n                        senão:\n                            para cada símbolo t em FIRST[símbolo]:\n                                se t != \"\\varepsilon\":\n                                    se t não está em FOLLOW[B]:\n                                        adicionar t a FOLLOW[B]\n                                        mudou = verdadeiro\n                            se \"\\varepsilon\" não está em FIRST[símbolo]:\n                                pare\n                    # Se não há mais símbolos após B ou todos podem derivar \\varepsilon\n                    se não há mais símbolos após B ou todos podem derivar \\varepsilon:\n                        para cada símbolo t em FOLLOW[A]:\n                            se t não está em FOLLOW[B]:\n                                adicionar t a FOLLOW[B]\n                                mudou = verdadeiro\n\n    retornar FOLLOW\nAgora que temos um pseudo código, podemos partir para o código em Python, para isso é preciso lembrar que vamos precisar do conjunto \\(FIRST\\) para encontrar o conjunto \\(FOLLOW\\).\ndef calcular_FIRST(producoes):\n    FIRST = {}\n\n    # Passo 1: Inicialização para não-terminais\n    nao_terminais = {regra.split('\\rightarrow')[0].strip() for regra in producoes}\n    for nao_terminal in nao_terminais:\n        FIRST[nao_terminal] = set()\n\n    mudou = True\n    while mudou:\n        mudou = False\n        for producao in producoes:\n            partes = producao.split('\\rightarrow')\n            X = partes[0].strip()\n            Y = partes[1].strip().split()\n\n            k = 0\n            adicionou_epsilon = True\n            while k &lt; len(Y) and adicionou_epsilon:\n                adicionou_epsilon = False\n                Yk = Y[k]\n\n                if Yk in nao_terminais:\n                    for simbolo in FIRST[Yk]:\n                        if simbolo != \"\\varepsilon\":\n                            if simbolo not in FIRST[X]:\n                                FIRST[X].add(simbolo)\n                                mudou = True\n                        else:\n                            adicionou_epsilon = True\n                else:\n                    if Yk not in FIRST[X]:\n                        FIRST[X].add(Yk)\n                        mudou = True\n                    adicionou_epsilon = False\n                k += 1\n\n            if k == len(Y) and adicionou_epsilon:\n                if \"\\varepsilon\" not in FIRST[X]:\n                    FIRST[X].add(\"\\varepsilon\")\n                    mudou = True\n\n    return FIRST\n\ndef calcular_FOLLOW(producoes, simbolo_inicial):\n    FIRST = calcular_FIRST(producoes)  # Calcula o conjunto FIRST necessário para FOLLOW\n    FOLLOW = {}\n\n    # Inicializa FOLLOW para todos os não-terminais\n    nao_terminais = {regra.split('\\rightarrow')[0].strip() for regra in producoes}\n    for nao_terminal in nao_terminais:\n        FOLLOW[nao_terminal] = set()\n\n    # Passo 2: Colocar o símbolo de fim de entrada ($) no FOLLOW do símbolo inicial\n    FOLLOW[simbolo_inicial].add('$')\n\n    # Passo 3: Iteração para todos os não-terminais\n    mudou = True  # Flag para controlar se houve mudanças nos conjuntos FOLLOW\n    while mudou:\n        mudou = False  # Inicializa a flag como False no início de cada iteração\n        for producao in producoes:  # Para cada produção na gramática\n            partes = producao.split('\\rightarrow')\n            A = partes[0].strip()  # Não-terminal do lado esquerdo da produção\n            alfa = partes[1].strip().split()  # Símbolos do lado direito da produção\n\n            for i in range(len(alfa)):\n                B = alfa[i]  # Símbolo atual na produção\n                if B in nao_terminais:  # Se B é um não-terminal\n                    beta = alfa[i+1:]  # Símbolos após B na produção\n                    if beta:  # Se há símbolos após B\n                        # Calcula FIRST(beta) e adiciona ao FOLLOW(B) exceto por \\varepsilon\n                        for simbolo in calcular_FIRST(['temp \\rightarrow ' + ' '.join(beta)]).get('temp', []):\n                            if simbolo != '\\varepsilon' and simbolo not in FOLLOW[B]:\n                                FOLLOW[B].add(simbolo)\n                                mudou = True\n                        # Se FIRST(beta) contém \\varepsilon, adiciona FOLLOW(A) ao FOLLOW(B)\n                        if '\\varepsilon' in calcular_FIRST(['temp \\rightarrow ' + ' '.join(beta)]).get('temp', []):\n                            for simbolo in FOLLOW[A]:\n                                if simbolo not in FOLLOW[B]:\n                                    FOLLOW[B].add(simbolo)\n                                    mudou = True\n                    else:  # Se não há símbolos após B\n                        # Adiciona FOLLOW(A) ao FOLLOW(B) diretamente\n                        for simbolo in FOLLOW[A]:\n                            if simbolo not in FOLLOW[B]:\n                                FOLLOW[B].add(simbolo)\n                                mudou = True\n    return FOLLOW\n\n# Exemplo de uso\nproducoes = [\n    \"S \\rightarrow a B\",\n    \"S \\rightarrow b A\",\n    \"A \\rightarrow c\",\n    \"A \\rightarrow d\",\n    \"B \\rightarrow e\",\n    \"B \\rightarrow f\"\n]\nsimbolo_inicial = \"S\"\n\nFOLLOW = calcular_FOLLOW(producoes, simbolo_inicial)\nfor nao_terminal in FOLLOW:\n    print(f\"FOLLOW({nao_terminal}) = {FOLLOW[nao_terminal]}\")",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conjuntos FIRST e FOLLOW</span>"
    ]
  },
  {
    "objectID": "tabela-derivacao.html",
    "href": "tabela-derivacao.html",
    "title": "8  Tabelas de Derivação (Análise) LL(1)",
    "section": "",
    "text": "8.1 Conflitos na Tabela de Derivação\nConflitos na Tabela de Derivação \\(LL(1)\\) ocorrem quando há mais de uma regra de produção associada a um mesmo par indicador (terminal, não-terminal). Esta ambiguidade significa que, ao encontrar esse par, o parser \\(LL(1)\\) não conseguirá determinar, de forma única e inequívoca, qual regra deverá aplicar a um determinado símbolo de entrada, tornando a gramática ambígua e inadequada para uso com parsers \\(LL(1)\\).\nExemplo 2: o conflito. Observe que gramática a seguir foi criada para criar um conflito na Tabela de Derivação. Antes de nos preocuparmos com os tipos de conflito, e como solucioná-los, vamos rever todo o processo de criação de uma Tabela de Derivação para entender o problema.\n\\[\n\\begin{aligned}\n1. &\\ E \\rightarrow T + E \\ \\vert  \\ T \\\\\n2. &\\ T \\rightarrow int \\ \\vert  \\ (E)\n\\end{aligned}\n\\]",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tabelas de Derivação (Análise) LL(1)</span>"
    ]
  },
  {
    "objectID": "tabela-derivacao.html#conflitos-na-tabela-de-derivação",
    "href": "tabela-derivacao.html#conflitos-na-tabela-de-derivação",
    "title": "8  Tabelas de Derivação (Análise) LL(1)",
    "section": "",
    "text": "8.1.1 Conjunto FIRST\n\nPara o não-terminal E:\n\n\\(E \\rightarrow T + E\\): o primeiro símbolo é \\(T\\). Portanto, incluímos \\(FIRST(T)\\) em \\(FIRST(E)\\).\n\\(E \\rightarrow T\\): o primeiro símbolo é \\(T\\). Portanto, incluímos \\(FIRST(T)\\) em \\(FIRST(E)\\).\n\nPara o não-terminal T:\n\n\\(T \\rightarrow int\\): o primeiro símbolo é \\(int\\). Portanto, \\(FIRST(T)\\) inclui \\(int\\).\n\\(T \\rightarrow (E)\\): o primeiro símbolo é $( $. Portanto, \\(FIRST(T)\\) inclui $( $.\n\n\nAssim, temos:\n\\[\n\\begin{aligned}\nFIRST(T) &= \\{ int, ( \\} \\\\\nFIRST(E) &= FIRST(T) = \\{ int, ( \\}\n\\end{aligned}\n\\]\n\n\n8.1.2 Conjunto FOLLOW\n\nPara o símbolo inicial E:\n\n\\(FOLLOW(E)\\) inclui $.\n\nPara as produções de E:\n\n\\(E \\rightarrow T + E\\):\n\no símbolo \\(T\\) pode ser seguido por \\(+\\), então \\(+\\) está em \\(FOLLOW(T)\\).\no símbolo \\(E\\) é o último da produção, então \\(FOLLOW(E)\\) inclui \\(FOLLOW(E)\\).\n\n\\(E \\rightarrow T\\): o símbolo \\(T\\) é o último da produção, então \\(FOLLOW(T)\\) inclui \\(FOLLOW(E)\\).\n\nPara as produções de T:\n\n\\(T \\rightarrow int\\): \\(int\\) é um terminal, não influencia \\(FOLLOW\\).\n\\(T \\rightarrow (E)\\): \\(E\\) pode ser seguido por \\()\\), então \\(FOLLOW(E)\\) inclui \\()\\).\n\n\nAssim, teremos:\n\\[\n\\begin{aligned}\nFOLLOW(E) &= \\{ \\$, +, ) \\} \\\\\nFOLLOW(T) &= \\{ +, \\$, ) \\}\n\\end{aligned}\n\\]\n\n\n8.1.3 Nullable\nPara determinar se algum não-terminal é nullable, verificamos se ele pode derivar a string vazia \\(\\epsilon\\).\n\nPara E:\n\n\\(E \\rightarrow T + E\\): \\(T\\) não é nullable, portanto, \\(E\\) não é nullable a partir desta produção.\n\\(E \\rightarrow T\\): \\(T\\) não é nullable, portanto, \\(E\\) não é nullable.\n\nPara T:\n\n\\(T \\rightarrow int\\): \\(int\\) não é nullable.\n\\(T \\rightarrow (E)\\): \\(E\\) não é nullable e \\((\\) é um terminal.\n\n\nOu seja, nenhum dos não-terminais é nullable:\n\\[\n\\begin{aligned}\nNullable(E) &= false \\\\\nNullable(T) &= false\n\\end{aligned}\n\\]\n\n\n8.1.4 Resumo dos Conjuntos\n\\[\n\\begin{aligned}\nFIRST(E) &= \\{ int, ( \\} \\\\\nFIRST(T) &= \\{ int, ( \\} \\\\\nFOLLOW(E) &= \\{ \\$, +, ) \\} \\\\\nFOLLOW(T) &= \\{ +, \\$, ) \\} \\\\\nNullable(E) &= false \\\\\nNullable(T) &= false\n\\end{aligned}\n\\]\nO que permite gerar a seguinte Tabela de Derivação:\n\n\n\n\n\n\n\n\n\n\n\nnão-terminal\nint\n(\n+\n$\n)\n\n\n\n\nE\n\\(E \\to T + E\\)\\(E \\to T\\)\n\\(E \\to T + E\\)\\(E \\to T\\)\n\n\n\n\n\nT\n\\(T \\to int\\)\n\\(T \\to (E)\\)\n\n\n\n\n\n\nNesta tabela podemos ver um conflito explícito. O não-terminal \\(E\\) possui duas produções para os símbolos \\(int\\) e \\((\\).\nSempre que na criação de Tabelas de Derivação existir um conflito estaremos gerando ambiguidades na derivação. A gramática é ambígua, sempre que uma sentença puder ser derivada de duas ou mais formas diferentes, gerando árvores sintáticas diferentes. Por exemplo, na gramática do Exemplo 2, a sentença \\(int + int\\) pode ser derivada tanto como \\(E → T → int\\) seguido de \\(E → T + E → int + int\\) quanto como \\(E → T + E → T + T → int + int\\).",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tabelas de Derivação (Análise) LL(1)</span>"
    ]
  },
  {
    "objectID": "tabela-derivacao.html#tipos-de-conflitos",
    "href": "tabela-derivacao.html#tipos-de-conflitos",
    "title": "8  Tabelas de Derivação (Análise) LL(1)",
    "section": "8.2 Tipos de Conflitos",
    "text": "8.2 Tipos de Conflitos\nConflitos na tabela \\(LL(1)\\) podem ser classificados em dois tipos principais:\n\nConflito \\(FIRST\\)/\\(FIRST\\): ocorre quando o conjunto \\(FIRST\\) de duas ou mais produções de um mesmo não-terminal possui um terminal em comum. Na gramática do Exemplo 2, as produções \\(E \\to T + E\\) e \\(E \\to T\\) possuem os terminais \\(int\\) e \\((\\) em seus conjuntos \\(FIRST\\). Ao encontrar \\(int\\) ou \\((\\) na entrada, o parser não sabe se deve aplicar a regra que deriva uma expressão com um operador \\(+\\) ou a regra que deriva um termo. Neste momento do processo de parsing determinismo saiu pela janela e o parser \\(LL(1)\\) é inútil.\nConflito \\(FIRST\\)/\\(FOLLOW\\): ocorre quando uma produção tem \\(\\varepsilon\\) (a string vazia) em seu conjunto \\(FIRST\\) e o conjunto \\(FOLLOW\\) do não-terminal da produção possui um terminal em comum com o \\(FIRST\\) de outra produção do mesmo não-terminal. Por exemplo, na gramática do Exemplo 2, se \\(E \\rightarrow \\varepsilon\\) fosse uma produção e o conjunto \\(FOLLOW(E)\\) contivesse \\(+\\), que também está no \\(FIRST\\) da produção \\(E \\rightarrow T + E\\), ao encontrar o fim de uma expressão (representado por um símbolo em \\(FOLLOW(E)\\)), o parser não saberia se deve aplicar a regra que deriva apenas um termo ou a regra que deriva uma expressão com um operador \\(+\\).\n\n\n8.2.1 Resolução de Conflitos\nConflitos na tabela \\(LL(1)\\) podem ser resolvidos das seguintes formas:\n\nRefatoração da gramática: A gramática pode ser reescrita para eliminar ambiguidades e recursões à esquerda, evitando assim os conflitos.\nFatoração à esquerda: Produções com prefixos comuns podem ser fatoradas para que a decisão entre elas possa ser tomada com base em um único símbolo de lookahead.\nUso de analisadores mais poderosos: Se os conflitos não puderem ser resolvidos na gramática, pode ser necessário usar um analisador sintático mais poderoso, como um analisador \\(LR(1)\\) ou \\(LALR(1)\\), que conseguem lidar com gramáticas mais complexas.\n\nAs soluções 1 e 2 implicam na modificação da sua gramática, o que ocorre com frequência quando começamos do zero. A solução 3, em linguagens complexas, pode ser a solução adequada, mas implica em mudar de algoritmo de parser\nExemplo 3: resolução de conflito. No exemplo da gramática anterior, o conflito \\(FIRST\\)/\\(FIRST\\) pode ser resolvido fatorando as produções de \\(E\\):\n\\[\n\\begin{aligned}\n1. &\\ E \\rightarrow T E' \\\\\n2. &\\ E' \\rightarrow + E \\ \\vert  \\ \\epsilon \\\\\n3. &\\ T \\rightarrow int \\ \\vert  \\ (E)\n\\end{aligned}\n\\]\nSe calcularmos os conjuntos \\(FIRST\\) e \\(FOLLOW\\) novamente, teremos:\n\n\n8.2.2 Conjunto FIRST\n\nPara o não-terminal E:\n\n\\(E \\rightarrow T E'\\): o primeiro símbolo é \\(T\\). Portanto, incluímos \\(FIRST(T)\\) em \\(FIRST(E)\\).\n\nPara o não-terminal E’:\n\n\\(E' \\rightarrow + E\\): o primeiro símbolo é \\(+\\). Portanto, \\(FIRST(E')\\) inclui \\(+\\).\n\\(E' \\rightarrow \\epsilon\\): incluímos \\(\\epsilon\\) em \\(FIRST(E')\\).\n\nPara o não-terminal T:\n\n\\(T \\rightarrow int\\): o primeiro símbolo é \\(int\\). Portanto, \\(FIRST(T)\\) inclui \\(int\\).\n\\(T \\rightarrow (E)\\): o primeiro símbolo é $( $. Portanto, \\(FIRST(T)\\) inclui $( $.\n\n\nAssim, teremos:\n\\[\n\\begin{aligned}\nFIRST(T) &= \\{ int, ( \\} \\\\\nFIRST(E') &= \\{ +, \\epsilon \\} \\\\\nFIRST(E) &= FIRST(T) = \\{ int, ( \\}\n\\end{aligned}\n\\]\n\n\n8.2.3 Conjunto FOLLOW\n\nPara o símbolo inicial E:\n\n\\(FOLLOW(E)\\) inclui $.\n\nPara as produções de E:\n\n\\(E \\rightarrow T E'\\):\n\nO símbolo \\(E'\\) pode ser seguido por \\(FOLLOW(E)\\).\nEntão, \\(FOLLOW(E')\\) inclui \\(FOLLOW(E)\\).\n\n\nPara as produções de E’:\n\n\\(E' \\rightarrow + E\\):\n\nO símbolo \\(E\\) pode ser seguido por \\(FOLLOW(E')\\).\nEntão, \\(FOLLOW(E)\\) inclui \\(FOLLOW(E')\\).\n\n\\(E' \\rightarrow \\epsilon\\): não há efeito em \\(FOLLOW\\).\n\nPara as produções de T:\n\n\\(T \\rightarrow int\\): \\(int\\) é um terminal, não influencia \\(FOLLOW\\).\n\\(T \\rightarrow (E)\\): \\(E\\) pode ser seguido por \\()\\), então \\(FOLLOW(E)\\) inclui \\()\\).\n\n\nAssim, teremos:\n\\[\n\\begin{aligned}\nFOLLOW(E) &= \\{ \\$, ) \\} \\\\\nFOLLOW(E') &= \\{ \\$, ) \\} \\\\\nFOLLOW(T) &= \\{ +, \\$, ) \\}\n\\end{aligned}\n\\]\n\n\n8.2.4 Nullable\nPara determinar se algum não-terminal é nullable, verificamos se ele pode derivar a string vazia \\(\\epsilon\\).\n\nPara E:\n\n\\(E \\rightarrow T E'\\): \\(T\\) não é nullable, portanto, \\(E\\) não é nullable.\n\nPara E’:\n\n\\(E' \\rightarrow + E\\): \\(+\\) é um terminal, portanto, não é nullable.\n\\(E' \\rightarrow \\epsilon\\): \\(E'\\) é nullable.\n\nPara T:\n\n\\(T \\rightarrow int\\): \\(int\\) não é nullable.\n\\(T \\rightarrow (E)\\): $( $ é um terminal, portanto, não é nullable.\n\n\nAssim, teremos:\n\\[\n\\begin{aligned}\nNullable(E) &= false \\\\\nNullable(E') &= true \\\\\nNullable(T) &= false\n\\end{aligned}\n\\]\n\n\n8.2.5 Resumo dos Conjuntos\n\\[\n\\begin{aligned}\nFIRST(E) &= \\{ int, ( \\} \\\\\nFIRST(E') &= \\{ +, \\epsilon \\} \\\\\nFIRST(T) &= \\{ int, ( \\} \\\\\nFOLLOW(E) &= \\{ \\$, ) \\} \\\\\nFOLLOW(E') &= \\{ \\$, ) \\} \\\\\nFOLLOW(T) &= \\{ +, \\$, ) \\} \\\\\nNullable(E) &= false \\\\\nNullable(E') &= true \\\\\nNullable(T) &= false\n\\end{aligned}\n\\]\nO que permite gerar a seguinte Tabela de Derivação:\n\n\n\n\n\n\n\n\n\n\n\nnão-terminal\nint\n(\n+\n$\n)\n\n\n\n\nE\n\\(E \\to TE'\\)\n\\(E \\to TE'\\)\n\n\n\n\n\nE’\n\n\n\\(E' \\to +E\\)\n\\(E' \\to \\varepsilon\\)\n\\(E' \\to \\varepsilon\\)\n\n\nT\n\\(T \\to int\\)\n\\(T \\to (E)\\)\n\n\n\n\n\n\nAgora, a decisão entre derivar uma expressão com um operador \\(+\\) ou apenas um termo pode ser tomada com base no próximo símbolo da entrada: se for \\(+\\), aplica-se a regra \\(E' \\rightarrow + E\\); caso contrário, aplica-se a regra \\(E' \\rightarrow \\varepsilon\\).\nAssim como fiz nos artigos anteriores, vou sugerir um pseudocódigo, um tanto inocente, para a criação de tabelas de derivação. Acredito que, com um pouco de cuidado, depois que a amável leitora dominar esta técnica possa criar um pseudocódigo mais eficiente. A fé move montanhas.",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tabelas de Derivação (Análise) LL(1)</span>"
    ]
  },
  {
    "objectID": "tabela-derivacao.html#pseudocódigo-para-a-criação-da-tabela-de-derivação",
    "href": "tabela-derivacao.html#pseudocódigo-para-a-criação-da-tabela-de-derivação",
    "title": "8  Tabelas de Derivação (Análise) LL(1)",
    "section": "8.3 Pseudocódigo para a criação da Tabela de Derivação",
    "text": "8.3 Pseudocódigo para a criação da Tabela de Derivação\n**Entrada**:\n- Conjuntos de produções da gramática: `Productions`\n- Conjuntos FIRST: `First`\n- Conjuntos FOLLOW: `Follow`\n- Conjuntos Nullable: `Nullable`\n- Conjunto de não-terminais: `NonTerminals`\n- Conjunto de terminais: `Terminals`\n\n**Saída**:\n- Tabela de Derivação LL(1): `ParsingTable`\n\n# Inicialização da Tabela de Derivação LL(1)\nParsingTable = {A: {a: None for a in Terminals + ['$']} for A in NonTerminals}\n\n# Para cada produção A → α\nfor A, productions in Productions.items():\n    for α in productions:\n        # Para cada terminal a em FIRST(α)\n        for a in First[α]:\n            if a != 'ε':\n                # Adicionar a produção A → α na célula [A, a]\n                if ParsingTable[A][a] is None:\n                    ParsingTable[A][a] = A + \" → \" + α\n                else:\n                    raise ConflictError(f\"Conflito na tabela para [{A}, {a}]\")\n\n        # Se ε está em FIRST(α)\n        if 'ε' in First[α]:\n            # Para cada b em FOLLOW(A)\n            for b in Follow[A]:\n                # Adicionar a produção A → α na célula [A, b]\n                if ParsingTable[A][b] is None:\n                    ParsingTable[A][b] = A + \" → \" + α\n                else:\n                    raise ConflictError(f\"Conflito na tabela para [{A}, {b}]\")\n\n            # Se $ está em FOLLOW(A)\n            if '$' in Follow[A]:\n                # Adicionar a produção A → α na célula [A, $]\n                if ParsingTable[A]['$'] is None:\n                    ParsingTable[A]['$'] = A + \" → \" + α\n                else:\n                    raise ConflictError(f\"Conflito na tabela para [{A}, $]\")\n\n# Função para calcular FIRST de uma string\ndef compute_first(α):\n    if α == '':\n        return {'ε'}\n    first_set = set()\n    for symbol in α:\n        first_set.update(First[symbol] - {'ε'})\n        if 'ε' not in First[symbol]:\n            break\n    else:\n        first_set.add('ε')\n    return first_set\nCaramba! Só notei agora, eu praticamente escrevo em python quando estou fazendo pseudocódigos. Isto deve ser mal, muito mal… Acho que vou ficar sem sorvete hoje. Em fim, este código pode ser implementado em python por:\n# Conjuntos de produções\nproductions = {\n    'E': ['T+E', 'T'],\n    'T': ['int', '(E)']\n}\n\n# Conjuntos FIRST\nfirst = {\n    'E': {'int', '('},\n    'T': {'int', '('},\n    'T+E': {'int', '('},\n    'int': {'int'},\n    '(E)': {'('}\n}\n\n# Conjuntos FOLLOW\nfollow = {\n    'E': {'$', ')'},\n    'T': {'+', '$', ')'}\n}\n\n# Conjuntos Nullable\nnullable = {\n    'E': False,\n    'T': False\n}\n\n# Conjunto de não-terminais\nnon_terminals = ['E', 'T']\n\n# Conjunto de terminais\nterminals = ['int', '(', ')', '+', '$']\n\n# Inicialização da Tabela de Derivação LL(1)\nparsing_table = {A: {a: None for a in terminals} for A in non_terminals}\n\n# Função para calcular FIRST de uma string\ndef compute_first(α):\n    if α == '':\n        return {'ε'}\n    first_set = set()\n    i = 0\n    while i &lt; len(α):\n        symbol = α[i]\n        if symbol in first:\n            first_set.update(first[symbol] - {'ε'})\n            if 'ε' not in first[symbol]:\n                break\n        else:\n            # Adiciona todo o terminal se não for um não-terminal\n            terminal = ''\n            while i &lt; len(α) and (α[i].isalnum() or α[i] in ['_', '+', '(', ')']):\n                terminal += α[i]\n                i += 1\n            first_set.add(terminal)\n            if terminal in first and 'ε' in first[terminal]:\n                continue\n            break\n        i += 1\n    else:\n        first_set.add('ε')\n    return first_set\n\n# Preenchimento da Tabela de Derivação\nfor A in productions:\n    for α in productions[A]:\n        first_α = compute_first(α)\n\n        # Para cada terminal em FIRST(α)\n        for a in first_α:\n            if a != 'ε':\n                if a in parsing_table[A]:\n                    if parsing_table[A][a] is None:\n                        parsing_table[A][a] = f\"{A} → {α}\"\n                    else:\n                        print(f\"Conflito na tabela para [{A}, {a}]\")\n                else:\n                    print(f\"Terminal {a} não reconhecido na tabela\")\n\n        # Se ε está em FIRST(α)\n        if 'ε' in first_α:\n            for b in follow[A]:\n                if b in parsing_table[A]:\n                    if parsing_table[A][b] is None:\n                        parsing_table[A][b] = f\"{A} → {α}\"\n                    else:\n                        print(f\"Conflito na tabela para [{A}, {b}]\")\n                else:\n                    print(f\"Terminal {b} não reconhecido na tabela\")\n\n# Exibir a Tabela de Derivação de forma legível\nimport pandas as pd\n\n# Converte os valores None para strings vazias\nformatted_parsing_table = {A: {a: (parsing_table[A][a] if parsing_table[A][a] is not None else '') for a in terminals} for A in non_terminals}\n\n# Cria o DataFrame\ndf = pd.DataFrame(formatted_parsing_table).T\n\n# Reordena as colunas para incluir o símbolo de final de string '$'\ndf = df[terminals]\n\n# Calcula a largura máxima de cada coluna (incluindo o cabeçalho)\ncol_widths = {col: max(df[col].astype(str).str.len().max(), len(col)) for col in df.columns}\n\n# Estiliza o DataFrame para uma melhor visualização com bordas, linhas zebradas e ajuste de largura\nstyled_df = df.style.set_properties(**{'text-align': 'center', 'border': '1px solid black'}).set_table_styles(\n    [dict(selector='th', props=[('text-align', 'center'), ('border', '1px solid black')])]\n).set_caption(\"Tabela de Parsing LL(1)\")\n\n# Define o estilo das células vazias como cinza claro\nstyled_df.set_properties(subset=pd.IndexSlice[:, :], **{'background-color': 'lightgrey' if v == '' else '' for v in df.values.flatten()})\n\n# Adiciona o estilo zebrado\nstyled_df.set_table_styles([\n    {'selector': 'tbody tr:nth-child(even)', 'props': [('background-color', 'lightblue')]},\n    {'selector': 'tbody tr:nth-child(odd)', 'props': [('background-color', 'white')]}\n])\n\n# Ajusta a largura das colunas\nstyled_df.set_properties(subset=pd.IndexSlice[:, :], **{f'width': f'{col_widths[col]*8}px' for col in df.columns})\n\nfrom IPython.display import display\n\ndisplay(styled_df)",
    "crumbs": [
      "Analisadores Sintáticos",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tabelas de Derivação (Análise) LL(1)</span>"
    ]
  },
  {
    "objectID": "fase1.html",
    "href": "fase1.html",
    "title": "9  Fase 1 - Projeto Prático",
    "section": "",
    "text": "9.1 Objetivo\nPesquisar e praticar conceitos de analisador léxico para desenvolver um programa em Python, C, ou C++ que processe expressões aritméticas em notação polonesa reversa (RPN), conforme definida neste texto, a partir de um arquivo de texto, utilizando máquinas de estado finito (FSMs) implementadas obrigatoriamente com funções. O programa deve executar as expressões em um ambiente de teste (ex.: o notebook do aluno) e em um Arduino Uno, ou Mega. O seu trabalho será gerar o código Assembly, compatível com a arquitetura do Arduino Uno, ou Mega. Um guia de como compilar e executar o código Assembly está incluído na seção Chapter 10 no final deste documento.",
    "crumbs": [
      "Projeto da Disciplina - 2025-2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#descrição-do-trabalho",
    "href": "fase1.html#descrição-do-trabalho",
    "title": "9  Fase 1 - Projeto Prático",
    "section": "9.2 Descrição do Trabalho",
    "text": "9.2 Descrição do Trabalho\nO objetivo é desenvolver um programa capaz de:\n\nLer um arquivo de texto contendo expressões aritméticas em Escritas RPN, segundo o formato especificado neste documento, com uma expressão por linha. Este arquivo contém o código do programa que será analisado pelo analisador léxico.\nAnalisar as expressões usando um analisador léxico baseado em Autômatos Finitos Determinísticos, com estados implementados por funções.\nTransformar as expressões em um texto contendo o código Assembly para o Arduino (uno|mega), utilizando as operações aritméticas e comandos especiais especificados. As operações serão realizadas no Arduino.\nExecutar as expressões em um Arduino (Uno|Mega), de forma que os resultados possam ser acompanhados (display, leds ou serial).\nHospedar o código, arquivos de teste e documentação em um repositório público no GitHub.\n\n\n9.2.1 Características Especiais\nAs expressões devem ser escritas em notação RPN, no formato (A B op), no qual A e B são números reais, os operandos, e op é um operador aritmético entre os listados neste documento. O programa deve suportar operações aritméticas básicas listadas neste documento, comandos especiais para manipulação de memória, e deve ser capaz de lidar com expressões aninhadas sem limites de aninhamento. Segundo a seguinte sintaxe:\n\nConsiderando que A e B são números reais, e usando o ponto como separador decimal, (ex.: 3.14), teremos:\nOperadores suportados na Fase 1:\n\nAdição: + (ex.: (A B +));\nSubtração: - (ex.: (A B -));\nMultiplicação: * (ex.: (A B *));\nDivisão real: / (ex.: (A B /));\nDivisão inteira: / (ex.: (A B /) para inteiros);\nResto da divisão inteira: % (ex.: (A B %));\nPotenciação: ^ (ex.: (A B ^), onde B é um inteiro positivo);\n\nTodas as operações (exceto divisão inteira e resto) usam números reais em formato de meia precisão (16 bits, IEEE 754), com duas casas decimais. A página Os desafios da norma IEEE 754 contém informações relevantes sobre a norma IEEE 754.\nExpressões podem ser aninhadas sem limite, por exemplo:\n\n(A (C D *) +): Soma A ao produto de C e D;\n((A B *) (D E *) /): Divide o produto de A e B pelo produto de D e E.\n((A B +) (C D *) /): Divide a soma de A e B pelo produto de C e D.\n\nA ordem de precedência das operações segue a ordem de precedência usual em matemática.\n\n\n\n9.2.2 Comandos Especiais\nA linguagem que estamos criando inclui comandos especiais para manipulação de memória e resultados:\n\n(N RES): Retorna o resultado da expressão N linhas anteriores (N é um inteiro não negativo).\n(V MEM): Armazena o valor real V em uma memória chamada MEM.\n(MEM): Retorna o valor armazenado em MEM. Se a memória não foi inicializada, retorna \\(0.0\\).\nCada arquivo de texto, código fonte da linguagem que estamos criando, representa um escopo independente de memória.\nMEM pode ser qualquer conjunto de letras maiúsculas, tal como MEM, VAR, X, etc.\nRES é uma keyword da linguagem que estamos criando. A única keyword da linguagem nesta fase.",
    "crumbs": [
      "Projeto da Disciplina - 2025-2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#analisador-léxico-com-autômatos-finitos-determinístico",
    "href": "fase1.html#analisador-léxico-com-autômatos-finitos-determinístico",
    "title": "9  Fase 1 - Projeto Prático",
    "section": "9.3 Analisador Léxico com Autômatos Finitos Determinístico",
    "text": "9.3 Analisador Léxico com Autômatos Finitos Determinístico\n\nO analisador léxico deve ser implementado usando Autômatos Finitos Determinísticos, com cada estado representado por uma função. Qualquer forma diferente de implementação provocará o zeramento do trabalho.\nO Autômato Finito Determinístico deve reconhecer tokens válidos: números reais (duas casas decimais), operadores (+, -, *, /, %, ^), comandos especiais (RES, MEM), e parênteses.\nFunções de teste específicas devem ser criadas para validar o analisador léxico, cobrindo:\n\n\nEntradas válidas (ex.: (3.14 2.0 +), (5 RES), (10.5 CONTADOr));\nEntradas inválidas (ex.: (3.14 2.0 &), números malformados como 3.14.5, 3,45 ou parênteses desbalanceados).",
    "crumbs": [
      "Projeto da Disciplina - 2025-2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#arquivos-de-teste",
    "href": "fase1.html#arquivos-de-teste",
    "title": "9  Fase 1 - Projeto Prático",
    "section": "9.4 Arquivos de Teste",
    "text": "9.4 Arquivos de Teste\n\nFornecer mínimo de 3 arquivos de texto, cada um com pelo menos 10 linhas de expressões segundo as especificações deste documento.\nCada arquivo deve incluir todas as operações (+, -, *, /, %, ^) e comandos especiais ((N RES), (V MEM), (MEM)).\nOs arquivos devem estar no mesmo diretório do código-fonte e ser processados via argumento de linha de comando (ex.: ./NomeDoSeuPrograma teste1.txt).\nO programa não deve incluir menu ou qualquer seleção interativa de arquivos.",
    "crumbs": [
      "Projeto da Disciplina - 2025-2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#hospedagem-no-github",
    "href": "fase1.html#hospedagem-no-github",
    "title": "9  Fase 1 - Projeto Prático",
    "section": "9.5 Hospedagem no GitHub",
    "text": "9.5 Hospedagem no GitHub\n\nO projeto deve ser hospedado em um repositório público no GitHub. Se o professor não indicar qual repositório deve ser usado, o repositório deve ser criado por um dos alunos do grupo.\nO repositório deve conter:\n\nCódigo-fonte do programa;\nArquivos de teste (mínimo 3);\nFunções de teste para o analisador léxico;\nÚltima versão do Código Assembly para Arduino gerado pelo programa;\nDocumentação (ex.: README.md) explicando como compilar, executar e testar o programa. Contendo o nome da instituição, disciplina, professor e nome dos alunos do grupo, em ordem alfabética seguido do usuário deste aluno no github.\n\nO repositório deve ser organizado com commits claros, as contribuições de cada um dos alunos devem estar registradas na forma de pull requests.",
    "crumbs": [
      "Projeto da Disciplina - 2025-2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#requisitos-do-código",
    "href": "fase1.html#requisitos-do-código",
    "title": "9  Fase 1 - Projeto Prático",
    "section": "9.6 Requisitos do Código",
    "text": "9.6 Requisitos do Código\n\nAs primeiras linhas do código devem conter:\n\n\nNomes dos integrantes do grupo, em ordem alfabética seguidos do usuário deste aluno no github.\nNome do grupo no ambiente virtual de aprendizagem (Canvas).\n\n\nO programa deve receber o nome do arquivo de teste como argumento na linha de comando.\nO código deve ser escrito em Python, C, ou C++. Com as funções nomeadas como está explicitado na Section 9.7.\nA última versão do código Assembly gerado para o Arduino deve ser funcional e incluído no repositório.\nA versão em Assembly deve ser exatamente o mesmo algoritmo explicitado em cada texto de teste de forma que o resultados das expressões seja obtido pelo cálculo realizado no Arduino.",
    "crumbs": [
      "Projeto da Disciplina - 2025-2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#sec-divisaoTarefas",
    "href": "fase1.html#sec-divisaoTarefas",
    "title": "9  Fase 1 - Projeto Prático",
    "section": "9.7 Divisão de Tarefas para a Fase 1",
    "text": "9.7 Divisão de Tarefas para a Fase 1\nPara resolver o problema de processamento da expressões definidas nos arquivos de teste da fase 1, o trabalho será dividido entre até quatro alunos, trabalhando independentemente, na mesma sala, ou de forma remota. Cada aluno será responsável por uma parte específica do sistema, com interfaces claras para facilitar a integração. Abaixo está uma sugestão da divisão das tarefas, considerando as funções solicitadas: parseExpressao, executarExpressao, gerarAssembly, e exibirResultados.\n\n\n\n\n\n\nWarning\n\n\n\nNota: As tarefas podem ser divididas da forma que cada grupo achar mais conveniente, desde que as funções e interfaces sejam respeitadas.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNota: O vetor de tokens gerado pelo Analisador Léxico deve ser salvo em txt para uso nas próximas fases do projeto. Cabe ao grupo decidir o formato que será usado para salvar os tokens. Apenas os tokens referentes a última execução do código do analisador léxico devem ser salvos.\n\n\n\n9.7.1 Aluno 1: Função parseExpressao e Analisador Léxico com Autômato Finito Determinístico\nResponsabilidades:\n\nImplementar parseExpressao(std::string linha, std::vector&lt;std::string&gt;& _tokens_) (ou equivalente em Python/C) para analisar uma linha de expressão RPN e extrair tokens.\nImplementar o analisador léxico usando Autômatos Finitos Determinísticos (AFDs), com cada estado como uma função (ex.: estadoNumero, estadoOperador, estadoParenteses).\nValidar tokens:\n\nNúmeros reais (ex.: 3.14) usando ponto como separador decimal;\nOperadores (+, -, *, /, %, ^);\nComandos especiais (RES, MEM) e parênteses;\nDetectar erros como números malformados (ex.: 3.14.5), parênteses desbalanceados ou operadores inválidos;\n\nCriar funções de teste para o analisador léxico, cobrindo entradas válidas e inválidas.\n\nTarefas Específicas:\n\nEscrever parseExpressao para dividir a linha em tokens usando um Autômato Finito Determinístico;\nValidar tokens;\nTestar o Autômato Finito Determinístico com entradas diversificadas (3.14 2.0 +), (5 RES), (3.14.5 2.0 +) (inválido);\nCriar um método, antes do Autômato Finito Determinístico, para lidar com parênteses aninhados.\n\nInterface:\n\nRecebe uma linha de texto e retorna um vetor de tokens;\nFornece tokens válidos para executarExpressao.\n\n\n\n9.7.2 Aluno 2: Função executarExpressao e Gerenciamento de Memória\nResponsabilidades:\n\nImplementar executarExpressao(const std::vector&lt;std::string&gt;& _tokens_, std::vector&lt;float&gt;& resultados, float& memoria) para executar uma expressão RPN;\nGerenciar a memória MEM para comandos (V MEM) e (MEM);\nManter um histórico de resultados para suportar (N RES);\nCriar funções de teste para validar a execução de expressões e comandos especiais.\n\nTarefas Específicas:\n\nUsar uma pilha para avaliar expressões RPN (ex.: em C++: std::stack&lt;float&gt;);\nImplementar operações (+, -, *, /, %, ^) com precisão de 16 bits (IEEE 754);\nTratar divisão inteira e resto separadamente;\nTestar com expressões como (3.14 2.0 +), ((1.5 2.0 *) (3.0 4.0 *) /), (5.0 MEM), (2 RES);\nVerificar erros como divisão por zero ou N inválido em (N RES).\n\nInterface:\n\nRecebe tokens de parseExpressao e atualiza resultados e memoria;\nFornece resultados para exibirResultados e Assembly.\n\n\n\n9.7.3 Aluno 3: Função gerarAssembly e Leitura de Arquivo\nResponsabilidades:\n\nImplementar gerarAssembly(const std::vector&lt;std::string&gt;& _tokens_, std::string& codigoAssembly) para gerar código Assembly para Arduino;\nImplementar lerArquivo(std::string nomeArquivo, std::vector&lt;std::string&gt;& linhas) para ler o arquivo de entrada;\nCriar funções de teste para validar a leitura de arquivos e a geração de Assembly; Lembre-se o Assembly deve conter todas as operações do texto de teste.\nAlertar se o arquivo tiver linhas malformadas ou exceder limites.\n\nTarefas Específicas:\n\nLer o arquivo de tokens linha por linha, ignorando linhas vazias;\nGerar Assembly AVR para operações RPN e comandos especiais, usando registradores e instruções do Arduíno considerando os modelos adequados a esta fase;\nTestar com arquivos contendo 10 linhas, ou mais, incluindo expressões aninhadas e comandos especiais;\nVerificar erros de abertura de arquivo e exibir mensagens claras.\n\nInterface:\n\nlerArquivo fornece linhas para parseExpressao;\ngerarAssembly produz código Assembly para Arduino.\n\n\n\n9.7.4 Aluno 4: Função exibirResultados, Interface do Usuário e Testes\nResponsabilidades:\n\nImplementar exibirResultados(const std::vector&lt;float&gt;& resultados) para exibir os resultados das expressões;\nImplementar exibirMenu() e gerenciar a interface no main, incluindo leitura do argumento de linha de comando;\nCorrigir problemas de entrada (ex.: em C++: std::cin.ignore(std::numeric_limits&lt;std::streamsize&gt;::max(), '\\n'));\nCriar funções de teste para validar a saída e o comportamento do programa completo.\n\nTarefas Específicas:\n\nExibir resultados com formato claro (ex.: uma casa decimal para números reais);\nImplementar o main para chamar lerArquivo, parseExpressao, executarExpressao, e exibirResultados;\nTestar com arquivos de teste fornecidos, verificando saídas para expressões simples e complexas;\nTestar o FSM com comandos especiais como (V MEM) e (MEM);\n\nInterface:\n\nUsa resultados de executarExpressao para exibir saídas;\nGerencia a execução do programa via argumento de linha de comando.",
    "crumbs": [
      "Projeto da Disciplina - 2025-2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#considerações-para-integração",
    "href": "fase1.html#considerações-para-integração",
    "title": "9  Fase 1 - Projeto Prático",
    "section": "9.8 Considerações para Integração",
    "text": "9.8 Considerações para Integração\n\nInterfaces: concordar com assinaturas das funções e formatos de dados (ex.: vetor de tokens, resultados em float);\nDepuração: testar cada parte isoladamente, simulando entradas/saídas;\nPassos de Integração:\n\nCopiar main e exibirMenu do Aluno 4;\nInserir lerArquivo e gerarAssembly do Aluno 3;\nAdicionar executarExpressao do Aluno 2;\nIncluir parseExpressao do Aluno 1;\n\nResolução de Conflitos: discutir problemas imediatamente na sala, ou de forma remota.\nDepuração Final: Testar o programa com os 3 arquivos de teste, verificando expressões, comandos especiais, e saída Assembly.",
    "crumbs": [
      "Projeto da Disciplina - 2025-2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#avaliação",
    "href": "fase1.html#avaliação",
    "title": "9  Fase 1 - Projeto Prático",
    "section": "9.9 Avaliação",
    "text": "9.9 Avaliação\nO trabalho será avaliado antes da prova de autoria, com os seguintes critérios:\n\nCálculos e Funcionalidades (70%):\n\nImplementação completa de todas as operações RPN e comandos especiais.\nCada operação não implementada reduz 10% dos 70%.\nFalha na divisão inteira reduz 50% dos 70%.\nAnalisador léxico com FSM funcional e testado.\n\nOrganização e Legibilidade do Código (15%):\n\nCódigo claro, comentado e bem estruturado.\nRepositório GitHub organizado, com README claro.\n\nRobustez (15%):\n\nTratamento de erros em expressões complexas e entradas inválidas.\nTestes do analisador léxico cobrindo todos os casos.",
    "crumbs": [
      "Projeto da Disciplina - 2025-2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#prova-de-autoria",
    "href": "fase1.html#prova-de-autoria",
    "title": "9  Fase 1 - Projeto Prático",
    "section": "9.10 Prova de Autoria",
    "text": "9.10 Prova de Autoria\n\nUm aluno do grupo será sorteado, por um sistema disponível online para responder uma pergunta em uma lista de 10 perguntas;\nFalha na resposta reduz 35% da nota obtida na Avaliação do Projeto.",
    "crumbs": [
      "Projeto da Disciplina - 2025-2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#entrega",
    "href": "fase1.html#entrega",
    "title": "9  Fase 1 - Projeto Prático",
    "section": "9.11 Entrega",
    "text": "9.11 Entrega\n\nO repositório GitHub deve conter:\n\nCódigo-fonte (Python, C, ou C++);\nTrês arquivos de teste com expressões RPN;\nFunções de teste para o analisador léxico;\nCódigo Assembly para Arduino da última execução do analisador léxico;\nArquivo de texto contendo os tokens gerados na última execução do analisador léxico;\nREADME com instruções de compilação, execução e testes;\n\nO programa deve ser executado com o comando ./NomeDoSeuPrograma.cpp.",
    "crumbs": [
      "Projeto da Disciplina - 2025-2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#toolchain-assembly-para-arduino-linha-de-comando",
    "href": "fase1.html#toolchain-assembly-para-arduino-linha-de-comando",
    "title": "9  Fase 1 - Projeto Prático",
    "section": "10.1 Toolchain Assembly para Arduino (Linha de Comando)",
    "text": "10.1 Toolchain Assembly para Arduino (Linha de Comando)\nO conjunto de ferramentas (toolchain) padrão, gratuito e de código aberto para a arquitetura AVR (usada no Arduino Uno/Mega) é o AVR-GCC Toolchain e o AVRDUDE. Este guia mostra como compilar um arquivo Assembly (.s) e enviá-lo para a placa via linha de comando, sem usar a IDE do Arduino.\n\n\n\n\n\n\nWarning\n\n\n\nNota: O plugin platformIO do VsCode deve simplificar muito este processo. Cabe ao grupo instalar e testar este plugin na máquina que usará para fazer a prova de autoria. Não ser capaz de gerar o código Assembly por qualquer motivo pode resulta em zeramento da tarefa.\nVocê não precisará fazer nada manualmente na linha de comando. O PlatformIO gerenciará toda a cadeia de ferramentas (compilador, linker, uploader) para você, bastando clicar nos botões de Compilar e Gravar.",
    "crumbs": [
      "Projeto da Disciplina - 2025-2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#ferramentas-necessárias",
    "href": "fase1.html#ferramentas-necessárias",
    "title": "9  Fase 1 - Projeto Prático",
    "section": "10.2 1. Ferramentas Necessárias",
    "text": "10.2 1. Ferramentas Necessárias\nVocê precisa instalar:\n\navr-gcc: compilador/montador para AVR.\navr-libc: biblioteca C padrão para AVR (necessária mesmo em código assembly puro porque fornece símbolos de inicialização como __do_copy_data, __do_clear_bss, etc.).\navrdude: programa de upload para o microcontrolador.",
    "crumbs": [
      "Projeto da Disciplina - 2025-2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#instalação",
    "href": "fase1.html#instalação",
    "title": "9  Fase 1 - Projeto Prático",
    "section": "10.3 2. Instalação",
    "text": "10.3 2. Instalação\n\n10.3.1 Linux (Debian/Ubuntu)\nsudo apt update\nsudo apt install gcc-avr binutils-avr avr-libc avrdude\n\nObservação: Para outras distribuições, use o gerenciador de pacotes correspondente (ex: dnf para Fedora ou pacman para Arch).\n\n\n\n10.3.2 Windows\n\n10.3.2.1 Opção 1: Microchip AVR-GCC Toolchain (Recomendado)\n\nFaça o download do “AVR 8-bit Toolchain” do site da Microchip.\nInstale o .exe e adicione a pasta bin da toolchain à variável de ambiente PATH (ex: C:\\Program Files (x86)\\Atmel\\Studio\\7.0\\toolchain\\avr8\\avr8-gnu-toolchain\\bin).\nInstale avrdude via Chocolatey ou Scoop:\nchoco install avrdude\n\n\n\n10.3.2.2 Opção 2: WSL (Windows Subsystem for Linux)\n\nInstale o WSL e uma distribuição Linux (ex: Ubuntu) via Microsoft Store.\nSiga as mesmas instruções de instalação para Linux dentro do ambiente WSL.",
    "crumbs": [
      "Projeto da Disciplina - 2025-2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#o-processo-de-compilação-e-upload",
    "href": "fase1.html#o-processo-de-compilação-e-upload",
    "title": "9  Fase 1 - Projeto Prático",
    "section": "10.4 3. O Processo de Compilação e Upload",
    "text": "10.4 3. O Processo de Compilação e Upload\nAssumindo que você tenha um arquivo chamado meu_codigo.s.\n\n10.4.1 Passos Principais\n\nMontar e Linkar: Converter o código Assembly (.s) em um arquivo executável no formato ELF (.elf).\nExtrair o HEX: Converter o arquivo .elf para o formato Intel HEX (.hex).\nFazer o Upload: Enviar o arquivo .hex para a memória flash do Arduino usando avrdude.\n\n\n\n10.4.2 Exemplo Completo para Arduino UNO\n\n10.4.2.1 Informações do Hardware\n\nMCU: atmega328p\nProgramador: arduino\nBaud Rate: 115200\nPorta Serial: /dev/ttyACM0 (Linux) ou COM3 (Windows)\n\n# Nome do arquivo de entrada (sem extensão)\nFILENAME=meu_codigo\n\n# MCU e parâmetros do avrdude\nMCU=atmega328p\nAVRDUDE_PARTNO=m328p\nAVRDUDE_PROGRAMMER=arduino\nAVRDUDE_PORT=COM3 # Mude para sua porta. Ex: /dev/ttyACM0 no Linux\nAVRDUDE_BAUDRATE=115200\n\n# 1. Montar e Linkar (Assembly -&gt; ELF)\n# Usamos avr-gcc como front-end. Ele chamará o montador (avr-as) e o linker (avr-ld)\navr-gcc -mmcu=$MCU -o $FILENAME.elf $FILENAME.s\n\n# 2. Extrair o arquivo .hex (ELF -&gt; HEX)\n# -O ihex: formato de saída Intel HEX\n# -R .eeprom: remove a seção de dados da EEPROM do arquivo de saída\navr-objcopy -O ihex -R .eeprom $FILENAME.elf $FILENAME.hex\n\n# 3. Fazer o Upload para o Arduino UNO\navrdude -c $AVRDUDE_PROGRAMMER -p $AVRDUDE_PARTNO -P $AVRDUDE_PORT -b $AVRDUDE_BAUDRATE -U flash:w:$FILENAME.hex\n\n\n10.4.2.2 Exemplo Completo para Arduino MEGA\n\n\n\n10.4.3 Informações do Hardware:\n\nMCU: atmega2560\nProgramador: wiring\nBaud Rate: 115200\nPorta Serial: /dev/ttyACM0 (Linux) ou COM4 (Windows)\n\n# Nome do arquivo de entrada (sem extensão)\nFILENAME=meu_codigo_mega\n\n# MCU e parâmetros do avrdude\nMCU=atmega2560\nAVRDUDE_PARTNO=m2560\nAVRDUDE_PROGRAMMER=wiring\nAVRDUDE_PORT=COM4 # Mude para sua porta. Ex: /dev/ttyACM0 no Linux\nAVRDUDE_BAUDRATE=115200\n\n# 1. Montar e Linkar (Assembly -&gt; ELF)\navr-gcc -mmcu=$MCU -o $FILENAME.elf $FILENAME.s\n\n# 2. Extrair o arquivo .hex (ELF -&gt; HEX)\navr-objcopy -O ihex -R .eeprom $FILENAME.elf $FILENAME.hex\n\n# 3. Fazer o Upload para o Arduino MEGA\navrdude -c $AVRDUDE_PROGRAMMER -p $AVRDUDE_PARTNO -P $AVRDUDE_PORT -b $AVRDUDE_BAUDRATE -U flash:w:$FILENAME.hex",
    "crumbs": [
      "Projeto da Disciplina - 2025-2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "fase1.html#observações-importantes",
    "href": "fase1.html#observações-importantes",
    "title": "9  Fase 1 - Projeto Prático",
    "section": "10.5 Observações Importantes",
    "text": "10.5 Observações Importantes\n\nDiferença entre avr-gcc e avrdude:\n\navr-gcc usa -mmcu=atmega328p.\navrdude usa -p m328p.\n\nBootloader:\n\nPressione o botão Reset na placa antes de executar avrdude para garantir que o bootloader esteja ativo.\n\nExemplo Prático:\n\n// meu_codigo.s\n.device atmega328p\n.org 0x0000\nrjmp reset\n\nreset:\n    ldi r16, 0xFF\n    out 0x24, r16        ; DDRB = 0xFF (PB7..PB0 como saída)\n    ldi r16, 0x00\n    out 0x25, r16        ; PORTB = 0x00 (todos apagados)\n\nloop:\n    out 0x25, r16        ; PORTB = valor atual\n    ldi r16, 0x01\n    lsr r16              ; desloca bit para baixo\n    rjmp loop\nCompile e envie:\navr-gcc -mmcu=atmega328p -o led.elf led.s\navr-objcopy -O ihex -R .eeprom led.elf led.hex\navrdude -c arduino -p m328p -P COM3 -b 115200 -U flash:w:led.hex",
    "crumbs": [
      "Projeto da Disciplina - 2025-2",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Fase 1 - Projeto Prático</span>"
    ]
  },
  {
    "objectID": "apend1.html",
    "href": "apend1.html",
    "title": "10  Apêndice 1: A Relação de Myhill-Nerode",
    "section": "",
    "text": "10.1 Definição Formal da Relação\nSeja \\(L\\) uma linguagem sobre um alfabeto \\(\\Sigma\\). A relação de Myhill-Nerode para \\(L\\) é uma relação de equivalência \\(\\sim_L\\) definida sobre \\(\\Sigma^*\\) (o conjunto de todas as strings possíveis sobre \\(\\Sigma\\)).\nDefinição: Para duas strings \\(x, y \\in \\Sigma^*\\), dizemos que \\(x \\sim_L y\\) (lê-se “\\(x\\) é equivalente a \\(y\\) módulo \\(L\\)”) se, e somente se:\n\\[\\forall z \\in \\Sigma^*: (xz \\in L \\iff yz \\in L)\\]\nEm outras palavras, duas strings são equivalentes segundo Myhill-Nerode se, ao concatenarmos qualquer string \\(z\\) a cada uma delas, ambas as strings resultantes têm o mesmo comportamento em relação à linguagem \\(L\\): ou ambas pertencem a \\(L\\), ou ambas não pertencem.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Apêndice 1: A Relação de Myhill-Nerode</span>"
    ]
  },
  {
    "objectID": "apend1.html#definição-formal-da-relação",
    "href": "apend1.html#definição-formal-da-relação",
    "title": "10  Apêndice 1: A Relação de Myhill-Nerode",
    "section": "",
    "text": "10.1.1 Intuição por Trás da Definição\nA intuição fundamental é que strings equivalentes são indistinguíveis do ponto de vista de \\(L\\). Se um autômato finito está processando uma entrada e chega a um estado após ler \\(x\\), ele deveria estar no mesmo estado após ler \\(y\\) (se \\(x \\sim_L y\\)), porque qualquer continuação da entrada (\\(z\\)) levará ao mesmo resultado final.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Apêndice 1: A Relação de Myhill-Nerode</span>"
    ]
  },
  {
    "objectID": "apend1.html#classes-de-equivalência",
    "href": "apend1.html#classes-de-equivalência",
    "title": "10  Apêndice 1: A Relação de Myhill-Nerode",
    "section": "10.2 Classes de Equivalência",
    "text": "10.2 Classes de Equivalência\nA relação \\(\\sim_L\\) particiona \\(\\Sigma^*\\) em classes de equivalência. Cada classe agrupa todas as strings que são mutuamente equivalentes segundo a relação.\nNotação: A classe de equivalência de uma string \\(x\\) é denotada por \\([x]_L\\) ou simplesmente \\([x]\\) quando \\(L\\) está claro no contexto:\n\\[[x]_L = \\{y \\in \\Sigma^* \\mid x \\sim_L y\\}\\]\n\n10.2.1 Exemplo Fundamental\nConsidere a linguagem \\(L = \\{w \\in \\{0,1\\}^* \\mid w \\text{ termina em } 01\\}\\).\nAnálise das classes:\n\nStrings que terminam em \\(0\\): \\([0] = \\{0, 10, 00, 110, 010, \\ldots\\}\\)\n\nPara qualquer \\(z\\), temos \\(xz \\in L\\) se e somente se \\(z\\) começar com \\(1\\)\n\nStrings que terminam em \\(1\\) (exceto \\(01\\)): \\([1] = \\{1, 11, 001, 101, \\ldots\\}\\)\n\nPara qualquer \\(z\\), temos \\(xz \\in L\\) se e somente se \\(z = \\epsilon\\) ou \\(z\\) começar com \\(0\\) seguido de algo que termine em \\(1\\)\n\nStrings que terminam em \\(01\\): \\([01] = \\{01, 101, 001, 1001, \\ldots\\}\\)\n\nTemos \\(x\\epsilon = x \\in L\\), então \\(z = \\epsilon\\) sempre leva à aceitação\n\nString vazia e outras: \\([\\epsilon] = \\{\\epsilon, 11, 0011, \\ldots\\}\\)\n\nApós análise cuidadosa, esta linguagem possui exatamente 3 classes de equivalência.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Apêndice 1: A Relação de Myhill-Nerode</span>"
    ]
  },
  {
    "objectID": "apend1.html#o-teorema-de-myhill-nerode",
    "href": "apend1.html#o-teorema-de-myhill-nerode",
    "title": "10  Apêndice 1: A Relação de Myhill-Nerode",
    "section": "10.3 O Teorema de Myhill-Nerode",
    "text": "10.3 O Teorema de Myhill-Nerode\nO teorema principal estabelece a conexão fundamental entre regularidade e finitude das classes de equivalência.\nTeorema (Myhill-Nerode): Uma linguagem \\(L\\) é regular se, e somente se, a relação \\(\\sim_L\\) possui um número finito de classes de equivalência.\n\n10.3.1 Demonstração (Esboço)\n(\\(\\Rightarrow\\)) Se \\(L\\) é regular, então \\(\\sim_L\\) tem finitas classes:\nSe \\(L\\) é regular, existe um autômato finito determinístico (AFD) \\(M\\) com \\(n\\) estados que reconhece \\(L\\). Definimos uma função \\(f: \\Sigma^* \\to Q\\) (onde \\(Q\\) é o conjunto de estados) que mapeia cada string para o estado em que \\(M\\) para após processá-la.\nSe \\(f(x) = f(y)\\), então para qualquer \\(z\\), o autômato, partindo do mesmo estado, chegará ao mesmo estado final ao processar \\(z\\). Logo, \\(xz \\in L \\iff yz \\in L\\), provando que \\(x \\sim_L y\\).\nComo há no máximo \\(n\\) estados distintos, há no máximo \\(n\\) classes de equivalência.\n(\\(\\Leftarrow\\)) Se \\(\\sim_L\\) tem finitas classes, então \\(L\\) é regular:\nSuponha que existam \\(k\\) classes de equivalência: \\(C_1, C_2, \\ldots, C_k\\). Construímos um AFD onde:\n\nEstados: \\(Q = \\{C_1, C_2, \\ldots, C_k\\}\\)\nEstado inicial: \\(C_i\\) tal que \\(\\epsilon \\in C_i\\)\n\nEstados finais: \\(F = \\{C_i \\mid \\exists x \\in C_i: x \\in L\\}\\)\nFunção de transição: \\(\\delta(C_i, a) = C_j\\) onde \\(j\\) é tal que se \\(x \\in C_i\\), então \\(xa \\in C_j\\)\n\nEste autômato reconhece exatamente \\(L\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Apêndice 1: A Relação de Myhill-Nerode</span>"
    ]
  },
  {
    "objectID": "apend1.html#aplicações-práticas",
    "href": "apend1.html#aplicações-práticas",
    "title": "10  Apêndice 1: A Relação de Myhill-Nerode",
    "section": "10.4 Aplicações Práticas",
    "text": "10.4 Aplicações Práticas\n\n10.4.1 1. Determinação de Regularidade\nA relação de Myhill-Nerode fornece um método sistemático para provar que uma linguagem não é regular:\nExemplo: \\(L = \\{a^nb^n \\mid n \\geq 0\\}\\)\nPara cada \\(i \\neq j\\), as strings \\(a^i\\) e \\(a^j\\) estão em classes diferentes, de forma que:\n\n\\(a^i b^i \\in L\\) mas \\(a^j b^i \\notin L\\) (quando \\(i \\neq j\\))\n\nComo há infinitas strings \\(a^i\\) duas-a-duas não equivalentes, \\(L\\) não é regular.\n\n\n10.4.2 2. Minimização de Autômatos\nO teorema de Myhill-Nerode também caracteriza o autômato mínimo para uma linguagem regular:\nTeorema: Se \\(L\\) é uma linguagem regular com \\(k\\) classes de equivalência segundo Myhill-Nerode, então: 1. Todo AFD que reconhece \\(L\\) tem pelo menos \\(k\\) estados 2. Existe um único AFD (a menos de isomorfismo) com exatamente \\(k\\) estados que reconhece \\(L\\)\nEste AFD é chamado de autômato canônico ou autômato mínimo para \\(L\\).\n\n\n10.4.3 3. Algoritmo de Construção do Autômato Mínimo\nEntrada: Uma linguagem regular \\(L\\)\nSaída: O autômato mínimo para \\(L\\)\nPassos: 1. Identificar representantes: Encontre strings representativas de cada classe de equivalência 2. Construir estados: Cada classe torna-se um estado 3. Definir transições: Para cada classe \\([x]\\) e símbolo \\(a\\), determine \\([xa]\\) 4. Marcar estados finais: Classes contendo strings de \\(L\\)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Apêndice 1: A Relação de Myhill-Nerode</span>"
    ]
  },
  {
    "objectID": "apend1.html#exemplo-detalhado-construção-passo-a-passo",
    "href": "apend1.html#exemplo-detalhado-construção-passo-a-passo",
    "title": "10  Apêndice 1: A Relação de Myhill-Nerode",
    "section": "10.5 Exemplo Detalhado: Construção Passo a Passo",
    "text": "10.5 Exemplo Detalhado: Construção Passo a Passo\nLinguagem: \\(L = \\{w \\in \\{a,b\\}^* \\mid |w|_a \\text{ é par}\\}\\) (número par de \\(a\\)’s)\nPasso 1 - Identificação das classes:\nPrecisamos rastrear apenas a paridade do número de \\(a\\)’s lidos:\n\nClasse \\(C_0\\) (par de \\(a\\)’s): \\([\\epsilon] = \\{b^*ab^*ab^*...\\}\\) onde há um número par de \\(a\\)’s\nClasse \\(C_1\\) (ímpar de \\(a\\)’s): \\([a] = \\{b^*ab^*...\\}\\) onde há um número ímpar de \\(a\\)’s\n\nPasso 2 - Verificação da equivalência:\nPara \\(x, y \\in C_0\\): qualquer \\(z\\) mudará ambos da mesma forma (mantendo ou alternando paridade)\nPara \\(x \\in C_0, y \\in C_1\\): existe \\(z = \\epsilon\\) tal que \\(x \\in L\\) mas \\(y \\notin L\\)\nPasso 3 - Construção do autômato:\nEstados: {C_0, C_1}\nEstado inicial: C_0\nEstados finais: {C_0}\n\nTransições:\nC_0 --a--&gt; C_1\nC_0 --b--&gt; C_0  \nC_1 --a--&gt; C_0\nC_1 --b--&gt; C_1\nResultado: Autômato mínimo com 2 estados.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Apêndice 1: A Relação de Myhill-Nerode</span>"
    ]
  },
  {
    "objectID": "apend1.html#propriedades-importantes",
    "href": "apend1.html#propriedades-importantes",
    "title": "10  Apêndice 1: A Relação de Myhill-Nerode",
    "section": "10.6 Propriedades Importantes",
    "text": "10.6 Propriedades Importantes\n\n10.6.1 1. Refinamento de Relações\nSe \\(\\approx\\) é qualquer relação de equivalência sobre \\(\\Sigma^*\\) que respeita \\(L\\) (isto é, se \\(x \\approx y\\) então \\(\\forall z: xz \\in L \\iff yz \\in L\\)), então \\(\\sim_L\\) é um refinamento de \\(\\approx\\). Em outras palavras, Myhill-Nerode é a relação mais fina possível que mantém as propriedades da linguagem.\n\n\n10.6.2 2. Invariância por Operações\nA relação de Myhill-Nerode se comporta bem com operações regulares:\n\nUnião: \\(\\sim_{L_1 \\cup L_2}\\) tem no máximo \\(|\\sim_{L_1}| \\times |\\sim_{L_2}|\\) classes\nConcatenação: Comportamento mais complexo, mas ainda finito para linguagens regulares\nFechamento de Kleene: Preserve a finitude das classes\n\n\n\n10.6.3 3. Conexão com Congruências\nA relação \\(\\sim_L\\) é não apenas uma equivalência, mas também uma congruência à direita: se \\(x \\sim_L y\\), então \\(xz \\sim_L yz\\) para qualquer \\(z \\in \\Sigma^*\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Apêndice 1: A Relação de Myhill-Nerode</span>"
    ]
  },
  {
    "objectID": "apend1.html#limitações-e-extensões",
    "href": "apend1.html#limitações-e-extensões",
    "title": "10  Apêndice 1: A Relação de Myhill-Nerode",
    "section": "10.7 Limitações e Extensões",
    "text": "10.7 Limitações e Extensões\n\n10.7.1 Limitações\n\nComputabilidade: Para linguagens arbitrárias, determinar as classes pode ser indecidível\nComplexidade: Mesmo para linguagens regulares, o número de classes pode ser exponencial no tamanho da descrição\nAplicabilidade: Limitado a linguagens regulares por definição\n\n\n\n10.7.2 Extensões Modernas\n\nRelações de Nerode Generalizadas: Para linguagens livres de contexto\nMyhill-Nerode Probabilístico: Para autômatos probabilísticos\n\nVersões Categóricas: Em teoria das categorias e sistemas de tipos",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Apêndice 1: A Relação de Myhill-Nerode</span>"
    ]
  },
  {
    "objectID": "apend1.html#conclusão-a-elegância-da-teoria",
    "href": "apend1.html#conclusão-a-elegância-da-teoria",
    "title": "10  Apêndice 1: A Relação de Myhill-Nerode",
    "section": "10.8 Conclusão: A Elegância da Teoria",
    "text": "10.8 Conclusão: A Elegância da Teoria\nA relação de Myhill-Nerode exemplifica a beleza da matemática teórica aplicada à ciência da computação. Com uma definição simples e elegante, ela resolve problemas fundamentais:\n\nCaracteriza completamente as linguagens regulares\nFornece algoritmos para minimização de autômatos\nEstabelece limites teóricos para a representação de linguagens\nConecta aspectos algébricos e combinatórios da teoria\n\nPara a engenhosa leitora, esta teoria representa não apenas uma ferramenta técnica, mas um exemplo paradigmático de como abstração matemática pode revelar estruturas profundas em sistemas computacionais. A relação de Myhill-Nerode continua sendo uma das joias conceituais da ciência da computação teórica, influenciando desenvolvimentos em áreas que vão desde compiladores até aprendizado de máquina.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Apêndice 1: A Relação de Myhill-Nerode</span>"
    ]
  },
  {
    "objectID": "sol-exercicios.html",
    "href": "sol-exercicios.html",
    "title": "11  Solução dos Exercícios",
    "section": "",
    "text": "11.1 Capítulo: Chapter 3",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Solução dos Exercícios</span>"
    ]
  },
  {
    "objectID": "sol-exercicios.html#capítulo-sec-alfabeto-linguagem-string",
    "href": "sol-exercicios.html#capítulo-sec-alfabeto-linguagem-string",
    "title": "11  Solução dos Exercícios",
    "section": "",
    "text": "11.1.1 Exercícios 1: Section 3.1.3\n\nSolução:\n\n\n\\(\\Sigma_1 = \\{a, b, c, +, -, *, /, (, )\\}\\)\nContando cada símbolo: \\(a, b, c, +, -, *, /, (, )\\)\n\\[|\\Sigma_1| = 9\\]\n\\(\\Sigma_2 = \\{0, 1, 2, \\ldots, 9, A, B, C, D, E, F\\}\\) (hexadecimal)\nDígitos decimais: \\(\\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\\}\\) → 10 símbolos\nLetras hexadecimais: \\(\\{A, B, C, D, E, F\\}\\) → 6 símbolos\n\\[|\\Sigma_2| = 10 + 6 = 16\\]\n\\(\\Sigma_3 = \\{\\text{verdadeiro}, \\text{falso}, \\land, \\lor, \\neg, (, )\\}\\)\nContando cada símbolo: \\(\\text{verdadeiro}, \\text{falso}, \\land, \\lor, \\neg, (, )\\)\n\\[|\\Sigma_3| = 7\\]\n\n\nSolução:\n\n\nExpressões lógicas booleanas simples com variáveis \\(p\\), \\(q\\), \\(r\\):\n\\[\\Sigma_{\\text{bool}} = \\{p, q, r, \\land, \\lor, \\neg, (, )\\}\\]\nJustificativa: Inclui as três variáveis proposicionais, os conectivos lógicos básicos (conjunção, disjunção, negação) e parênteses para agrupamento.\nNúmeros em notação científica (ex: \\(1.23 \\times 10^{-4}\\)):\n\\[\\Sigma_{\\text{cient}} = \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, +, -, ., \\times, ^{}, 1, 0\\}\\]\nOu de forma mais concisa:\n\\[\\Sigma_{\\text{cient}} = \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, +, -, ., \\times, ^\\}\\]\nJustificativa: Inclui dígitos para o número base, sinais para número e expoente, ponto decimal, símbolo de multiplicação e símbolo de potência.\nCoordenadas cartesianas no formato \\((x, y)\\):\n\\[\\Sigma_{\\text{coord}} = \\{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, +, -, ., (, ), ,\\}\\]\nJustificativa: Inclui dígitos para números, sinais, ponto decimal, parênteses para delimitação e vírgula como separador.\n\n\nSolução:\n\n\n\\(A = \\emptyset\\)\nNÃO é um alfabeto válido.\nJustificativa: A definição formal exige que um alfabeto seja um conjunto finito não-vazio. Como \\(A\\) é o conjunto vazio, ele viola a condição \\(n \\geq 1\\).\n\\(B = \\{\\epsilon\\}\\)\nÉ um alfabeto válido.\nJustificativa: É um conjunto finito (\\(|B| = 1\\)) e não-vazio. O símbolo \\(\\epsilon\\) pode ser tratado como um símbolo atômico qualquer para fins de definição de alfabeto.\n\\(C = \\{1, 2, 3, \\ldots\\}\\)\nNÃO é um alfabeto válido.\nJustificativa: Este é o conjunto dos números naturais positivos, que é infinito (\\(|C| = \\infty\\)). A definição de alfabeto exige finitude.\n\\(D = \\{a, b, a\\}\\)\nÉ um alfabeto válido, mas equivalente a \\(\\{a, b\\}\\).\nJustificativa: Como conjuntos não possuem elementos repetidos, \\(D = \\{a, b\\}\\). É finito (\\(|D| = 2\\)) e não-vazio, portanto é um alfabeto válido.\n\n\nSolução:\n\nDado \\(S = \\{\\text{if}, \\text{then}, \\text{else}, \\text{fi}\\}\\), precisamos encontrar todos os símbolos que aparecem nas strings:\n\n\\(\\text{if}\\): símbolos \\(i, f\\)\n\\(\\text{then}\\): símbolos \\(t, h, e, n\\)\n\n\\(\\text{else}\\): símbolos \\(e, l, s, e\\) (note que \\(e\\) se repete)\n\\(\\text{fi}\\): símbolos \\(f, i\\)\n\nColetando todos os símbolos únicos:\n\\[\\Sigma_{\\text{mín}} = \\{e, f, h, i, l, n, s, t\\}\\]\nCardinalidade: \\(|\\Sigma_{\\text{mín}}| = 8\\)\nVerificação: Todas as strings em \\(S\\) podem ser formadas usando apenas estes símbolos, e nenhum símbolo pode ser removido sem impossibilitar a formação de pelo menos uma string.\n\nSolução:\n\nComparando \\(\\Sigma_A = \\{0, 1\\}\\) e \\(\\Sigma_B = \\{a, b, c\\}\\):\nCardinalidade: - \\(|\\Sigma_A| = 2\\) - \\(|\\Sigma_B| = 3\\)\nNúmero de strings de comprimento 3:\nPara um alfabeto de tamanho \\(n\\), o número de strings de comprimento \\(k\\) é \\(n^k\\).\n\nPara \\(\\Sigma_A\\): \\(|\\Sigma_A|^3 = 2^3 = 8\\) strings\nStrings: \\(\\{000, 001, 010, 011, 100, 101, 110, 111\\}\\)\nPara \\(\\Sigma_B\\): \\(|\\Sigma_B|^3 = 3^3 = 27\\) strings\nExemplos: \\(\\{aaa, aab, aac, aba, abb, abc, \\ldots, ccc\\}\\)\n\nAplicabilidade para representar números binários:\n\n\\(\\Sigma_A\\): Perfeitamente adequado para representação binária, porque contém exatamente os símbolos \\(0\\) e \\(1\\) necessários.\n\\(\\Sigma_B\\): Inadequado para representação binária direta. Seria necessário estabelecer uma convenção de mapeamento (por exemplo, \\(a \\rightarrow 0\\), \\(b \\rightarrow 1\\), e \\(c\\) seria um símbolo extra não utilizado).\n\nConclusão: \\(\\Sigma_A\\) é mais eficiente para representação binária, enquanto \\(\\Sigma_B\\) oferece maior capacidade expressiva para outras aplicações devido ao seu maior tamanho.\n\n\n11.1.2 Exercícios 2: Section 3.2.4\n\nSolução:\n\nDado: \\(x = ab\\) e \\(y = cd\\)\nConcatenações: - \\(xy = ab \\cdot cd = abcd\\) - \\(yx = cd \\cdot ab = cdab\\)\nObservação: Note que \\(xy \\neq yx\\), demonstrando que a concatenação não é comutativa.\nPotências: - \\(x^3 = (ab)^3 = ab \\cdot ab \\cdot ab = ababab\\) - \\(y^2 = (cd)^2 = cd \\cdot cd = cdcd\\)\nPotências de concatenações: - \\((xy)^2 = (abcd)^2 = abcd \\cdot abcd = abcdabcd\\) - \\(x^2y^2 = (ab)^2(cd)^2 = abab \\cdot cdcd = ababcdcd\\)\nObservação: \\((xy)^2 \\neq x^2y^2\\), mostrando que \\((uv)^n \\neq u^nv^n\\) em geral.\nComprimento de \\(x^n\\):\nComo \\(|x| = |ab| = 2\\), temos:\n\\[|x^n| = n \\cdot |x| = n \\cdot 2 = 2n\\]\nVerificação: \\(|x^3| = |ababab| = 6 = 2 \\cdot 3\\)\n\nSolução:\n\nAplicando a definição recursiva \\(\\epsilon^R = \\epsilon\\) e \\((wa)^R = aw^R\\):\n\n\\(w_1 = abcde\\)\nAplicando passo a passo: \\[w_1^R = (abcde)^R = e(abcd)^R = ed(abc)^R = edc(ab)^R = edcb(a)^R = edcba\\]\n\\(w_2 = palindromo\\)\n\\[w_2^R = (palindromo)^R = omordnilap\\]\n\\(w_3 = \\epsilon\\) (string vazia)\n\\[w_3^R = \\epsilon^R = \\epsilon\\]\n\nProva de que \\((\\epsilon)^R = \\epsilon\\):\nDemonstração: Pela definição recursiva, o caso base estabelece diretamente que \\(\\epsilon^R = \\epsilon\\). Isso é consistente, porque a string vazia não possui símbolos para inverter, mantendo-se inalterada.\n\nSolução:\n\nAssociatividade: \\((xy)z = x(yz)\\) para \\(x = a\\), \\(y = bc\\), \\(z = d\\)\n\\[\\text{Lado esquerdo: } (xy)z = (a \\cdot bc)d = (abc)d = abcd\\] \\[\\text{Lado direito: } x(yz) = a(bc \\cdot d) = a(bcd) = abcd\\]\n\\[\\therefore (xy)z = x(yz) = abcd\\]\nElemento neutro: \\(w\\epsilon = \\epsilon w = w\\) para \\(w = abc\\)\n\\[w\\epsilon = abc \\cdot \\epsilon = abc\\] \\[\\epsilon w = \\epsilon \\cdot abc = abc\\]\n\\[\\therefore w\\epsilon = \\epsilon w = w = abc\\]\nNão-comutatividade: Encontrar \\(x\\) e \\(y\\) tais que \\(xy \\neq yx\\)\nExemplo: \\(x = a\\) e \\(y = b\\)\n\\[xy = a \\cdot b = ab\\] \\[yx = b \\cdot a = ba\\]\nComo \\(ab \\neq ba\\), a concatenação não é comutativa.\n\nSolução:\n\nDado: \\(w = compilador\\)\nPrefixos próprios (todos os prefixos exceto a própria string):\n\\[\\{\\epsilon, c, co, com, comp, compi, compil, compila, compilado\\}\\]\nTotal de prefixos próprios: 9\nSufixos próprios (todos os sufixos exceto a própria string):\n\\[\\{\\epsilon, r, or, dor, ador, lador, ilador, pilador, mpilador\\}\\]\nTotal de sufixos próprios: 9\nSubstrings de comprimento 4:\nPosições possíveis para substrings de comprimento 4 em uma string de comprimento 10:\n\\[\\{comp, ompi, mpil, pila, ilad, lado, ador\\}\\]\nTotal de substrings de comprimento 4: 7\nContagem total: - Prefixos totais: 10 (incluindo \\(\\epsilon\\) e a própria string) - Sufixos totais: 10 (incluindo \\(\\epsilon\\) e a própria string)\nFórmula geral: Para uma string de comprimento \\(n\\), existem \\(n+1\\) prefixos e \\(n+1\\) sufixos.\n\nSolução:\n\nDado: \\(w = aba\\)\nCálculo de \\(w^R\\): \\[w^R = (aba)^R = a(ba)^R = ab(a)^R = aba\\]\nObservação: \\(w = aba\\) é um palíndromo, logo \\(w^R = w\\).\n\\((w^R)^2\\): \\[w^R = aba\\] \\[(w^R)^2 = (aba)^2 = aba \\cdot aba = abaaba\\]\n\\((w^2)^R\\): \\[w^2 = aba \\cdot aba = abaaba\\] \\[(w^2)^R = (abaaba)^R = abaaba\\]\n\\(w^R w\\): \\[w^R w = aba \\cdot aba = abaaba\\]\nVerificação se \\((w^2)^R = (w^R)^2\\):\n\\[\\text{Lado esquerdo: } (w^2)^R = abaaba\\] \\[\\text{Lado direito: } (w^R)^2 = abaaba\\]\n\\[\\therefore (w^2)^R = (w^R)^2\\]\nExplicação: Esta igualdade vale neste caso específico porque \\(w\\) é um palíndromo (\\(w = w^R\\)). Em geral, para strings arbitrárias, \\((w^n)^R = (w^R)^n\\), que se reduz a \\(w^n = w^n\\) quando \\(w\\) é um palíndromo.\nPropriedade geral: Para qualquer string \\(u\\) e inteiro positivo \\(n\\): \\[(u^n)^R = (u^R)^n\\]\nNo nosso caso, como \\(w^R = w\\), ambos os lados se tornam \\(w^2 = abaaba\\).\n\n\n11.1.3 Exercícios 3: Section 3.3.5\n\nSolução:\n\nDado: \\(L_1 = \\{a, ab, b\\}\\) e \\(L_2 = \\{b, ba, \\epsilon\\}\\)\nUnião: \\(L_1 \\cup L_2\\) \\[L_1 \\cup L_2 = \\{a, ab, b\\} \\cup \\{b, ba, \\epsilon\\} = \\{a, ab, b, ba, \\epsilon\\}\\]\nInterseção: \\(L_1 \\cap L_2\\) \\[L_1 \\cap L_2 = \\{a, ab, b\\} \\cap \\{b, ba, \\epsilon\\} = \\{b\\}\\]\nDiferença: \\(L_1 - L_2\\) \\[L_1 - L_2 = \\{a, ab, b\\} - \\{b, ba, \\epsilon\\} = \\{a, ab\\}\\]\nCardinalidades: - \\(|L_1 \\cup L_2| = |\\{a, ab, b, ba, \\epsilon\\}| = 5\\) - \\(|L_1 \\cap L_2| = |\\{b\\}| = 1\\)\nVerificação: \\(|L_1| + |L_2| - |L_1 \\cap L_2| = 3 + 3 - 1 = 5 = |L_1 \\cup L_2|\\)\n\nSolução:\n\nDado: \\(L_1 = \\{a, bb\\}\\) e \\(L_2 = \\{c, dd\\}\\)\n\\(L_1 \\cdot L_2\\):\nAplicando a definição \\(L_1 \\cdot L_2 = \\{xy \\mid x \\in L_1 \\text{ e } y \\in L_2\\}\\):\n\\[L_1 \\cdot L_2 = \\{ac, add, bbc, bbdd\\}\\]\nCálculo detalhado: - \\(a \\cdot c = ac\\) - \\(a \\cdot dd = add\\)\n- \\(bb \\cdot c = bbc\\) - \\(bb \\cdot dd = bbdd\\)\n\\(L_2 \\cdot L_1\\):\n\\[L_2 \\cdot L_1 = \\{ca, cbb, dda, ddbb\\}\\]\nCálculo detalhado: - \\(c \\cdot a = ca\\) - \\(c \\cdot bb = cbb\\) - \\(dd \\cdot a = dda\\) - \\(dd \\cdot bb = ddbb\\)\nCardinalidade: \\[|L_1 \\cdot L_2| = |L_1| \\times |L_2| = 2 \\times 2 = 4\\] \\[|L_2 \\cdot L_1| = |L_2| \\times |L_1| = 2 \\times 2 = 4\\]\nVerificação de comutatividade: \\[L_1 \\cdot L_2 = \\{ac, add, bbc, bbdd\\}\\] \\[L_2 \\cdot L_1 = \\{ca, cbb, dda, ddbb\\}\\]\nComo os conjuntos são distintos, \\(L_1 \\cdot L_2 \\neq L_2 \\cdot L_1\\). A concatenação de linguagens não é comutativa.\n\nSolução:\n\nDado: \\(L = \\{a, b\\}\\)\n\\(L^0\\): Por definição, \\(L^0 = \\{\\epsilon\\}\\) para qualquer linguagem \\(L\\).\n\\(L^1\\): \\[L^1 = L = \\{a, b\\}\\]\n\\(L^2\\): \\[L^2 = L \\cdot L = \\{xy \\mid x \\in L \\text{ e } y \\in L\\}\\] \\[L^2 = \\{aa, ab, ba, bb\\}\\]\nCálculo detalhado de \\(L^2\\): - \\(a \\cdot a = aa\\) - \\(a \\cdot b = ab\\) - \\(b \\cdot a = ba\\) - \\(b \\cdot b = bb\\)\nFórmula para \\(|L^n|\\):\nComo \\(|L| = 2\\), temos: \\[|L^n| = |L|^n = 2^n\\]\nVerificação: - \\(|L^0| = |\\{\\epsilon\\}| = 1 = 2^0\\) - \\(|L^1| = |\\{a, b\\}| = 2 = 2^1\\)\n- \\(|L^2| = |\\{aa, ab, ba, bb\\}| = 4 = 2^2\\)\nTrês primeiras strings de \\(L^3\\) em ordem lexicográfica:\n\\(L^3\\) contém todas as strings de comprimento 3 sobre \\(\\{a, b\\}\\).\nEm ordem lexicográfica: \\(\\{aaa, aab, aba, abb, baa, bab, bba, bbb\\}\\)\nTrês primeiras: \\(aaa, aab, aba\\)\n\nSolução:\n\nDado: \\(L = \\{ab\\}\\)\nElementos de \\(L^*\\) até strings de comprimento 6:\nPela definição: \\(L^* = L^0 \\cup L^1 \\cup L^2 \\cup L^3 \\cup \\ldots\\)\n\n\\(L^0 = \\{\\epsilon\\}\\)\n\\(L^1 = \\{ab\\}\\) (comprimento 2)\n\\(L^2 = \\{ab \\cdot ab\\} = \\{abab\\}\\) (comprimento 4)\n\n\\(L^3 = \\{ab \\cdot ab \\cdot ab\\} = \\{ababab\\}\\) (comprimento 6)\n\n\\[L^* \\text{ até comprimento 6} = \\{\\epsilon, ab, abab, ababab\\}\\]\n\\(L^+\\): \\[L^+ = L^1 \\cup L^2 \\cup L^3 \\cup \\ldots = L^* - \\{\\epsilon\\}\\] \\[L^+ = \\{ab, abab, ababab, abababab, \\ldots\\}\\]\nVerificações:\n\\(\\epsilon \\in L^*\\)? Sim, porque \\(\\epsilon \\in L^0\\) e \\(L^0 \\subseteq L^*\\).\n\\(\\epsilon \\in L^+\\)? Não, porque \\(L^+ = L^1 \\cup L^2 \\cup \\ldots\\) e \\(\\epsilon \\notin L^n\\) para \\(n \\geq 1\\) quando \\(L = \\{ab\\}\\).\nPadrão geral: Como \\(L = \\{ab\\}\\) contém apenas uma string de comprimento 2, temos: \\[L^* = \\{\\epsilon, ab, (ab)^2, (ab)^3, \\ldots\\} = \\{(ab)^n \\mid n \\geq 0\\}\\] \\[L^+ = \\{ab, (ab)^2, (ab)^3, \\ldots\\} = \\{(ab)^n \\mid n \\geq 1\\}\\]\n\nSolução:\n\nDado: \\(L = \\{a\\}\\)\nPropriedade 1: \\(L^* = L^+\\)?\n\\[L^* = \\{\\epsilon, a, aa, aaa, \\ldots\\} = \\{a^n \\mid n \\geq 0\\}\\] \\[L^+ = \\{a, aa, aaa, \\ldots\\} = \\{a^n \\mid n \\geq 1\\}\\]\nComo \\(\\epsilon \\in L^*\\) mas \\(\\epsilon \\notin L^+\\), temos \\(L^* \\neq L^+\\).\nResposta: FALSO\nPropriedade 2: \\(L^* \\cup L^+ = L^*\\)?\nComo \\(L^+ \\subseteq L^*\\) (pela definição \\(L^+ = L^* - \\{\\epsilon\\}\\) quando \\(\\epsilon \\notin L\\)), temos: \\[L^* \\cup L^+ = L^*\\]\nResposta: VERDADEIRO\nPropriedade 3: \\((L^*)^* = L^*\\)?\nEsta é uma propriedade geral do fechamento de Kleene.\nDemonstração: - \\((L^*)^*\\) contém todas as concatenações possíveis de elementos de \\(L^*\\) - Como \\(L^* = \\{a^n \\mid n \\geq 0\\}\\), qualquer concatenação de elementos de \\(L^*\\) resulta em \\(a^k\\) para algum \\(k \\geq 0\\) - Portanto, \\((L^*)^* \\subseteq L^*\\) - Como \\(L^* \\subseteq (L^*)^*\\) (porque \\(L^* \\subseteq (L^*)^1 \\subseteq (L^*)^*\\)), temos a igualdade\nResposta: VERDADEIRO\nPropriedade 4: Se \\(\\epsilon \\in L\\), então \\(L^+ = L^*\\)?\nNo nosso caso, \\(L = \\{a\\}\\) e \\(\\epsilon \\notin L\\), então a premissa é falsa.\nMas vamos analisar o caso geral: se \\(\\epsilon \\in L\\), então: - \\(L^* = L^0 \\cup L^1 \\cup L^2 \\cup \\ldots\\) - \\(L^+ = L^1 \\cup L^2 \\cup L^3 \\cup \\ldots\\) - Como \\(\\epsilon \\in L = L^1\\), temos \\(\\epsilon \\in L^+\\) - Como \\(\\epsilon \\in L^0\\) também, e \\(L^+ \\supseteq L^1\\), de fato \\(L^+ = L^*\\)\nResposta: VERDADEIRO (propriedade geral válida)\nPara nosso caso específico: A propriedade não se aplica porque \\(\\epsilon \\notin \\{a\\}\\).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Solução dos Exercícios</span>"
    ]
  },
  {
    "objectID": "sol-exercicios.html#capítulo-sec-aut-finitos-deterministicos",
    "href": "sol-exercicios.html#capítulo-sec-aut-finitos-deterministicos",
    "title": "11  Solução dos Exercícios",
    "section": "12.1 Capítulo sec-aut-finitos-deterministicos",
    "text": "12.1 Capítulo sec-aut-finitos-deterministicos\n\n12.1.1 Exercícios 1 {Section 4.1}\n1.\n\nQ=s_0,s_1,s_2,s_3: O conjunto finito de estados.\n\nSigma=a,b: O alfabeto de entrada.\n\ndelta: A função de transição, que mapeia pares de estado e símbolo para um estado de destino (ex: delta(s_0,a)=s_1).\n\ns_0: O estado inicial único.\n\nF=s_0,s_3: O conjunto de estados de aceitação (ou finais).\n\n2. Não, o modelo não representa um Autômato Finito Determinístico válido. A justificativa é que o conjunto de estados de aceitação F=q_2 contém um estado, q_2, que não pertence ao conjunto de estados da máquina, Q=q_0,q_1. A definição formal exige que FsubseteqQ.\n3. O domínio da função de transição é QtimesSigma. O número de pares nesse domínio é o produto das cardinalidades dos conjuntos, ou seja, ∣Q∣times∣Sigma∣=3times4=12.\n4. Sim, é possível. Se F=Q, significa que todo estado do autômato é um estado de aceitação. Isso implica que, independentemente da sequência de transições, o estado final será sempre de aceitação. Portanto, o autômato reconheceria todas as strings possíveis sobre seu alfabeto, incluindo a string vazia. A linguagem reconhecida seria Sigma\\*.\n5. Sim, é perfeitamente possível. Se o estado inicial q_0 também é um estado de aceitação (q_0inF), isso significa que a computação que começa e termina em q_0 sem consumir nenhum símbolo é uma computação de aceitação. Portanto, a string vazia (epsilon) é aceita pelo autômato.\n\n\n12.1.2 Exercícios 2 {(exercicios-2?)}\n1. Não, o autômato não é completo. Faltam transições para o símbolo ‘d’ a partir de q_0 e para o símbolo ‘l’ a partir de q_1. Para completá-lo, adicionamos q_e:\n\ndelta(q_0,l)=q_1 (existente)\n\ndelta(q_0,d)=q_e (nova)\n\ndelta(q_1,d)=q_1 (existente)\n\ndelta(q_1,l)=q_e (nova)\n\ndelta(q_e,l)=q_e (nova)\n\ndelta(q_e,d)=q_e (nova)\n\n2. A propriedade determinística garante que para qualquer estado atual e qualquer símbolo de entrada, há apenas um único próximo estado possível. Isso elimina qualquer ambiguidade. Em uma implementação, isso se traduz em uma operação muito rápida e simples: uma única consulta a uma tabela (ou uma instrução switch) para determinar o próximo estado. Não há necessidade de explorar múltiplos caminhos, voltar atrás (backtracking) ou gerenciar escolhas, tornando o processamento da string de entrada linear e extremamente eficiente.\n3. Não. A função de um estado de erro é capturar permanentemente qualquer sequência de entrada que desvie de um padrão válido. Por definição, uma string que leva a um estado de erro deve ser rejeitada. Se o estado de erro fosse também um estado de aceitação, ele aceitaria strings que deveriam ser rejeitadas, contradizendo seu propósito.\n4. Assume-se que as transições não mostradas levam a um estado de erro implícito, não desenhado. A partir desse estado de erro, todas as transições subsequentes (para qualquer símbolo do alfabeto) apontam de volta para ele mesmo. Isso é feito para manter os diagramas mais limpos e legíveis.\n2.5. Sim, é muito comum. Isso é chamado de auto-loop ou laço. Significa que, ao ler um determinado símbolo, a máquina não precisa mudar a informação que está armazenando (representada pelo estado). Exemplo prático: no autômato que reconhece números com paridade par de ‘1’s, a transição delta(q_par,0)=q_par indica que ler um ’0’ não muda a paridade, então a máquina permanece no mesmo estado.\n\n\n12.1.3 Exercícios 3: {(exercicios-3?)}\n1. O diagrama de transições é o seguinte:\n\nTrês nós: q_A,q_B,q_C.\n\nq_A tem uma seta de entrada “do nada” (estado inicial).\n\nq_C é um círculo duplo (estado de aceitação).\n\nSetas:\n\nDe q_A para q_B com rótulo ‘0’.\n\nUm laço em q_A com rótulo ‘1’.\n\nUm laço em q_B com rótulo ‘0’.\n\nDe q_B para q_C com rótulo ‘1’.\n\nDe q_C para q_B com rótulo ‘0’.\n\nDe q_C para q_A com rótulo ‘1’.\n\n\n2.\n\nQ=q_pp,q_ip,q_pi,q_ii (par-par, ímpar-par, etc.)\n\nSigma=a,b\n\nq_0=q_pp\n\nF=q_pp\n\ndelta é definida por:\n\ndelta(q_pp,a)=q_ip, delta(q_pp,b)=q_pi\n\ndelta(q_ip,a)=q_pp, delta(q_ip,b)=q_ii\n\ndelta(q_pi,a)=q_ii, delta(q_pi,b)=q_pp\n\ndelta(q_ii,a)=q_pi, delta(q_ii,b)=q_ip\n\n\n3.\n\n\n\nEstado\na\nb\nr\ne\nx\n\n\n\n\nrightarrowq_0\nq_1\nq_e\nq_e\nq_e\nq_e\n\n\nq_1\nq_e\nq_2\nq_e\nq_e\nq_e\n\n\nq_2\nq_e\nq_e\nq_3\nq_e\nq_e\n\n\nq_3\nq_e\nq_e\nq_e\nq_4\nq_e\n\n\n\\*q_4\nq_e\nq_e\nq_e\nq_e\nq_e\n\n\nq_e\nq_e\nq_e\nq_e\nq_e\nq_e\n\n\n\n4.\n\nPara depurar: A representação gráfica (diagrama) é geralmente a mais útil, porque oferece uma visualização intuitiva do fluxo de controle. É fácil seguir os caminhos e entender como diferentes strings são processadas.\n\nPara implementar: A representação tabular (tabela de transições) é a mais direta para implementação. Ela mapeia naturalmente para uma estrutura de dados como um array 2D ou um dicionário de dicionários, permitindo acesso O(1) ao próximo estado.\n\n3.5. Para aceitar strings que terminam com “010”, precisamos de um novo estado final. Podemos adicionar um estado q_D e modificar as transições.\n\n\n\nEstado\n0\n1\n\n\n\n\nrightarrowq_A\nq_B\nq_A\n\n\nq_B\nq_B\nq_C\n\n\nq_C\n\\*q_D\nq_A\n\n\n\\*q_D\nq_B\nq_A\n\n\n\n\n\n12.1.4 Exercícios 4: {(exercicios-4?)}\n1.\n\nComeça em q_A.\n\nLê ‘0’: q_Arightarrowq_B. Estado atual: q_B.\n\nLê ‘1’: q_Brightarrowq_C. Estado atual: q_C.\n\nLê ‘1’: q_Crightarrowq_A. Estado atual: q_A.\n\nLê ‘0’: q_Arightarrowq_B. Estado atual: q_B.\n\nLê ‘1’: q_Brightarrowq_C. Estado atual: q_C.\nA computação termina em q_C, que é um estado de aceitação. Portanto, a string 01101 é aceita.\n\n2.\n\nComeça em q_0.\n\nLê ‘&lt;’: q_0rightarrowq_\\&lt;. Estado atual: q_\\&lt;.\n\nLê ‘=’: q_\\&lt;rightarrowq_leq. Estado atual: q_leq.\nA computação termina em q_leq, que é um estado de aceitação. A string &lt;= é aceita.\n\n3.\n\nPara !=: q_0xrightarrowq_xrightarrow=q_neq. Termina em q_neq (aceitação). Aceita.\n\nPara =!: q_0xrightarrow=q_=xrightarrowq_erro. Termina em q_erro (rejeição). Rejeitada.\n\n4. Não. A definição formal de aceitação exige que o estado final, após o processamento de toda a string, pertença ao conjunto F. Passar por um estado de aceitação no meio do caminho é irrelevante para a decisão final.\n5.\n\nAceita: ababa. Caminho: q_ppxrightarrowaq_ipxrightarrowbq_iixrightarrowaq_pixrightarrowbq_ppxrightarrowaq_ip. Ops, essa é rejeitada. Vamos tentar aabb.\n\nAceita: aabb. Caminho: q_ppxrightarrowaq_ipxrightarrowaq_ppxrightarrowbq_pixrightarrowbq_pp. Termina em q_ppinF. Aceita.\n\n\nRejeitada: ababa. Caminho: q_ppxrightarrowaq_ipxrightarrowbq_iixrightarrowaq_pixrightarrowbq_ppxrightarrowaq_ip. Termina em q_ipnotinF. Rejeitada.\n\n\n\n12.1.5 Exercícios 5: {(exercicios-5?)}\n1. Diagrama para “aba”:\n\nq_0xrightarrowbq_0\n\nq_0xrightarrowaq_1\n\nq_1xrightarrowaq_1\n\nq_1xrightarrowbq_2\n\nq_2xrightarrowbq_0\n\nq_2xrightarrowaq_3\n\nq_3 é estado de aceitação (círculo duplo) com laços para ‘a’ e ‘b’ (delta(q_3,a)=q_3,delta(q_3,b)=q_3).\n\n2. Diagrama para múltiplos de 3:\n\nEstados: q_0 (resto 0), q_1 (resto 1), q_2 (resto 2).\n\nq_0 é inicial e de aceitação.\n\nTransições:\n\ndelta(q_0,0)=q_0, delta(q_0,1)=q_1\n\ndelta(q_1,0)=q_2, delta(q_1,1)=q_0\n\ndelta(q_2,0)=q_1, delta(q_2,1)=q_2\n\n\n3. Tabela de transições para //:\n\n\n\nEstado\n/\nc\n\n\n\n\nrightarrowq_0\nq_1\nq_e\n\n\nq_1\n\\*q_2\nq_e\n\n\n\\*q_2\n\\*q_2\n\\*q_2\n\n\nq_e\nq_e\nq_e\n\n\n\n4. Diagrama para comprimento ímpar e termina com ‘a’:\n\nEstados: q_0 (par), q_1 (ímpar, termina ‘a’), q_2 (ímpar, termina ‘b’), q_e (erro).\n\nq_0 é inicial. q_1 é de aceitação.\n\ndelta(q_0,a)=q_1, delta(q_0,b)=q_2\n\ndelta(q_1,a)=q_0, delta(q_1,b)=q_0\n\ndelta(q_2,a)=q_0, delta(q_2,b)=q_0\n(Uma versão mais simples pode combinar os estados de paridade e o último caractere).\n\n5. Diagrama para cat ou car:\n\nq_0xrightarrowcq_1\n\nq_1xrightarrowaq_2\n\nq_2xrightarrowtq_3 (q_3 é de aceitação)\n\nq_2xrightarrowrq_4 (q_4 é de aceitação)\n\nTodas as outras transições levam a um estado de erro q_e.\n\n\n\n12.1.6 Exercícios 6: {(Exercicios-6?)}\n1. O autômato união é construído usando o produto cartesiano dos conjuntos de estados: \\[Q = Q_1 \\times Q_2\\]\nPortanto, o número de estados será: \\[|Q| = |Q_1| \\times |Q_2| = 3 \\times 2 = 6 \\text{ estados}\\]\nResposta: O autômato \\(M_1 \\cup M_2\\) terá 6 estados.\n2.\nMudando a condição de aceitação de OU para E, estaríamos realizando a interseção dos autômatos.\n\nUnião (\\(M_1 \\cup M_2\\)): Estado final se \\(p \\in F_1\\) OU \\(q \\in F_2\\)\n\nAceita strings que são aceitas por \\(M_1\\) ou por \\(M_2\\) (ou ambos)\n\nInterseção (\\(M_1 \\cap M_2\\)): Estado final se \\(p \\in F_1\\) E \\(q \\in F_2\\)\n\nAceita strings que são aceitas por \\(M_1\\) e por \\(M_2\\) simultaneamente\n\n\nResposta: Estaríamos realizando a operação de interseção dos autômatos.\n3.\nEspecificação dos autômatos originais:\n\\(M_1\\) aceita apenas “a”: - \\(Q_1 = \\{s_0, s_1, s_2\\}\\) (inicial, aceita “a”, rejeita) - \\(F_1 = \\{s_1\\}\\) - \\(\\delta_1(s_0, a) = s_1\\), \\(\\delta_1(s_0, b) = s_2\\) - \\(\\delta_1(s_1, a) = s_2\\), \\(\\delta_1(s_1, b) = s_2\\) - \\(\\delta_1(s_2, a) = s_2\\), \\(\\delta_1(s_2, b) = s_2\\)\n\\(M_2\\) aceita apenas “b”: - \\(Q_2 = \\{t_0, t_1, t_2\\}\\) (inicial, aceita “b”, rejeita) - \\(F_2 = \\{t_1\\}\\) - \\(\\delta_2(t_0, a) = t_2\\), \\(\\delta_2(t_0, b) = t_1\\) - \\(\\delta_2(t_1, a) = t_2\\), \\(\\delta_2(t_1, b) = t_2\\) - \\(\\delta_2(t_2, a) = t_2\\), \\(\\delta_2(t_2, b) = t_2\\)\nAutômato união \\(M_1 \\cup M_2\\): - Estados finais: \\((s_1, t_0)\\), \\((s_1, t_1)\\), \\((s_1, t_2)\\), \\((s_0, t_1)\\), \\((s_2, t_1)\\)\nTabela de transições:\n\n\n\nEstado\na\nb\n\n\n\n\n→\\((s_0,t_0)\\)\n\\((s_1,t_2)\\)\n\\((s_2,t_1)\\)\n\n\n*\\((s_0,t_1)\\)\n\\((s_1,t_2)\\)\n\\((s_2,t_2)\\)\n\n\n\\((s_0,t_2)\\)\n\\((s_1,t_2)\\)\n\\((s_2,t_2)\\)\n\n\n*\\((s_1,t_0)\\)\n\\((s_2,t_2)\\)\n\\((s_2,t_1)\\)\n\n\n*\\((s_1,t_1)\\)\n\\((s_2,t_2)\\)\n\\((s_2,t_2)\\)\n\n\n*\\((s_1,t_2)\\)\n\\((s_2,t_2)\\)\n\\((s_2,t_2)\\)\n\n\n\\((s_2,t_0)\\)\n\\((s_2,t_2)\\)\n\\((s_2,t_1)\\)\n\n\n*\\((s_2,t_1)\\)\n\\((s_2,t_2)\\)\n\\((s_2,t_2)\\)\n\n\n\\((s_2,t_2)\\)\n\\((s_2,t_2)\\)\n\\((s_2,t_2)\\)\n\n\n\n4.\nEspecificação dos autômatos:\n\\(M_1\\) (termina em ‘01’): - \\(Q_1 = \\{p_0, p_1, p_2\\}\\) onde \\(p_0\\) é inicial, \\(p_1\\) leu ‘0’, \\(p_2\\) leu ‘01’ - \\(F_1 = \\{p_2\\}\\) - Transições: \\(\\delta_1(p_0,0) = p_1\\), \\(\\delta_1(p_0,1) = p_0\\), \\(\\delta_1(p_1,0) = p_1\\), \\(\\delta_1(p_1,1) = p_2\\), \\(\\delta_1(p_2,0) = p_1\\), \\(\\delta_1(p_2,1) = p_0\\)\n\\(M_2\\) (número par de ’1’s): - \\(Q_2 = \\{q_0, q_1\\}\\) onde \\(q_0\\) é par, \\(q_1\\) é ímpar - \\(F_2 = \\{q_0\\}\\) - Transições: \\(\\delta_2(q_0,0) = q_0\\), \\(\\delta_2(q_0,1) = q_1\\), \\(\\delta_2(q_1,0) = q_1\\), \\(\\delta_2(q_1,1) = q_0\\)\nAutômato união \\(M_1 \\cup M_2\\):\nEstados finais: \\((p_2,q_0)\\), \\((p_2,q_1)\\), \\((p_0,q_0)\\), \\((p_1,q_0)\\)\nTabela de transições:\n\n\n\nEstado\n0\n1\n\n\n\n\n→\\((p_0,q_0)\\)\n\\((p_1,q_0)\\)\n\\((p_0,q_1)\\)\n\n\n\\((p_0,q_1)\\)\n\\((p_1,q_1)\\)\n\\((p_0,q_0)\\)\n\n\n*\\((p_1,q_0)\\)\n\\((p_1,q_0)\\)\n\\((p_2,q_1)\\)\n\n\n\\((p_1,q_1)\\)\n\\((p_1,q_1)\\)\n\\((p_2,q_0)\\)\n\n\n*\\((p_2,q_0)\\)\n\\((p_1,q_0)\\)\n\\((p_0,q_1)\\)\n\n\n*\\((p_2,q_1)\\)\n\\((p_1,q_1)\\)\n\\((p_0,q_0)\\)\n\n\n\nA linguagem aceita é \\(L(M_1 \\cup M_2)\\): strings que terminam em ‘01’ OU têm número par de ’1’s.\n5.\nPasso 1: Especificar os autômatos individuais\n\\(M_1\\) (começa com ‘1’): - \\(Q_1 = \\{r_0, r_1\\}\\) onde \\(r_0\\) é inicial, \\(r_1\\) aceita - \\(F_1 = \\{r_1\\}\\) - \\(\\delta_1(r_0,0) = r_0\\), \\(\\delta_1(r_0,1) = r_1\\), \\(\\delta_1(r_1,0) = r_1\\), \\(\\delta_1(r_1,1) = r_1\\)\n\\(M_2\\) (comprimento par): - \\(Q_2 = \\{s_0, s_1\\}\\) onde \\(s_0\\) é par, \\(s_1\\) é ímpar - \\(F_2 = \\{s_0\\}\\) - \\(\\delta_2(s_0,0) = s_1\\), \\(\\delta_2(s_0,1) = s_1\\), \\(\\delta_2(s_1,0) = s_0\\), \\(\\delta_2(s_1,1) = s_0\\)\nPasso 2: Criar conjunto de estados\n\\(Q = Q_1 \\times Q_2 = \\{(r_0,s_0), (r_0,s_1), (r_1,s_0), (r_1,s_1)\\}\\)\nTotal: \\(2 \\times 2 = 4\\) estados\nPasso 3: Determinar estado inicial\nEstado inicial: \\((r_0, s_0)\\)\nPasso 4: Determinar estados finais\nEstados finais onde \\(r_i \\in F_1\\) OU \\(s_j \\in F_2\\): - \\((r_0,s_0)\\): \\(r_0 \\notin F_1\\) mas \\(s_0 \\in F_2\\) → FINAL - \\((r_0,s_1)\\): \\(r_0 \\notin F_1\\) e \\(s_1 \\notin F_2\\) → não final - \\((r_1,s_0)\\): \\(r_1 \\in F_1\\) e \\(s_0 \\in F_2\\) → FINAL - \\((r_1,s_1)\\): \\(r_1 \\in F_1\\) mas \\(s_1 \\notin F_2\\) → FINAL\nEstados finais: \\(F = \\{(r_0,s_0), (r_1,s_0), (r_1,s_1)\\}\\)\nPasso 5: Construir função de transição\n\\(\\delta((r_i,s_j), x) = (\\delta_1(r_i,x), \\delta_2(s_j,x))\\)\nTransições com ‘0’: - \\(\\delta((r_0,s_0), 0) = (r_0,s_1)\\) - \\(\\delta((r_0,s_1), 0) = (r_0,s_0)\\) - \\(\\delta((r_1,s_0), 0) = (r_1,s_1)\\) - \\(\\delta((r_1,s_1), 0) = (r_1,s_0)\\)\nTransições com ‘1’: - \\(\\delta((r_0,s_0), 1) = (r_1,s_1)\\) - \\(\\delta((r_0,s_1), 1) = (r_1,s_0)\\) - \\(\\delta((r_1,s_0), 1) = (r_1,s_1)\\) - \\(\\delta((r_1,s_1), 1) = (r_1,s_0)\\)\nPasso 6: Tabela final\n\n\n\nEstado\n0\n1\n\n\n\n\n→*\\((r_0,s_0)\\)\n\\((r_0,s_1)\\)\n\\((r_1,s_1)\\)\n\n\n\\((r_0,s_1)\\)\n\\((r_0,s_0)\\)\n\\((r_1,s_0)\\)\n\n\n*\\((r_1,s_0)\\)\n\\((r_1,s_1)\\)\n\\((r_1,s_1)\\)\n\n\n*\\((r_1,s_1)\\)\n\\((r_1,s_0)\\)\n\\((r_1,s_0)\\)\n\n\n\nLinguagem aceita: \\(L(M_1 \\cup M_2)\\) são strings que começam com ‘1’ OU têm comprimento par.\nExemplos: \\(\\varepsilon\\), “00”, “1”, “10”, “11”, “0011”, “1010”\n\n\n12.1.7 Exercícios 7: {(Exercios-7?)}\n1. Especificação dos autômatos:\n\\(M_1\\) (termina em ‘10’): - \\(Q_1 = \\{p_0, p_1, p_2\\}\\) onde \\(p_0\\) é inicial, \\(p_1\\) leu ‘1’, \\(p_2\\) leu ‘10’ - \\(F_1 = \\{p_2\\}\\) - Transições: - \\(\\delta_1(p_0, 0) = p_0\\), \\(\\delta_1(p_0, 1) = p_1\\) - \\(\\delta_1(p_1, 0) = p_2\\), \\(\\delta_1(p_1, 1) = p_1\\) - \\(\\delta_1(p_2, 0) = p_0\\), \\(\\delta_1(p_2, 1) = p_1\\)\n\\(M_2\\) (número ímpar de ’0’s): - \\(Q_2 = \\{q_0, q_1\\}\\) onde \\(q_0\\) é par de ’0’s, \\(q_1\\) é ímpar de ’0’s - \\(F_2 = \\{q_1\\}\\) - Transições: - \\(\\delta_2(q_0, 0) = q_1\\), \\(\\delta_2(q_0, 1) = q_0\\) - \\(\\delta_2(q_1, 0) = q_0\\), \\(\\delta_2(q_1, 1) = q_1\\)\nAutômato interseção \\(M_1 \\cap M_2\\):\nEstados: \\(Q = Q_1 \\times Q_2 = \\{(p_0,q_0), (p_0,q_1), (p_1,q_0), (p_1,q_1), (p_2,q_0), (p_2,q_1)\\}\\)\nEstado inicial: \\((p_0, q_0)\\)\nConjunto de estados finais: Para a interseção, um estado \\((p_i, q_j)\\) é final se \\(p_i \\in F_1\\) E \\(q_j \\in F_2\\):\n\n\\((p_0,q_0)\\): \\(p_0 \\notin F_1\\) → não é final\n\\((p_0,q_1)\\): \\(p_0 \\notin F_1\\) → não é final\n\n\\((p_1,q_0)\\): \\(p_1 \\notin F_1\\) → não é final\n\\((p_1,q_1)\\): \\(p_1 \\notin F_1\\) → não é final\n\\((p_2,q_0)\\): \\(p_2 \\in F_1\\) mas \\(q_0 \\notin F_2\\) → não é final\n\\((p_2,q_1)\\): \\(p_2 \\in F_1\\) e \\(q_1 \\in F_2\\) → É FINAL\n\nResposta: \\(F = \\{(p_2, q_1)\\}\\)\nLinguagem aceita: \\(L(M_1 \\cap M_2)\\) são strings que terminam em ‘10’ E têm número ímpar de ’0’s.\nExemplos aceitos: “10”, “010”, “110”, “0010”, “1010”, “0110”\n2.\nSim, é possível que \\(L(M_1 \\cup M_2) = L(M_1 \\cap M_2)\\).\nCondição necessária e suficiente: Isso ocorre se e somente se \\(L(M_1) = L(M_2)\\).\nDemonstração:\nCaso 1: Se \\(L(M_1) = L(M_2)\\), então: - \\(L(M_1 \\cup M_2) = L(M_1) \\cup L(M_2) = L(M_1) \\cup L(M_1) = L(M_1)\\) - \\(L(M_1 \\cap M_2) = L(M_1) \\cap L(M_2) = L(M_1) \\cap L(M_1) = L(M_1)\\)\nPortanto, \\(L(M_1 \\cup M_2) = L(M_1 \\cap M_2)\\).\nCaso 2: Se \\(L(M_1) \\neq L(M_2)\\), então existe alguma string \\(w\\) tal que: - \\(w \\in L(M_1)\\) e \\(w \\notin L(M_2)\\), ou - \\(w \\notin L(M_1)\\) e \\(w \\in L(M_2)\\)\nSem perda de generalidade, suponha \\(w \\in L(M_1)\\) e \\(w \\notin L(M_2)\\).\nEntão: - \\(w \\in L(M_1 \\cup M_2)\\) (porque \\(w \\in L(M_1)\\)) - \\(w \\notin L(M_1 \\cap M_2)\\) (porque \\(w \\notin L(M_2)\\))\nLogo, \\(L(M_1 \\cup M_2) \\neq L(M_1 \\cap M_2)\\).\nExemplos práticos: - Se \\(M_1\\) e \\(M_2\\) ambos aceitam apenas strings que começam com ‘1’, então \\(L(M_1 \\cup M_2) = L(M_1 \\cap M_2)\\) - Se \\(M_1\\) aceita strings pares e \\(M_2\\) aceita strings ímpares, então \\(L(M_1 \\cup M_2) = \\Sigma^*\\) mas \\(L(M_1 \\cap M_2) = \\emptyset\\)\n3. Se \\(F_1 = Q_1\\), então \\(L(M_1 \\cap M_2) = L(M_2)\\).\nDemonstração:\nQuando \\(F_1 = Q_1\\), o autômato \\(M_1\\) aceita todas as strings sobre o alfabeto \\(\\Sigma\\), ou seja, \\(L(M_1) = \\Sigma^*\\).\nAnálise dos estados finais da interseção: No autômato \\(M_1 \\cap M_2\\), um estado \\((p, q)\\) é final se \\(p \\in F_1\\) E \\(q \\in F_2\\).\nComo \\(F_1 = Q_1\\), temos \\(p \\in F_1\\) para todo estado \\(p \\in Q_1\\).\nPortanto, \\((p, q)\\) é final se e somente se \\(q \\in F_2\\).\nComportamento da interseção: - O autômato \\(M_1 \\cap M_2\\) aceita uma string \\(w\\) se e somente se ela leva a um estado \\((p, q)\\) onde \\(q \\in F_2\\) - Isso é equivalente a dizer que \\(M_2\\) aceita \\(w\\), porque a componente \\(q\\) segue exatamente as transições de \\(M_2\\) - A componente \\(p\\) não influencia na aceitação, já que todos os estados de \\(M_1\\) são finais\nResultado: \\[L(M_1 \\cap M_2) = L(M_1) \\cap L(M_2) = \\Sigma^* \\cap L(M_2) = L(M_2)\\]\nInterpretação prática: Quando \\(M_1\\) aceita tudo (\\(L(M_1) = \\Sigma^*\\)), a interseção \\(M_1 \\cap M_2\\) se comporta exatamente como \\(M_2\\), porque a única restrição vem de \\(M_2\\).\nExemplo: - Se \\(M_1\\) tem \\(F_1 = Q_1\\) e \\(M_2\\) aceita strings com número par de ’1’s - Então \\(M_1 \\cap M_2\\) aceita exatamente as strings com número par de ’1’s - Ou seja, \\(L(M_1 \\cap M_2) = L(M_2)\\)\n4.\nEspecificação dos autômatos:\n\\(M_1\\) (termina em ‘ab’): - \\(Q_1 = \\{q_0, q_1, q_2\\}\\) onde \\(q_0\\) é inicial, \\(q_1\\) leu ‘a’, \\(q_2\\) leu ‘ab’ - \\(F_1 = \\{q_2\\}\\)\n\\(M_2\\) (número par de ’a’s): - \\(Q_2 = \\{r_0, r_1\\}\\) onde \\(r_0\\) é par de ’a’s, \\(r_1\\) é ímpar de ’a’s - \\(F_2 = \\{r_0\\}\\)\nAutômato interseção \\(M_1 \\cap M_2\\): - \\(Q = Q_1 \\times Q_2 = \\{(q_0,r_0), (q_0,r_1), (q_1,r_0), (q_1,r_1), (q_2,r_0), (q_2,r_1)\\}\\) - Estado inicial: \\((q_0, r_0)\\) - Estados finais: \\(F = \\{(q_2, r_0)\\}\\) (termina em ‘ab’ E número par de ’a’s)\nLinguagem aceita: \\(L(M_1 \\cap M_2)\\) são strings que terminam em ‘ab’ e têm número par de ’a’s.\nExemplos: “ab”, “aabb”, “baaabb”, “aaab”, “babab”\n5.\nPasso 1: Analisar os autômatos individuais\n\\(M_1\\): Começa com ‘a’ - \\(Q_1 = \\{s_0, s_1, s_2\\}\\) (inicial, aceita, rejeita) - \\(F_1 = \\{s_1\\}\\)\n\\(M_2\\): Comprimento múltiplo de 3 - \\(Q_2 = \\{t_0, t_1, t_2\\}\\) (resto 0, resto 1, resto 2) - \\(F_2 = \\{t_0\\}\\)\n\\(M_3\\): Contém ‘bb’ - \\(Q_3 = \\{u_0, u_1, u_2\\}\\) (inicial, leu ‘b’, encontrou ‘bb’) - \\(F_3 = \\{u_2\\}\\)\nPasso 2: Construir \\(M_1 \\cup M_2\\)\nEstados: \\(Q_{1 \\cup 2} = Q_1 \\times Q_2 = 3 \\times 3 = 9\\) estados\nEstados finais de \\(M_1 \\cup M_2\\): \\((s_i, t_j)\\) onde \\(s_i \\in F_1\\) OU \\(t_j \\in F_2\\) - Estados finais: \\(\\{(s_1, t_0), (s_1, t_1), (s_1, t_2), (s_0, t_0), (s_2, t_0)\\}\\)\nPasso 3: Construir \\((M_1 \\cup M_2) \\cap M_3\\)\nEstados: \\(Q = Q_{1 \\cup 2} \\times Q_3 = 9 \\times 3 = 27\\) estados\nCada estado é da forma \\(((s_i, t_j), u_k)\\)\nPasso 4: Determinar estados finais de \\((M_1 \\cup M_2) \\cap M_3\\)\nUm estado \\(((s_i, t_j), u_k)\\) é final se: - \\((s_i, t_j) \\in F_{1 \\cup 2}\\) E \\(u_k \\in F_3\\) - Ou seja: \\((s_i \\in F_1\\) OU \\(t_j \\in F_2)\\) E \\(u_k = u_2\\)\nEstados finais: - \\(((s_1, t_0), u_2)\\), \\(((s_1, t_1), u_2)\\), \\(((s_1, t_2), u_2)\\), \\(((s_0, t_0), u_2)\\), \\(((s_2, t_0), u_2)\\)\nPasso 5: Função de transição\nPara cada estado \\(((s_i, t_j), u_k)\\) e símbolo \\(x\\): \\[\\delta(((s_i, t_j), u_k), x) = ((\\delta_1(s_i, x), \\delta_2(t_j, x)), \\delta_3(u_k, x))\\]\nResposta final: - Número de estados: 27 estados - Linguagem aceita: Strings que (começam com ‘a’ OU têm comprimento múltiplo de 3) E contêm ‘bb’ - Exemplos: “abb”, “abba”, “aabbb”, “bbaaa”, “bbbbbb”\nO autômato resultante aceita strings que satisfazem simultaneamente a condição de união (começa com ‘a’ ou comprimento múltiplo de 3) e a condição de \\(M_3\\) (contém ‘bb’).\n\n\n12.1.8 Exercícios 8: {(Exercicios-8?)}\n1.\nAutômato original \\(M\\):\n\n\n\nEstado\na\nb\nFinal?\n\n\n\n\n→\\(q_0\\)\n\\(q_1\\)\n\\(q_0\\)\nSim\n\n\n\\(q_1\\)\n\\(q_0\\)\n\\(q_1\\)\nNão\n\n\n\nConstrução do complemento \\(\\overline{M}\\):\nPara construir \\(\\overline{M}\\), mantemos a mesma estrutura de estados e transições, mas invertemos o conjunto de estados finais: - Estados que eram finais tornam-se não finais - Estados que eram não finais tornam-se finais\nAutômato complemento \\(\\overline{M}\\): - \\(Q_{\\overline{M}} = Q = \\{q_0, q_1\\}\\) - \\(\\delta_{\\overline{M}} = \\delta\\) (mesmas transições) - \\(F_{\\overline{M}} = Q \\setminus F = \\{q_1\\}\\)\nTabela de transições de \\(\\overline{M}\\):\n\n\n\nEstado\na\nb\nFinal?\n\n\n\n\n→\\(q_0\\)\n\\(q_1\\)\n\\(q_0\\)\nNão\n\n\n\\(q_1\\)\n\\(q_0\\)\n\\(q_1\\)\nSim\n\n\n\nVerificação: - \\(M\\) aceita strings com número par de ’a’s - \\(\\overline{M}\\) aceita strings com número ímpar de ’a’s - \\(L(M) \\cup L(\\overline{M}) = \\Sigma^*\\) e \\(L(M) \\cap L(\\overline{M}) = \\emptyset\\)\n2.\nPor que a completude é essencial:\nA completude garante que o autômato tenha uma transição definida para todo par (estado, símbolo). Sem completude, strings que levam a transições indefinidas teriam comportamento ambíguo no complemento.\nProblema sem completude: - Em um autômato incompleto, algumas strings podem “travar” (não ter transição definida) - Strings que travam são implicitamente rejeitadas no autômato original - No complemento, seria necessário decidir se essas strings devem ser aceitas ou rejeitadas - Sem completude, essa decisão não fica clara, tornando o complemento mal definido\nExemplo prático:\nConsidere um autômato incompleto \\(M'\\) sobre \\(\\Sigma = \\{a, b\\}\\): - \\(Q = \\{q_0, q_1\\}\\), estado inicial \\(q_0\\), \\(F = \\{q_1\\}\\) - \\(\\delta(q_0, a) = q_1\\) (transição para ‘a’) - $(q_0, b) = $ indefinido (sem transição para ‘b’) - $(q_1, a) = $ indefinido - $(q_1, b) = $ indefinido\nProblema na construção do complemento: 1. String “a”: aceita por \\(M'\\) → deve ser rejeitada por \\(\\overline{M'}\\) 2. String “b”: trava em \\(M'\\) (rejeitada) → deveria ser aceita por \\(\\overline{M'}\\)? 3. String “aa”: trava em \\(M'\\) após aceitar “a” → comportamento indefinido\nSolução: Completar primeiro o autômato adicionando um estado “lixeira”: - Adicionar estado \\(q_{trap}\\) não final - \\(\\delta(q_0, b) = q_{trap}\\) - \\(\\delta(q_1, a) = q_{trap}\\), \\(\\delta(q_1, b) = q_{trap}\\) - \\(\\delta(q_{trap}, a) = q_{trap}\\), \\(\\delta(q_{trap}, b) = q_{trap}\\)\nAgora o complemento fica bem definido com \\(F_{\\overline{M}} = \\{q_0, q_{trap}\\}\\).\n3.\nTeorema: Se \\(L_1\\) e \\(L_2\\) são linguagens regulares, então \\(L_1 - L_2\\) (diferença de conjuntos) também é regular.\nDemonstração:\nPasso 1: Expressar diferença usando operações básicas \\[L_1 - L_2 = L_1 \\cap \\overline{L_2}\\]\nEsta igualdade é válida porque: - \\(w \\in L_1 - L_2 \\iff w \\in L_1 \\text{ e } w \\notin L_2\\) - \\(w \\in L_1 \\cap \\overline{L_2} \\iff w \\in L_1 \\text{ e } w \\in \\overline{L_2} \\iff w \\in L_1 \\text{ e } w \\notin L_2\\)\nPasso 2: Aplicar propriedades de fechamento\nPropriedade 1: Linguagens regulares são fechadas sob complemento - Se \\(L_2\\) é regular, então \\(\\overline{L_2}\\) também é regular\nPropriedade 2: Linguagens regulares são fechadas sob interseção - Se \\(L_1\\) e \\(\\overline{L_2}\\) são regulares, então \\(L_1 \\cap \\overline{L_2}\\) também é regular\nPasso 3: Conclusão Como: 1. \\(L_1\\) é regular (dado) 2. \\(L_2\\) é regular (dado) 3. \\(\\overline{L_2}\\) é regular (por fechamento sob complemento) 4. \\(L_1 \\cap \\overline{L_2}\\) é regular (por fechamento sob interseção) 5. \\(L_1 - L_2 = L_1 \\cap \\overline{L_2}\\)\nConcluímos que \\(L_1 - L_2\\) é regular. □\nAlgoritmo construtivo: 1. Construir autômato \\(M_2\\) para \\(L_2\\) 2. Construir autômato \\(\\overline{M_2}\\) para \\(\\overline{L_2}\\) (invertendo estados finais) 3. Construir autômato \\(M_1 \\cap \\overline{M_2}\\) usando produto cartesiano 4. O resultado reconhece \\(L_1 - L_2\\)\n4.\nConclusão: Se \\(L\\) é regular e \\(\\overline{L} = \\emptyset\\), então \\(L = \\Sigma^*\\).\nDemonstração:\nPropriedade fundamental: Para qualquer linguagem \\(L\\) sobre alfabeto \\(\\Sigma\\): \\[L \\cup \\overline{L} = \\Sigma^*\\]\nEsta propriedade vale porque toda string pertence a \\(L\\) ou ao seu complemento (mas não a ambos).\nAplicando a hipótese: Se \\(\\overline{L} = \\emptyset\\), então: \\[L \\cup \\overline{L} = L \\cup \\emptyset = L\\]\nCombinando com a propriedade fundamental: \\[L = L \\cup \\overline{L} = \\Sigma^*\\]\nVerificação por contradição: Suponha que existe \\(w \\in \\Sigma^*\\) tal que \\(w \\notin L\\).\nEntão, por definição de complemento, \\(w \\in \\overline{L}\\).\nMas isso contradiz a hipótese \\(\\overline{L} = \\emptyset\\).\nLogo, não existe tal \\(w\\), e portanto \\(L = \\Sigma^*\\).\nInterpretação prática: - Se o complemento de uma linguagem é vazio, significa que não existe nenhuma string que a linguagem não aceite - Portanto, a linguagem aceita todas as strings possíveis - Ou seja, \\(L\\) é a linguagem universal sobre o alfabeto\nExemplo: - Se \\(L = \\{w \\in \\{a,b\\}^* : w \\text{ contém pelo menos 0 símbolos}\\}\\) - Então \\(L = \\{a,b\\}^*\\) e \\(\\overline{L} = \\emptyset\\)\n5.\nLei de De Morgan generalizada: \\[\\overline{L_1 \\cap L_2 \\cap L_3} = \\overline{L_1} \\cup \\overline{L_2} \\cup \\overline{L_3}\\]\nDemonstração passo a passo:\nMétodo 1: Aplicação direta da lei de De Morgan\nPara três conjuntos, a lei de De Morgan estabelece: \\[\\overline{A \\cap B \\cap C} = \\overline{A} \\cup \\overline{B} \\cup \\overline{C}\\]\nAplicando diretamente com \\(A = L_1\\), \\(B = L_2\\), \\(C = L_3\\): \\[\\overline{L_1 \\cap L_2 \\cap L_3} = \\overline{L_1} \\cup \\overline{L_2} \\cup \\overline{L_3}\\]\nMétodo 2: Aplicação iterativa\nPasso 1: Tratar \\((L_1 \\cap L_2 \\cap L_3)\\) como \\((L_1 \\cap L_2) \\cap L_3\\) \\[\\overline{(L_1 \\cap L_2) \\cap L_3} = \\overline{L_1 \\cap L_2} \\cup \\overline{L_3}\\]\nPasso 2: Aplicar De Morgan novamente em \\(\\overline{L_1 \\cap L_2}\\) \\[\\overline{L_1 \\cap L_2} = \\overline{L_1} \\cup \\overline{L_2}\\]\nPasso 3: Substituir de volta \\[\\overline{L_1 \\cap L_2 \\cap L_3} = (\\overline{L_1} \\cup \\overline{L_2}) \\cup \\overline{L_3} = \\overline{L_1} \\cup \\overline{L_2} \\cup \\overline{L_3}\\]\nVerificação semântica:\nUma string \\(w\\) pertence a \\(\\overline{L_1 \\cap L_2 \\cap L_3}\\) se e somente se: \\[w \\notin (L_1 \\cap L_2 \\cap L_3)\\]\nIsso significa que \\(w\\) não pertence simultaneamente a \\(L_1\\), \\(L_2\\) e \\(L_3\\).\nOu seja, \\(w\\) falha em pelo menos uma das três linguagens: \\[w \\notin L_1 \\text{ ou } w \\notin L_2 \\text{ ou } w \\notin L_3\\]\nQue é equivalente a: \\[w \\in \\overline{L_1} \\text{ ou } w \\in \\overline{L_2} \\text{ ou } w \\in \\overline{L_3}\\]\nPortanto: \\[w \\in \\overline{L_1} \\cup \\overline{L_2} \\cup \\overline{L_3}\\]\nResposta final: \\[\\overline{L_1 \\cap L_2 \\cap L_3} = \\overline{L_1} \\cup \\overline{L_2} \\cup \\overline{L_3}\\]\n\n\n12.1.9 Exercícios 10 {(Exercicios-10?)}\n1.\na) Classes de equivalência de \\(\\equiv_L\\):\nPara \\(L = \\{w \\mid w \\text{ contém } 00\\}\\), temos três classes de equivalência:\n\nClasse \\(C_0\\): Strings que não terminam em \\(0\\) e não contêm \\(00\\)\n\nRepresentante: \\(\\epsilon\\)\nOutros membros: \\(1\\), \\(11\\), \\(111\\), \\(101\\), \\(1101\\), etc.\n\nClasse \\(C_1\\): Strings que terminam em exatamente um \\(0\\) e não contêm \\(00\\)\n\nRepresentante: \\(0\\)\nOutros membros: \\(10\\), \\(110\\), \\(1110\\), \\(1010\\), etc.\n\nClasse \\(C_2\\): Strings que contêm \\(00\\)\n\nRepresentante: \\(00\\)\nOutros membros: \\(000\\), \\(100\\), \\(001\\), \\(0011\\), qualquer string com \\(00\\)\n\n\nb) Justificativa da equivalência:\n\n\\(C_0\\): Para strings em \\(C_0\\), adicionar \\(0\\) as move para \\(C_1\\), adicionar \\(00\\) as leva para \\(L\\)\n\\(C_1\\): Para strings em \\(C_1\\), adicionar \\(0\\) as leva direto para \\(L\\), adicionar \\(1\\) as retorna para \\(C_0\\)\n\\(C_2\\): Para strings em \\(C_2\\), qualquer sufixo mantém a string em \\(L\\) (porque já contém \\(00\\))\n\nc) Autômato Finito Determinístico mínimo:\nConjunto de estados: \\(Q = \\{q_0, q_1, q_2\\}\\) Alfabeto: \\(\\Sigma = \\{0, 1\\}\\) Estado inicial: \\(q_0\\) Conjunto de estados finais: \\(F = \\{q_2\\}\\) Função de transição: \\(\\delta: Q \\times \\Sigma \\to Q\\) é definida por:\n$$\n\\begin{aligned}\n\\delta(q_0, 0) &= q_1 \\\\\n\\delta(q_0, 1) &= q_0 \\\\\n\\delta(q_1, 0) &= q_2 \\\\\n\\delta(q_1, 1) &= q_0 \\\\\n\\delta(q_2, 0) &= q_2 \\\\\n\\delta(q_2, 1) &= q_2\n\\end{aligned}\n$$\nd) Verificação: O Autômato Finito Determinístico mínimo tem 3 estados, correspondendo exatamente às 3 classes de equivalência de Myhill-Nerode.\n2.\nProva de que \\(L = \\{0^i1^j \\mid i &gt; j \\geq 0\\}\\) não é regular:\nConsidere as strings \\(s_n = 0^n\\) para \\(n = 1, 2, 3, ...\\)\nAfirmação: Para \\(m \\neq n\\), temos \\(s_m \\not\\equiv_L s_n\\).\nProva: Considere o sufixo \\(z = 1^m\\). Então: - \\(s_m \\cdot z = 0^m1^m \\notin L\\) (porque \\(m = m\\), não \\(m &gt; m\\)) - \\(s_n \\cdot z = 0^n1^m\\): - Se \\(n &gt; m\\), então \\(0^n1^m \\in L\\) - Se \\(n &lt; m\\), então \\(0^n1^m \\notin L\\)\nComo \\(n \\neq m\\), pelo menos um dos casos acima distingue \\(s_m\\) de \\(s_n\\).\nPortanto, existem infinitas classes de equivalência (uma para cada \\(n \\in \\mathbb{N}\\)), logo \\(L\\) não é regular pelo Teorema de Myhill-Nerode.\n3.\nEliminação de estados para encontrar ER:\nEstado inicial: Autômato Finito Determinístico com 3 estados\nPasso 1: Eliminar \\(q_1\\) - Aresta \\(q_0 \\to q_2\\): \\(ab\\) (via \\(q_1\\)) - Aresta \\(q_0 \\to q_0\\): \\(b\\) (direto) - Nova transição: \\(q_0 \\xrightarrow{ab} q_2\\)\nPasso 2: Eliminar \\(q_2\\) (estado final) - De \\(q_0\\) para \\(q_2\\): \\(ab(a|b)^*\\) - Loop em \\(q_0\\): \\(b^*\\)\nExpressão regular final: \\[r = b^*ab(a|b)^*\\]\nEsta ER descreve strings que: 1. Começam com zero ou mais \\(b\\)’s 2. Seguidos por \\(ab\\) 3. Terminam com qualquer sequência de \\(a\\)’s e \\(b\\)’s\n4.\na) Teste de equivalência via produto:\nConstruindo \\((L(M_1) - L(M_2)) \\cup (L(M_2) - L(M_1))\\):\nAnálise das linguagens: - \\(L(M_1)\\): strings com número par de \\(a\\)’s - \\(L(M_2)\\): NÃO é equivalente (aceita baseado em posição)\nb) Teste com strings: - \\(\\epsilon\\): \\(M_1\\) aceita (0 \\(a\\)’s = par), \\(M_2\\) aceita → ambos aceitam ✓ - \\(a\\): \\(M_1\\) rejeita (1 \\(a\\) = ímpar), \\(M_2\\) rejeita (posição 1 é ímpar) → ambos rejeitam ✓ - \\(b\\): \\(M_1\\) aceita (0 \\(a\\)’s), \\(M_2\\) aceita → ambos aceitam ✓ - \\(aa\\): \\(M_1\\) aceita (2 \\(a\\)’s = par), \\(M_2\\) rejeita → DIFEREM ✗ - \\(ba\\): \\(M_1\\) rejeita (1 \\(a\\)), \\(M_2\\) aceita (posição 2 é par) → DIFEREM ✗\nConclusão: \\(L(M_1) \\neq L(M_2)\\)\n5.\na) Minimização de Hopcroft:\nAutômato Finito Determinístico original para strings terminadas em \\(01\\):\nEstados iniciais: \\(\\{q_0, q_1, ..., q_7\\}\\)\nParticionamento: 1. Separar finais e não-finais 2. Refinar baseado em transições 3. Resultado: Autômato Finito Determinístico mínimo com 3 estados\nAutômato Finito Determinístico mínimo: - \\(s_0\\): não leu nada relevante - \\(s_1\\): último símbolo foi \\(0\\) - \\(s_2\\): últimos símbolos foram \\(01\\) (final)\nb) Complexidade: - Hopcroft: \\(O(k \\cdot n \\log n) = O(2 \\cdot 8 \\log 8) = O(48)\\) operações\nc) Comparação para Autômato Finito Determinísticos com 8 e 10 estados: - Via minimização: \\(O(8 \\log 8) + O(10 \\log 10) + O(1)\\) ≈ \\(O(57)\\) - Via produto: \\(O(8 \\times 10) = O(80)\\)\nPara este caso, minimização é mais eficiente.\n6.\na) Autômato Finito Determinísticos individuais:\nEste autômato reconhece sequências não vazias de dígitos.\nRepresentação Algébrica:\n\\(A_{int} = (Q_{int}, \\Sigma_{int}, \\delta_{int}, q_{0,int}, F_{int})\\), onde:\n\nConjunto de estados: \\(Q_{int} = \\{q_0, q_1, q_d\\}\\)\nAlfabeto (classes): \\(\\Sigma_{int} = \\{\\text{dígito}, \\text{outro}\\}\\), onde dígito é qualquer caractere de ‘0’ a ‘9’ e outro é qualquer caractere que não seja um dígito.\nEstado inicial: \\(q_{0,int} = q_0\\)\nConjunto de estados finais: \\(F_{int} = \\{q_1\\}\\)\nFunção de transição \\(\\delta_{int}\\): \\[\n  \\begin{aligned}\n  \\delta(q_0, \\text{dígito}) &= q_1 \\\\\n  \\delta(q_0, \\text{outro}) &= q_d \\\\\n  \\delta(q_1, \\text{dígito}) &= q_1 \\\\\n  \\delta(q_1, \\text{outro}) &= q_d \\\\\n  \\delta(q_d, \\text{dígito}) &= q_d \\\\\n  \\delta(q_d, \\text{outro}) &= q_d\n  \\end{aligned}\n  \\]\n\nTabela de Transição:\n\n\n\nEstado\ndígito (0-9)\noutro\n\n\n\n\n\\(\\to q_0\\)\n\\(q_1\\)\n\\(q_d\\)\n\n\n\\(*q_1\\)\n\\(q_1\\)\n\\(q_d\\)\n\n\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\n\n\nEste autômato reconhece tokens que começam com uma letra e são seguidos por zero ou mais letras ou dígitos.\nRepresentação Algébrica:\n\\(A_{id} = (Q_{id}, \\Sigma_{id}, \\delta_{id}, q_{0,id}, F_{id})\\), onde:\n\nConjunto de estados: \\(Q_{id} = \\{q_0, q_1, q_d\\}\\)\nAlfabeto (classes): \\(\\Sigma_{id} = \\{\\text{letra}, \\text{dígito}, \\text{outro}\\}\\), onde letra é ‘a’-‘z’ ou ‘A’-‘Z’, dígito é ‘0’-‘9’, e outro é qualquer caractere que não seja letra nem dígito.\nEstado inicial: \\(q_{0,id} = q_0\\)\nConjunto de estados finais: \\(F_{id} = \\{q_1\\}\\)\nFunção de transição \\(\\delta_{id}\\): \\[\n  \\begin{aligned}\n  \\delta(q_0, \\text{letra}) &= q_1 \\\\\n  \\delta(q_0, \\text{dígito}) &= q_d \\\\\n  \\delta(q_0, \\text{outro}) &= q_d \\\\\n  \\delta(q_1, \\text{letra}) &= q_1 \\\\\n  \\delta(q_1, \\text{dígito}) &= q_1 \\\\\n  \\delta(q_1, \\text{outro}) &= q_d \\\\\n  \\delta(q_d, \\text{letra}) &= q_d \\\\\n  \\delta(q_d, \\text{dígito}) &= q_d \\\\\n  \\delta(q_d, \\text{outro}) &= q_d\n  \\end{aligned}\n  \\]\n\nTabela de Transição:\n\n\n\nEstado\nletra (a-z, A-Z)\ndígito (0-9)\noutro\n\n\n\n\n\\(\\to q_0\\)\n\\(q_1\\)\n\\(q_d\\)\n\\(q_d\\)\n\n\n\\(*q_1\\)\n\\(q_1\\)\n\\(q_1\\)\n\\(q_d\\)\n\n\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\n\n\nEste autômato reconhece os quatro operadores: ++, +=, --, -=.\nRepresentação Algébrica:\n\\(A_{op} = (Q_{op}, \\Sigma_{op}, \\delta_{op}, q_{0,op}, F_{op})\\), onde:\n\nConjunto de estados: \\(Q_{op} = \\{q_0, q_1, q_2, q_3, q_4, q_5, q_6, q_d\\}\\)\nAlfabeto (símbolos): \\(\\Sigma_{op} = \\{+, -, =, \\text{outro}\\}\\)\nEstado inicial: \\(q_{0,op} = q_0\\)\nConjunto de estados finais: \\(F_{op} = \\{q_2, q_3, q_5, q_6\\}\\)\nFunção de transição \\(\\delta_{op}\\): \\[\n  \\begin{aligned}\n  \\delta(q_0, +) &= q_1 \\quad & \\delta(q_0, -) &= q_4 \\quad & \\delta(q_0, =) &= q_d \\quad & \\delta(q_0, \\text{outro}) &= q_d \\\\\n  \\delta(q_1, +) &= q_2 \\quad & \\delta(q_1, =) &= q_3 \\quad & \\delta(q_1, -) &= q_d \\quad & \\delta(q_1, \\text{outro}) &= q_d \\\\\n  \\delta(q_4, -) &= q_5 \\quad & \\delta(q_4, =) &= q_6 \\quad & \\delta(q_4, +) &= q_d \\quad & \\delta(q_4, \\text{outro}) &= q_d \\\\\n  \\end{aligned}\n  \\]\n\nPara todos os outros estados \\(q \\in \\{q_2, q_3, q_5, q_6, q_d\\}\\) e para qualquer símbolo \\(s \\in \\Sigma_{op}\\), a transição é \\(\\delta(q, s) = q_d\\).\nTabela de Transição:\n\n\n\nEstado\n+\n-\n=\noutro\n\n\n\n\n\\(\\to q_0\\)\n\\(q_1\\)\n\\(q_4\\)\n\\(q_d\\)\n\\(q_d\\)\n\n\n\\(q_1\\)\n\\(q_2\\)\n\\(q_d\\)\n\\(q_3\\)\n\\(q_d\\)\n\n\n\\(*q_2\\) (++)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\n\n\\(*q_3\\) (+=)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\n\n\\(q_4\\)\n\\(q_d\\)\n\\(q_5\\)\n\\(q_6\\)\n\\(q_d\\)\n\n\n\\(*q_5\\) (--)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\n\n\\(*q_6\\) (-=)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\n\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\\(q_d\\)\n\n\n\nb. Construção do Autômato Finito Determinístico Combinado\nPara criar um único Autômato Finito Determinístico que reconheça a união das linguagens dos três autômatos individuais (\\(A_{int}\\), \\(A_{id}\\), \\(A_{op}\\)), utilizamos o método da construção do produto. Este método gera um novo Autômato Finito Determinístico cujos estados representam a simulação simultânea dos três autômatos originais.\nO novo autômato, \\(A_{comb}\\), é definido pela 5-tupla \\(A_{comb} = (Q_{comb}, \\Sigma_{comb}, \\delta_{comb}, q_{0,comb}, F_{comb})\\).\n1. Alfabeto (\\(\\Sigma_{comb}\\)): O alfabeto combinado é a união de todos os caracteres e classes de caracteres dos autômatos individuais. \\(\\Sigma_{comb} = \\{\\text{letra}, \\text{dígito}, +, -, =, \\text{outro}\\}\\)\n2. Conjunto de Estados (\\(Q_{comb}\\)): O novo conjunto de estados é o produto cartesiano dos conjuntos de estados dos autômatos originais. Cada estado em \\(A_{comb}\\) é uma tupla que rastreia o estado atual de cada autômato individual.\n\\(Q_{comb} = Q_{int} \\times Q_{id} \\times Q_{op}\\)\nO número total de estados seria \\(|Q_{int}| \\times |Q_{id}| \\times |Q_{op}| = 3 \\times 3 \\times 8 = 72\\) estados. Devido ao grande número, a construção explícita da tabela de transição é impraticável, mas o princípio é o seguinte:\n3. Estado Inicial (\\(q_{0,comb}\\)): O estado inicial do autômato combinado é a tupla contendo os estados iniciais de cada autômato individual.\n\\(q_{0,comb} = (q_{0,int}, q_{0,id}, q_{0,op})\\)\n4. Função de Transição (\\(\\delta_{comb}\\)): A função de transição é aplicada componente a componente. Para um estado tupla \\((p, q, r) \\in Q_{comb}\\) e um símbolo de entrada \\(s \\in \\Sigma_{comb}\\), a transição é definida como:\n\\(\\delta_{comb}((p, q, r), s) = (\\delta_{int}(p, s), \\delta_{id}(q, s), \\delta_{op}(r, s))\\)\nExemplos de Transição: vamos calcular a transição a partir do estado inicial para dois símbolos diferentes:\n\nEntrada ‘7’ (um dígito):\n\nEstado atual: \\((q_{0,int}, q_{0,id}, q_{0,op})\\)\n\\(\\delta_{int}(q_{0,int}, \\text{dígito}) = q_{1,int}\\)\n\\(\\delta_{id}(q_{0,id}, \\text{dígito}) = q_{d,id}\\)\n\\(\\delta_{op}(q_{0,op}, \\text{dígito}) = q_{d,op}\\)\nNovo estado: \\((q_{1,int}, q_{d,id}, q_{d,op})\\)\n\nEntrada ‘+’ (o símbolo +):\n\nEstado atual: \\((q_{0,int}, q_{0,id}, q_{0,op})\\)\n\\(\\delta_{int}(q_{0,int}, +) = q_{d,int}\\)\n\\(\\delta_{id}(q_{0,id}, +) = q_{d,id}\\)\n\\(\\delta_{op}(q_{0,op}, +) = q_{1,op}\\)\nNovo estado: \\((q_{d,int}, q_{d,id}, q_{1,op})\\)\n\n\n5. Conjunto de Estados Finais (\\(F_{comb}\\)): Um estado no autômato combinado é final se pelo menos um de seus componentes for um estado final no seu respectivo autômato original. Isso corresponde à operação de união.\n\\(F_{comb} = \\{ (p, q, r) \\in Q_{comb} \\mid p \\in F_{int} \\lor q \\in F_{id} \\lor r \\in F_{op} \\}\\)\nPor exemplo, o estado \\((q_{1,int}, q_{d,id}, q_{d,op})\\) seria um estado final em \\(A_{comb}\\) porque \\(q_{1,int}\\) é um estado final em \\(A_{int}\\). Da mesma forma, um estado como \\((q_{d,int}, q_{d,id}, q_{2,op})\\) também seria final, porque \\(q_{2,op}\\) pertence a \\(F_{op}\\).\nc. Distinção entre os Tipos de tokens\nO autômato combinado \\(A_{comb}\\) pode reconhecer qualquer um dos tokens, mas, por si só, não informa qual tipo de token foi encontrado. A implementação de um analisador léxico real precisa dessa distinção. A chave está em analisar o estado final alcançado.\nComo os três autômatos originais reconhecem linguagens disjuntas (um identificador não pode ser um inteiro nem um operador, e vice-versa), nunca haverá ambiguidade. Ao atingir um estado final, apenas um dos componentes da tupla de estado será um estado final de seu autômato original.\nImplementação Prática: A estratégia para distinguir os tokens no analisador léxico seria:\n\nExecutar o Autômato Finito Determinístico: processe a cadeia de entrada usando o \\(A_{comb}\\) até que o final da cadeia seja alcançado ou a transição leve a um estado de erro global (ex: uma tupla onde todos os componentes são estados de erro, como \\((q_{d,int}, q_{d,id}, q_{d,op})\\)).\nVerificar o Estado Final: ao final de um lexema potencial, verifique se o estado atual \\((p, q, r)\\) é um estado final em \\(F_{comb}\\).\nIdentificar o Tipo de Token: se o estado for final, determine o tipo de token inspecionando qual componente da tupla pertence a um conjunto final original:\n\nSe \\(p \\in F_{int}\\) (ex: \\(p=q_{1,int}\\)), o token é um INTEIRO.\nSe \\(q \\in F_{id}\\) (ex: \\(q=q_{1,id}\\)), o token é um IDENTIFICADOR.\nSe \\(r \\in F_{op}\\) (ex: \\(r \\in \\{q_{2,op}, q_{3,op}, q_{5,op}, q_{6,op}\\}\\)), o token é um OPERADOR.\n\nMapear para um Tipo de Token: no código, isso se traduz em uma estrutura de decisão. Em vez de ter um único tipo de estado final, cada estado final no modelo teórico é associado a uma ação ou tipo de token no código prático. A sua sugestão de switch/case é uma excelente forma de representar isso.\n\nPara fins de implementação, não se criam as 72 tuplas explicitamente. Em vez disso, mantém-se três variáveis de estado (uma para cada autômato) e as atualiza em paralelo. O tipo do token é determinado pela primeira máquina que atingir um estado final.\n```c // Pseudocódigo da lógica do analisador enum TokenType { TOKEN_UNKNOWN, TOKEN_INTEGER, TOKEN_IDENTIFIER, TOKEN_OPERATOR_PLUSPLUS, … };\nTokenType recognize_token(string input) {\n    // Simulação do estado combinado\n    State state_int = q0_int;\n    State state_id  = q0_id;\n    State state_op  = q0_op;\n\n    for (char c : input) {\n        state_int = delta_int(state_int, c);\n        state_id  = delta_id(state_id, c);\n        state_op  = delta_op(state_op, c);\n    }\n\n    // Mapeia o estado final para o tipo de token\n    if (is_final_int(state_int)) {\n        return TOKEN_INTEGER;\n    }\n    if (is_final_id(state_id)) {\n        // Em um sistema real, aqui haveria uma verificação de palavras-chave\n        return TOKEN_IDENTIFIER;\n    }\n    if (is_final_op(state_op)) {\n        // O estado específico (q2, q3, q5, q6) diria qual operador é\n        return map_final_op_state_to_token(state_op); // ex: q2 -&gt; TOKEN_OPERATOR_PLUSPLUS\n    }\n\n    return TOKEN_UNKNOWN; // ou lança um erro léxico\n}\n```\nPrecedência: Embora não haja ambiguidade neste problema, em sistemas mais complexos (ex: palavras-chave vs. identificadores), a regra da maior precedência ou do maior casamento (longest match) é aplicada. Se if pudesse ser um identificador e uma palavra-chave, a tabela de símbolos ou a lógica do analisador priorizaria o TOKEN_KEYWORD_IF sobre o TOKEN_IDENTIFIER.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Solução dos Exercícios</span>"
    ]
  },
  {
    "objectID": "sol-exercicios.html#capítulo-sec-lingagens-livres-de-contexto",
    "href": "sol-exercicios.html#capítulo-sec-lingagens-livres-de-contexto",
    "title": "11  Solução dos Exercícios",
    "section": "12.2 Capítulo Chapter 5",
    "text": "12.2 Capítulo Chapter 5\n\n12.2.1 Exercícios de Derivação Section 5.2.2\n\n12.2.1.1 Exercício 1: Palíndromo Ímpar\nSolução: a derivação é feita para construir a string de fora para dentro, finalizando com o caractere central.\n\nPasso 1: Começamos com o símbolo inicial \\(K\\).\nPasso 2: A string 101 começa e termina com 1. Aplicamos a regra \\(K \\rightarrow 1K1\\).\nPasso 3: O símbolo restante no centro da string é 0. Aplicamos a regra de caso base \\(K \\rightarrow 0\\) para finalizar a derivação.\n\nA sequência completa da derivação será:\n\\[\nK \\Rightarrow 1K1 \\Rightarrow 101\n\\]\n\n\n12.2.1.2 Exercício 2: Expressão Aritmética Simples\nSolução: nesta derivação, vamos primeiro gerar a estrutura da adição e deporque resolver os operandos.\n\nPasso 1: iniciar com o símbolo inicial \\(E\\).\nPasso 2: a estrutura principal é uma soma. Aplicamos a regra \\(E \\rightarrow E + E\\).\nPasso 3: o operando à direita da soma é um id. Substituímos o segundo \\(E\\) usando a regra \\(E \\rightarrow id\\).\nPasso 4: o operando à esquerda da soma é uma multiplicação. Substituímos o primeiro \\(E\\) usando a regra \\(E \\rightarrow E * E\\).\nPasso 5: o operando à esquerda da multiplicação é um id. Substituímos o primeiro \\(E\\) da forma sentencial atual por id.\nPasso 6: o operando à direita da multiplicação também é um id. Substituímos o \\(E\\) restante por id para completar a derivação.\n\nA sequência completa da derivação Será:\n\\[\nE \\Rightarrow E + E \\Rightarrow E + id \\Rightarrow E * E + id \\Rightarrow id * E + id \\Rightarrow id * id + id\n\\]\n\n\n12.2.1.3 Exercício 3: Palíndromo de Comprimento Par e Aninhado\nSolução: esta derivação mostra o aninhamento repetido da mesma regra.\n\nPasso 1: Partimos do símbolo inicial \\(K\\).\nPasso 2: A string começa e termina com 0. Usamos \\(K \\rightarrow 0K0\\).\nPasso 3: A subcadeia interna, 1111, começa e termina com 1. Aplicamos a regra \\(K \\rightarrow 1K1\\) ao \\(K\\) interno.\nPasso 4: A nova subcadeia interna, 11, também começa e termina com 1. Aplicamos novamente a regra \\(K \\rightarrow 1K1\\).\nPasso 5: O centro da string agora está vazio. Aplicamos a regra \\(K \\rightarrow \\epsilon\\) para finalizar.\n\nA sequência completa da derivação será:\n\\[\nK \\Rightarrow 0K0 \\Rightarrow 01K10 \\Rightarrow 011K110 \\Rightarrow 011\\epsilon110 = 011110\n\\]\n\n\n12.2.1.4 Exercício 4: Linguagem \\(a^nb^n\\)\nSolução: a derivação aplica a regra recursiva três vezes para gerar os três pares de a e b.\n\nPasso 1: Iniciar com \\(S\\).\nPasso 2: Para o par mais externo, aplicamos \\(S \\rightarrow aSb\\).\nPasso 3: Para o segundo par, aplicamos novamente \\(S \\rightarrow aSb\\) ao \\(S\\) interno.\nPasso 4: Para o terceiro e último par, aplicamos \\(S \\rightarrow aSb\\) mais uma vez.\nPasso 5: Com todos os terminais gerados, substituímos o \\(S\\) final por \\(\\epsilon\\) para concluir.\n\nA sequência completa da derivação é:\n\\[\nS \\Rightarrow aSb \\Rightarrow aaSbb \\Rightarrow aaaSbbb \\Rightarrow aaa\\epsilon bbb = aaabbb\n\\]\n\n\n12.2.1.5 Exercício 5: Comando Condicional if-else\nSolução: esta derivação mostra como diferentes não terminais \\((C, A)\\) colaboram para formar uma estrutura completa.\n\nPasso 1: Começamos com o símbolo inicial \\(C\\).\nPasso 2: A única regra para \\(C\\) define a estrutura if-then-else. A aplicamos: \\(C \\rightarrow \\text{if } id \\text{ then } A \\text{ else } A\\).\nPasso 3: Agora, precisamos derivar o comando no bloco then. Substituímos o primeiro \\(A\\) usando a regra \\(A \\rightarrow id := 0\\).\nPasso 4: Finalmente, derivamos o comando no bloco else. Substituímos o segundo \\(A\\) usando a mesma regra \\(A \\rightarrow id := 0\\).\n\nA sequência completa da derivação será:\n\\[\nC \\Rightarrow \\text{if } id \\text{ then } A \\text{ else } A \\Rightarrow \\text{if } id \\text{ then } id := 0 \\text{ else } A \\Rightarrow \\text{if } id \\text{ then } id := 0 \\text{ else } id := 0\n\\]\n\n\n12.2.1.6 Exercício 6: Parênteses Balanceados\nSolução: esta derivação utiliza a concatenação (\\(BB\\)) e o aninhamento (\\((B)\\)).\n\nPasso 1: Partimos de \\(B\\).\nPasso 2: A string é uma concatenação de () e (()). Aplicamos a regra \\(B \\rightarrow BB\\).\nPasso 3: O primeiro \\(B\\) corresponde a (). Para derivá-lo, usamos a regra \\(B \\rightarrow (B)\\).\nPasso 4: O \\(B\\) interno na primeira parte deve ser a string vazia. Aplicamos \\(B \\rightarrow \\epsilon\\).\nPasso 5: Agora, focamos no segundo \\(B\\) da forma sentencial ()B, que corresponde a (()). Aplicamos a regra \\(B \\rightarrow (B)\\).\nPasso 6: O \\(B\\) interno corresponde a (). Aplicamos novamente \\(B \\rightarrow (B)\\).\nPasso 7: O último \\(B\\) é substituído pela string vazia com \\(B \\rightarrow \\epsilon\\) para finalizar.\n\nA sequência completa da derivação será:\n\\[\nB \\Rightarrow BB \\Rightarrow (B)B \\Rightarrow (\\epsilon)B \\Rightarrow ()B \\Rightarrow ()(B) \\Rightarrow ()( (B) ) \\Rightarrow ()( (\\epsilon) ) = ()(())\n\\]\n\n\n12.2.1.7 Exercício 7: Palíndromo Vazio\nSolução: esta é a derivação mais curta possível e testa o entendimento do caso base para a string vazia.\n\nPasso 1: Começamos com o símbolo inicial \\(K\\).\nPasso 2: A regra \\(K \\rightarrow \\epsilon\\) gera diretamente a string vazia.\n\nA sequência completa da derivação será:\n\\[\nK \\Rightarrow \\epsilon\n\\]",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Solução dos Exercícios</span>"
    ]
  },
  {
    "objectID": "referencias.html",
    "href": "referencias.html",
    "title": "Referências",
    "section": "",
    "text": "[1] MCCULLOCH, W. S.; PITTS, W. A\nlogical calculus of the ideas immanent in nervous activity.\nThe bulletin of mathematical biophysics, v. 5, n. 4, p.\n115–133, 1943. \n\n\n[2] KLEENE, S. C. Representation\nof events in nerve nets and finite automata. Em: SHANNON, C. E.;\nMCCARTHY, J. (Eds.). Automata studies. Annals of\nMathematics Studies. [s.l.] Princeton University Press, 1956. v. 34p.\n3–41. \n\n\n[3] LESK, M. E. Lex – A Lexical\nAnalyzer Generator. Murray Hill, NJ: Bell Laboratories,\n1975. \n\n\n[4] PAXSON, V. Flex, a fast\nlexical analyzer generator. [s.l: s.n.]. \n\n\n[5] MOORE, E. F. Gedanken-Experiments\non Sequential Machines. Em: SHANNON, C. E.; MCCARTHY, J. (Eds.).\nAutomata Studies. Princeton: Princeton University\nPress, 1956. p. 129–154. \n\n\n[6] MEALY, G. H. A Method for\nSynthesizing Sequential Circuits. The Bell System Technical\nJournal, v. 34, n. 5, p. 1045–1064, set. 1955. \n\n\n[7] RABIN, M. O.; SCOTT, D. Finite\nAutomata and Their Decision Problems. IBM Journal of\nResearch and Development, v. 3, n. 2, p. 114–125, 1959. \n\n\n[8] TIAN, Y. et al. Experimental\ndemonstration of quantum finite automaton. npj Quantum\nInformation, v. 5, 2019. \n\n\n[9] AHO, A. V. et al. Compilers:\nPrinciples, Techniques, and Tools. 2nd. ed. [s.l.] Pearson;\nAddison-Wesley, 2007.",
    "crumbs": [
      "Referências"
    ]
  }
]